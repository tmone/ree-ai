# REE AI - Implementation Plan Theo MÃ´ HÃ¬nh CTO

**Date:** 2025-10-31
**Status:** PLANNING - Cáº§n implement Ä‘áº§y Ä‘á»§ data pipeline

---

## ðŸš¨ Váº¥n Äá» Hiá»‡n Táº¡i

### ÄÃ£ CÃ³ (3/10 services):
- âœ… Core Gateway (LiteLLM + failover)
- âœ… Orchestrator (routing + intent detection)
- âœ… Service Registry

### ChÆ°a CÃ³ (7/10 services) - **CRITICAL**:
- âŒ Crawler (Crawl4AI) - Láº¥y data tá»« batdongsan.vn, nhatot.vn
- âŒ Semantic Chunking (6 steps) - Chia nhá» document
- âŒ Attribute Extraction - TrÃ­ch xuáº¥t giÃ¡, phÃ²ng ngá»§, Ä‘á»‹a Ä‘iá»ƒm
- âŒ Classification (3 modes) - PhÃ¢n loáº¡i property
- âŒ Completeness Feedback - Kiá»ƒm tra Ä‘áº§y Ä‘á»§
- âŒ Price Suggestion - Gá»£i Ã½ giÃ¡
- âŒ Rerank - Sáº¯p xáº¿p káº¿t quáº£

### ChÆ°a CÃ³ Storage:
- âŒ OpenSearch setup - Vector DB
- âŒ PostgreSQL schema - Metadata storage
- âŒ RAG pipeline - Search + retrieval

---

## ðŸ“‹ Implementation Roadmap

### Phase 1: Data Collection (Week 1) - CRITICAL
**Services cáº§n implement:**

#### 1. Crawler Service (Crawl4AI)
```python
# services/crawler/main.py

from crawl4ai import AsyncWebCrawler
import asyncio

class RealEstateCrawler:
    """
    Crawl real estate data from Vietnamese websites
    """

    async def crawl_batdongsan(self):
        """Crawl batdongsan.com.vn"""
        urls = [
            "https://batdongsan.com.vn/ban-nha-rieng",
            "https://batdongsan.com.vn/ban-can-ho-chung-cu"
        ]

        async with AsyncWebCrawler() as crawler:
            for url in urls:
                result = await crawler.arun(
                    url=url,
                    word_count_threshold=10,
                    extraction_strategy=LLMExtractionStrategy(
                        provider="ollama/llama3.1:8b",
                        extraction_type="schema",
                        schema={
                            "title": str,
                            "price": str,
                            "location": str,
                            "bedrooms": int,
                            "area": str,
                            "description": str
                        }
                    )
                )

                yield result.extracted_content

    async def crawl_nhatot(self):
        """Crawl nhatot.com"""
        # Similar implementation
        pass
```

**Test cáº§n viáº¿t:**
```python
# tests/test_crawler.py

@pytest.mark.asyncio
async def test_crawl_batdongsan_returns_properties():
    """Test crawler láº¥y Ä‘Æ°á»£c data tá»« batdongsan.com.vn"""
    crawler = RealEstateCrawler()
    results = []

    async for property in crawler.crawl_batdongsan():
        results.append(property)
        if len(results) >= 10:
            break

    # Verify data structure
    assert len(results) > 0
    assert "title" in results[0]
    assert "price" in results[0]
    assert "location" in results[0]

@pytest.mark.asyncio
async def test_crawler_extracts_correct_schema():
    """Test crawler trÃ­ch xuáº¥t Ä‘Ãºng schema"""
    # Test extraction vá»›i sample HTML
    pass

@pytest.mark.asyncio
async def test_crawler_handles_pagination():
    """Test crawler xá»­ lÃ½ pagination"""
    # Crawl nhiá»u trang
    pass
```

---

#### 2. Semantic Chunking Service (6 Steps theo CTO)
```python
# services/semantic_chunking/main.py

from sentence_transformers import SentenceTransformer
import nltk
import numpy as np

class SemanticChunker:
    """
    6 Steps Semantic Chunking theo yÃªu cáº§u CTO
    """

    def __init__(self):
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        nltk.download('punkt')

    def chunk(self, text: str) -> list:
        """
        6 Steps:
        1. Sentence Segmentation
        2. Generate Embedding cho tá»«ng cÃ¢u
        3. Cosine Similarity Calculation
        4. Combine vá»›i threshold >0.75
        5. Overlap window
        6. Create Embedding for whole chunk
        """

        # Step 1: Sentence Segmentation
        sentences = nltk.sent_tokenize(text, language='vietnamese')

        # Step 2: Generate Embedding
        embeddings = self.model.encode(sentences)

        # Step 3: Cosine Similarity
        similarities = self._calculate_similarities(embeddings)

        # Step 4: Combine vá»›i threshold
        chunks = self._combine_sentences(sentences, similarities, threshold=0.75)

        # Step 5: Overlap
        overlapped_chunks = self._add_overlap(chunks, window=2)

        # Step 6: Create chunk embeddings
        chunk_embeddings = [
            self.model.encode(chunk)
            for chunk in overlapped_chunks
        ]

        return [
            {
                "text": chunk,
                "embedding": emb.tolist()
            }
            for chunk, emb in zip(overlapped_chunks, chunk_embeddings)
        ]
```

**Test cáº§n viáº¿t:**
```python
# tests/test_semantic_chunking.py

def test_step1_sentence_segmentation():
    """Test Step 1: Chia cÃ¢u Ä‘Ãºng"""
    text = "NhÃ  3 phÃ²ng ngá»§. GiÃ¡ 5 tá»·. View Ä‘áº¹p á»Ÿ Quáº­n 1."
    chunker = SemanticChunker()
    sentences = chunker._segment_sentences(text)

    assert len(sentences) == 3
    assert sentences[0] == "NhÃ  3 phÃ²ng ngá»§."

def test_step2_generate_embeddings():
    """Test Step 2: Generate embeddings cho tá»«ng cÃ¢u"""
    sentences = ["NhÃ  3 phÃ²ng ngá»§", "GiÃ¡ 5 tá»·"]
    chunker = SemanticChunker()
    embeddings = chunker._generate_embeddings(sentences)

    assert len(embeddings) == 2
    assert len(embeddings[0]) == 384  # MiniLM dimension

def test_step3_cosine_similarity():
    """Test Step 3: TÃ­nh cosine similarity"""
    # Test vá»›i 2 cÃ¢u similar vÃ  1 cÃ¢u different
    pass

def test_step4_combine_threshold():
    """Test Step 4: Combine vá»›i threshold >0.75"""
    # Verify sentences with similarity >0.75 are merged
    pass

def test_step5_overlap_window():
    """Test Step 5: Overlap window"""
    # Verify overlap works correctly
    pass

def test_step6_chunk_embeddings():
    """Test Step 6: Generate embedding cho whole chunk"""
    # Verify final embeddings
    pass

def test_full_chunking_pipeline():
    """Test toÃ n bá»™ 6 steps"""
    text = """
    BÃ¡n nhÃ  máº·t tiá»n Ä‘Æ°á»ng Tráº§n HÆ°ng Äáº¡o, Quáº­n 1.
    Diá»‡n tÃ­ch: 80m2, 3 táº§ng, 4 phÃ²ng ngá»§, 5 toilet.
    GiÃ¡: 15 tá»· VNÄ (cÃ³ thÆ°Æ¡ng lÆ°á»£ng).
    NhÃ  má»›i xÃ¢y, ná»™i tháº¥t cao cáº¥p, view Ä‘áº¹p.
    Gáº§n trÆ°á»ng há»c, bá»‡nh viá»‡n, chá»£, siÃªu thá»‹.
    """

    chunker = SemanticChunker()
    chunks = chunker.chunk(text)

    # Verify chunks
    assert len(chunks) > 0
    for chunk in chunks:
        assert "text" in chunk
        assert "embedding" in chunk
        assert len(chunk["embedding"]) == 384
```

---

#### 3. Attribute Extraction Service
```python
# services/attribute_extraction/main.py

from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel

class PropertyAttributes(BaseModel):
    price: float
    bedrooms: int
    bathrooms: int
    area: float
    location: str
    property_type: str

class AttributeExtractor:
    """
    TrÃ­ch xuáº¥t attributes tá»« text báº±ng LLM
    """

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    async def extract(self, text: str) -> PropertyAttributes:
        """Extract structured attributes"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Báº¡n lÃ  chuyÃªn gia trÃ­ch xuáº¥t thÃ´ng tin báº¥t Ä‘á»™ng sáº£n.
TrÃ­ch xuáº¥t cÃ¡c thÃ´ng tin sau tá»« text:
- price: giÃ¡ (VNÄ, convert sang sá»‘)
- bedrooms: sá»‘ phÃ²ng ngá»§
- bathrooms: sá»‘ toilet
- area: diá»‡n tÃ­ch (m2)
- location: Ä‘á»‹a Ä‘iá»ƒm (Quáº­n, ThÃ nh phá»‘)
- property_type: loáº¡i (nhÃ , cÄƒn há»™, Ä‘áº¥t)

Tráº£ vá» JSON format."""),
            ("user", "{text}")
        ])

        response = await self.llm.ainvoke(prompt.format(text=text))
        # Parse response to PropertyAttributes
        return PropertyAttributes.parse_raw(response.content)
```

**Test cáº§n viáº¿t:**
```python
# tests/test_attribute_extraction.py

@pytest.mark.asyncio
async def test_extract_price():
    """Test trÃ­ch xuáº¥t giÃ¡"""
    text = "NhÃ  giÃ¡ 5 tá»· VNÄ"
    extractor = AttributeExtractor()
    attrs = await extractor.extract(text)

    assert attrs.price == 5_000_000_000

@pytest.mark.asyncio
async def test_extract_bedrooms():
    """Test trÃ­ch xuáº¥t sá»‘ phÃ²ng ngá»§"""
    text = "NhÃ  3 phÃ²ng ngá»§, 2 toilet"
    extractor = AttributeExtractor()
    attrs = await extractor.extract(text)

    assert attrs.bedrooms == 3
    assert attrs.bathrooms == 2

@pytest.mark.asyncio
async def test_extract_location():
    """Test trÃ­ch xuáº¥t Ä‘á»‹a Ä‘iá»ƒm"""
    text = "NhÃ  á»Ÿ Quáº­n 1, TP.HCM"
    extractor = AttributeExtractor()
    attrs = await extractor.extract(text)

    assert "quáº­n 1" in attrs.location.lower()

@pytest.mark.asyncio
async def test_extract_all_attributes():
    """Test trÃ­ch xuáº¥t Ä‘áº§y Ä‘á»§ attributes"""
    text = """
    BÃ¡n nhÃ  máº·t tiá»n Quáº­n 1
    Diá»‡n tÃ­ch: 80m2
    3 phÃ²ng ngá»§, 4 toilet
    GiÃ¡: 15 tá»· VNÄ
    """

    extractor = AttributeExtractor()
    attrs = await extractor.extract(text)

    assert attrs.price == 15_000_000_000
    assert attrs.bedrooms == 3
    assert attrs.bathrooms == 4
    assert attrs.area == 80.0
    assert "quáº­n 1" in attrs.location.lower()
```

---

### Phase 2: Storage Setup (Week 2)

#### 4. OpenSearch Setup
```yaml
# docker-compose.yml

opensearch:
  image: opensearchproject/opensearch:latest
  environment:
    - discovery.type=single-node
    - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
  ports:
    - 9200:9200

# services/db_gateway/opensearch_client.py

from opensearchpy import OpenSearch

class RealEstateIndex:
    """Quáº£n lÃ½ OpenSearch index cho real estate"""

    def __init__(self):
        self.client = OpenSearch([{'host': 'localhost', 'port': 9200}])

    def create_index(self):
        """Táº¡o index vá»›i mapping"""
        mapping = {
            "mappings": {
                "properties": {
                    "title": {"type": "text"},
                    "description": {"type": "text"},
                    "price": {"type": "float"},
                    "bedrooms": {"type": "integer"},
                    "location": {"type": "keyword"},
                    "embedding": {
                        "type": "knn_vector",
                        "dimension": 384
                    }
                }
            }
        }
        self.client.indices.create(index="real_estate", body=mapping)

    async def index_property(self, property_data: dict):
        """Index má»™t property"""
        await self.client.index(
            index="real_estate",
            body=property_data
        )

    async def search(self, query_embedding: list, filters: dict = None):
        """Vector search"""
        query = {
            "size": 10,
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": query_embedding}
                    }
                }
            }
        }

        if filters:
            query["query"]["script_score"]["query"] = {
                "bool": {"filter": [{"term": filters}]}
            }

        return await self.client.search(index="real_estate", body=query)
```

**Test cáº§n viáº¿t:**
```python
# tests/test_opensearch_storage.py

@pytest.mark.asyncio
async def test_create_index():
    """Test táº¡o index"""
    index = RealEstateIndex()
    await index.create_index()

    # Verify index exists
    assert await index.client.indices.exists("real_estate")

@pytest.mark.asyncio
async def test_index_property():
    """Test index má»™t property"""
    property_data = {
        "title": "NhÃ  3 phÃ²ng ngá»§",
        "price": 5_000_000_000,
        "bedrooms": 3,
        "embedding": [0.1] * 384
    }

    index = RealEstateIndex()
    doc_id = await index.index_property(property_data)

    # Verify indexed
    doc = await index.client.get(index="real_estate", id=doc_id)
    assert doc["_source"]["title"] == "NhÃ  3 phÃ²ng ngá»§"

@pytest.mark.asyncio
async def test_vector_search():
    """Test vector search"""
    # Index 10 properties
    # Search with query vector
    # Verify results ranked by similarity
    pass
```

---

### Phase 3: E2E Pipeline (Week 3)

#### 5. Full Data Pipeline Test
```python
# tests/test_full_pipeline.py

@pytest.mark.e2e
@pytest.mark.slow
class TestFullDataPipeline:
    """Test toÃ n bá»™ pipeline tá»« crawl Ä‘áº¿n search"""

    @pytest.mark.asyncio
    async def test_crawl_to_search_pipeline(self):
        """
        Test E2E pipeline:
        1. Crawl data tá»« batdongsan.com.vn
        2. Semantic chunking
        3. Attribute extraction
        4. Index vÃ o OpenSearch
        5. Search vÃ  verify káº¿t quáº£
        """

        # Step 1: Crawl
        crawler = RealEstateCrawler()
        properties = []
        async for prop in crawler.crawl_batdongsan():
            properties.append(prop)
            if len(properties) >= 10:
                break

        assert len(properties) == 10

        # Step 2: Semantic Chunking
        chunker = SemanticChunker()
        chunked_properties = []
        for prop in properties:
            chunks = chunker.chunk(prop["description"])
            chunked_properties.append({
                **prop,
                "chunks": chunks
            })

        # Step 3: Attribute Extraction
        extractor = AttributeExtractor()
        extracted_properties = []
        for prop in chunked_properties:
            attrs = await extractor.extract(prop["description"])
            extracted_properties.append({
                **prop,
                "attributes": attrs.dict()
            })

        # Step 4: Index to OpenSearch
        index = RealEstateIndex()
        for prop in extracted_properties:
            await index.index_property({
                "title": prop["title"],
                "description": prop["description"],
                "price": prop["attributes"]["price"],
                "bedrooms": prop["attributes"]["bedrooms"],
                "location": prop["attributes"]["location"],
                "embedding": prop["chunks"][0]["embedding"]
            })

        # Step 5: Search
        query = "TÃ¬m nhÃ  3 phÃ²ng ngá»§ giÃ¡ dÆ°á»›i 6 tá»· á»Ÿ Quáº­n 1"
        query_embedding = chunker.model.encode(query).tolist()

        results = await index.search(
            query_embedding=query_embedding,
            filters={"bedrooms": 3}
        )

        # Verify results
        assert len(results["hits"]["hits"]) > 0

        # Verify filtering worked
        for hit in results["hits"]["hits"]:
            assert hit["_source"]["bedrooms"] == 3
            assert hit["_source"]["price"] < 6_000_000_000

        print(f"âœ… E2E Pipeline completed: {len(results['hits']['hits'])} results found")
```

---

## ðŸ“Š Test Coverage Required

### Minimum Test Coverage:

| Component | Tests Required | Current | Status |
|-----------|---------------|---------|--------|
| Crawler | 10 tests | 0 | âŒ TODO |
| Semantic Chunking | 8 tests (6 steps + integration) | 0 | âŒ TODO |
| Attribute Extraction | 6 tests | 0 | âŒ TODO |
| Classification | 6 tests | 0 | âŒ TODO |
| Completeness | 4 tests | 0 | âŒ TODO |
| OpenSearch | 8 tests | 0 | âŒ TODO |
| PostgreSQL | 6 tests | 0 | âŒ TODO |
| RAG Pipeline | 10 tests | 0 | âŒ TODO |
| E2E Pipeline | 5 tests | 0 | âŒ TODO |
| **TOTAL** | **63 tests** | **0** | **0% coverage** |

---

## â±ï¸ Implementation Timeline

### Week 1: Data Collection
- [ ] Day 1-2: Crawler Service (Crawl4AI)
- [ ] Day 3-4: Semantic Chunking (6 steps)
- [ ] Day 5: Attribute Extraction
- [ ] Day 6-7: Tests for Week 1

### Week 2: Storage & Processing
- [ ] Day 1-2: OpenSearch setup + indexing
- [ ] Day 3-4: PostgreSQL schema + storage
- [ ] Day 5: Classification Service
- [ ] Day 6-7: Tests for Week 2

### Week 3: Search & RAG
- [ ] Day 1-2: RAG Pipeline
- [ ] Day 3: Rerank Service
- [ ] Day 4: Price Suggestion
- [ ] Day 5-7: E2E tests + integration

---

## ðŸŽ¯ Acceptance Criteria

### Crawler Service:
- [ ] Crawl batdongsan.com.vn successfully
- [ ] Crawl nhatot.com successfully
- [ ] Extract correct schema (title, price, location, bedrooms, area)
- [ ] Handle pagination (>100 properties)
- [ ] Error handling (network, parsing)

### Semantic Chunking:
- [ ] Step 1: Sentence segmentation works
- [ ] Step 2: Embeddings generated correctly
- [ ] Step 3: Cosine similarity calculated
- [ ] Step 4: Sentences combined with threshold >0.75
- [ ] Step 5: Overlap window applied
- [ ] Step 6: Final chunk embeddings created
- [ ] Vietnamese text support

### Storage:
- [ ] OpenSearch index created with correct mapping
- [ ] Properties indexed successfully
- [ ] Vector search works (cosine similarity)
- [ ] Filtering works (price, bedrooms, location)
- [ ] PostgreSQL stores metadata

### E2E:
- [ ] Full pipeline: Crawl â†’ Chunk â†’ Extract â†’ Store â†’ Search
- [ ] User query returns relevant results
- [ ] Results ranked by relevance
- [ ] Performance < 3s for search

---

**Status:** PLANNING
**Next Action:** Implement Crawler Service first
**Priority:** HIGH - Core data pipeline missing

