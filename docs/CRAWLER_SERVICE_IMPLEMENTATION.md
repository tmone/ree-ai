# Crawler Service - Automated Master Data Collection

## üéØ Overview

The **Crawler Service** automatically scrapes property listings from popular Vietnamese real estate websites to discover and populate master data. This enables the system to **continuously learn** from real-world data.

## üîÑ How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Crawl Real Estate Websites (Crawl4AI)                      ‚îÇ
‚îÇ    ‚îú‚îÄ Batdongsan.com.vn                                        ‚îÇ
‚îÇ    ‚îú‚îÄ Mogi.vn                                                  ‚îÇ
‚îÇ    ‚îî‚îÄ More sites can be added...                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Parse Listings & Extract Attributes                        ‚îÇ
‚îÇ    ‚îú‚îÄ Title, Price, Location                                  ‚îÇ
‚îÇ    ‚îú‚îÄ Property Type, Area, Bedrooms                           ‚îÇ
‚îÇ    ‚îú‚îÄ Amenities (pool, gym, parking, etc.)                    ‚îÇ
‚îÇ    ‚îî‚îÄ Features (direction, furniture, view, etc.)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Discover New Master Data                                   ‚îÇ
‚îÇ    ‚îú‚îÄ Compare with existing master data                       ‚îÇ
‚îÇ    ‚îú‚îÄ Find new districts, amenities, features                 ‚îÇ
‚îÇ    ‚îú‚îÄ Track frequency (how many times each appears)           ‚îÇ
‚îÇ    ‚îî‚îÄ Prioritize high-frequency items                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Store in pending_master_data for Admin Review              ‚îÇ
‚îÇ    ‚îú‚îÄ Auto-increment frequency if already exists              ‚îÇ
‚îÇ    ‚îú‚îÄ Store suggested table + category                        ‚îÇ
‚îÇ    ‚îî‚îÄ Ready for admin approval via Admin API                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üåê Supported Real Estate Sites

### 1. Batdongsan.com.vn
- **URL**: https://batdongsan.com.vn
- **Coverage**: Apartments, houses, land across Vietnam
- **Extracts**: Property type, location, price, area, amenities, features

### 2. Mogi.vn
- **URL**: https://mogi.vn
- **Coverage**: High-end properties, apartments in major cities
- **Extracts**: Property type, location, price, area, amenities

### 3. Extensible Design
- **Easy to add more sites**: Inherit from `BaseCrawler`
- **Consistent data format**: All crawlers return same structure
- **Plug-and-play**: Add new crawler ‚Üí Auto-discovered by service

## üìä Architecture

```
services/crawler_service/
‚îú‚îÄ‚îÄ main.py                          # FastAPI service
‚îú‚îÄ‚îÄ Dockerfile                        # Docker config
‚îú‚îÄ‚îÄ crawlers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_crawler.py              # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ batdongsan_crawler.py        # Batdongsan.com.vn
‚îÇ   ‚îî‚îÄ‚îÄ mogi_crawler.py              # Mogi.vn
‚îî‚îÄ‚îÄ master_data_populator.py         # Analyzes & stores new data
```

### Key Components

#### 1. BaseCrawler (Abstract)
```python
class BaseCrawler(ABC):
    @abstractmethod
    async def get_listing_urls(self, max_pages: int) -> List[str]:
        """Get URLs of listings to scrape"""
        pass

    @abstractmethod
    async def parse_listing(self, html: str, markdown: str) -> Optional[Dict]:
        """Parse a single listing"""
        pass
```

#### 2. Site-Specific Crawlers
Each site has its own crawler implementing:
- URL pattern recognition
- HTML parsing logic
- Attribute extraction

#### 3. Master Data Populator
Analyzes scraped data and:
- Compares with existing master data
- Identifies new attributes
- Tracks frequency
- Stores in `pending_master_data`

## üöÄ API Endpoints

### POST /crawl
Crawl listings from specified site(s).

**Request:**
```json
{
  "site": "batdongsan",  // or "mogi" or "all"
  "max_pages": 5,
  "extract_master_data": true,
  "auto_populate": true
}
```

**Response:**
```json
{
  "site": "batdongsan",
  "listings_scraped": 47,
  "new_attributes_found": 12,
  "processing_time_ms": 125000,
  "sample_listings": [
    {
      "title": "CƒÉn h·ªô 2PN Vinhomes Central Park",
      "price": 5500000000,
      "district": "Qu·∫≠n B√¨nh Th·∫°nh",
      "area": 80,
      "bedrooms": 2,
      "amenities": ["swimming_pool", "gym", "parking"],
      "source_url": "https://..."
    }
  ]
}
```

### GET /crawlers
List available crawlers.

**Response:**
```json
{
  "crawlers": [
    {
      "id": "batdongsan",
      "name": "Batdongsan.com.vn",
      "url": "https://batdongsan.com.vn"
    },
    {
      "id": "mogi",
      "name": "Mogi.vn",
      "url": "https://mogi.vn"
    }
  ]
}
```

### POST /schedule-crawl
Schedule periodic crawling (future enhancement).

## üì• Deployment

### 1. Start Crawler Service

```bash
# Start crawler service
docker-compose up crawler-service

# Or with all services
docker-compose --profile all up -d
```

### 2. Test Crawling

```bash
# Crawl Batdongsan (5 pages, auto-populate)
curl -X POST http://localhost:8095/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "site": "batdongsan",
    "max_pages": 5,
    "extract_master_data": true,
    "auto_populate": true
  }'
```

### 3. Review Discovered Data

```bash
# Check pending master data items
curl http://localhost:8084/admin/pending-items?status=pending

# Approve items via Admin API
curl -X POST http://localhost:8084/admin/approve-item \
  -H "Content-Type: application/json" \
  -d '{
    "pending_id": 1,
    "translations": {
      "vi": "H·ªì b∆°i v√¥ c·ª±c",
      "en": "Infinity pool"
    },
    "admin_user_id": "admin123"
  }'
```

## üîç What Gets Discovered?

### Districts & Wards
```
Discovered:
- "Qu·∫≠n 1" ‚Üí Already in master data ‚úì
- "Qu·∫≠n Th·ªß ƒê·ª©c" ‚Üí NEW (frequency: 15) ‚ö†Ô∏è
- "Ph∆∞·ªùng T√¢n Ph√∫" ‚Üí NEW (frequency: 8) ‚ö†Ô∏è
```

### Amenities
```
Discovered:
- "swimming_pool" ‚Üí Already in master data ‚úì
- "infinity_pool" ‚Üí NEW (frequency: 12) ‚ö†Ô∏è
- "sky_garden" ‚Üí NEW (frequency: 7) ‚ö†Ô∏è
- "pet_park" ‚Üí NEW (frequency: 5) ‚ö†Ô∏è
```

### View Types
```
Discovered:
- "river_view" ‚Üí Already in master data ‚úì
- "landmark_view" ‚Üí Already in master data ‚úì
- "golf_view" ‚Üí NEW (frequency: 3) ‚ö†Ô∏è
```

## üìà Master Data Growth Process

```
Week 1: Start with seed data
‚îú‚îÄ 25 districts (HCMC)
‚îú‚îÄ 27 amenities
‚îî‚îÄ 9 view types

Week 2: First crawl (100 listings)
‚îú‚îÄ Discovered 15 new amenities
‚îú‚îÄ Admin reviews and approves 12
‚îî‚îÄ Master data grows to 39 amenities

Week 3: Second crawl (200 listings)
‚îú‚îÄ Discovered 8 new amenities
‚îú‚îÄ 5 are duplicates (ignored)
‚îú‚îÄ Admin approves 3 new ones
‚îî‚îÄ Master data grows to 42 amenities

...and so on
```

## ‚öôÔ∏è Configuration

### Rate Limiting
```python
# In base_crawler.py
await asyncio.sleep(1)  # 1 second between requests
```

### Max Pages Per Crawl
```python
# Default: 5 pages per site
# Can be increased for more data
crawler.crawl(max_pages=10)
```

### Auto-Population
```python
# Enable auto-population (recommended)
{
  "auto_populate": true  // Automatically add to pending_master_data
}

# Disable for review-only mode
{
  "auto_populate": false  // Just discover, don't store
}
```

## üõ†Ô∏è Adding New Crawlers

### Step 1: Create Crawler Class

```python
# services/crawler_service/crawlers/mysite_crawler.py

from services.crawler_service.crawlers.base_crawler import BaseCrawler

class MySiteCrawler(BaseCrawler):
    def __init__(self):
        super().__init__(
            site_name="MySite.vn",
            base_url="https://mysite.vn"
        )

    async def get_listing_urls(self, max_pages: int) -> List[str]:
        # Implement: Get listing URLs from search results
        pass

    async def parse_listing(self, html: str, markdown: str) -> Optional[Dict]:
        # Implement: Parse listing HTML
        pass
```

### Step 2: Register in Service

```python
# services/crawler_service/main.py

from services.crawler_service.crawlers.mysite_crawler import MySiteCrawler

self.crawlers = {
    'batdongsan': BatdongsanCrawler(),
    'mogi': MogiCrawler(),
    'mysite': MySiteCrawler()  # ‚Üê Add here
}
```

### Step 3: Test

```bash
curl -X POST http://localhost:8095/crawl \
  -H "Content-Type: application/json" \
  -d '{"site": "mysite", "max_pages": 5}'
```

## üìä Monitoring & Analytics

### View Crawl Statistics
```bash
# Get crawler list
curl http://localhost:8095/crawlers

# Check logs
docker logs ree-ai-crawler-service --tail 100 -f
```

### Review Discovered Data
```bash
# High-frequency items (priority review)
curl http://localhost:8084/admin/pending-items?status=pending | jq '.high_frequency_items'

# All pending items
curl http://localhost:8084/admin/pending-items?limit=100
```

## üîí Best Practices

### 1. Rate Limiting
- Respect website terms of service
- Use reasonable delays between requests
- Crawl during off-peak hours

### 2. Data Quality
- Review high-frequency items first
- Normalize inconsistent naming
- Merge similar attributes

### 3. Periodic Crawling
- Crawl weekly to discover new trends
- Compare month-over-month changes
- Update master data quarterly

### 4. Legal Compliance
- Only scrape public listings
- Respect robots.txt
- Don't scrape personal data
- Attribute source in documentation

## üêõ Troubleshooting

### Issue: Crawler fails to start

**Cause**: Chromium not installed

**Solution**:
```dockerfile
# Dockerfile already includes:
RUN apt-get update && apt-get install -y chromium chromium-driver
```

### Issue: No listings found

**Cause**: Website structure changed

**Solution**: Update crawler parsing logic
```python
# Check logs for errors
docker logs ree-ai-crawler-service --tail 50

# Update selectors in crawler
soup.find('div', class_=re.compile(r'new-class-pattern'))
```

### Issue: Duplicate pending items

**Cause**: Already handled - frequency auto-increments

**Solution**: No action needed, system handles duplicates

## üìö Related Documentation

- [Master Data Extraction Guide](./MASTER_DATA_EXTRACTION_IMPLEMENTATION_GUIDE.md)
- [Admin API Documentation](./MASTER_DATA_EXTRACTION_COMPLETE_IMPLEMENTATION.md)
- [Crawl4AI Documentation](https://crawl4ai.com/docs)

---

**Service**: Crawler Service
**Port**: 8095
**Status**: ‚úÖ Production Ready
**Version**: 1.0.0
**Last Updated**: 2025-01-13
