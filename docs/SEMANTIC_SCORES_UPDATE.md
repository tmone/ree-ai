# Semantic Scores from OpenSearch - Implementation Update

## ğŸ“Š Overview

Updated orchestrator to use **semantic scores from OpenSearch** instead of rule-based match scores for property ranking in clarification responses.

**Date**: 2025-11-01
**Status**: âœ… Implemented & Testing

---

## ğŸ¯ What Changed

### Before (Rule-Based Scoring)

```python
# Orchestrator calculated scores based on hard-coded rules
def _calculate_match_score(self, prop: Dict, requirements: Dict) -> int:
    score = 0
    if district_matches: score += 40
    if bedrooms_match: score += 30
    if type_matches: score += 15
    if price_ok: score += 15
    return score
```

**Problems:**
- âŒ Ignores semantic similarity from OpenSearch
- âŒ Rules can't capture nuanced meanings
- âŒ Doesn't reflect actual search relevance

### After (Semantic Scoring)

```python
# Use OpenSearch's vector similarity scores
if results_have_semantic_scores:
    max_score = max(prop.get("score") for prop in results)
    for prop in results:
        semantic_score = prop.get("score")
        normalized_score = int((semantic_score / max_score) * 100)
        scored_results.append({"property": prop, "score": normalized_score})
else:
    # Fallback to rule-based scoring if no semantic scores
    for prop in results:
        score = self._calculate_match_score(prop, requirements)
        scored_results.append({"property": prop, "score": score})
```

**Benefits:**
- âœ… Uses actual vector similarity from OpenSearch
- âœ… Reflects semantic understanding of query
- âœ… Automatically improves as embedding model improves
- âœ… Fallback to rules if semantic scores unavailable

---

## ğŸ”§ Technical Implementation

### 1. DB Gateway Already Returns Scores

**File**: `/Users/tmone/ree-ai/services/db_gateway/main.py:419`

```python
results.append(PropertyResult(
    property_id=source.get('property_id', hit['_id']),
    title=source.get('title', ''),
    price=source.get('price', 0),
    # ... other fields ...
    score=float(hit['_score'])  # â† OpenSearch's relevance score
))
```

OpenSearch vector search returns `_score` field which represents cosine similarity or k-NN distance.

---

### 2. Orchestrator Normalization

**File**: `/Users/tmone/ree-ai/services/orchestrator/main.py:919-928`

```python
# Check if results have semantic scores from OpenSearch
has_semantic_scores = any(isinstance(prop.get("score"), (int, float)) for prop in results)

if has_semantic_scores:
    # Use semantic scores from OpenSearch and normalize to 0-100
    max_score = max((prop.get("score", 0) for prop in results), default=1.0)
    self.logger.info(f"{LogEmoji.INFO} Using OpenSearch semantic scores (max: {max_score:.2f})")

    for prop in results:
        semantic_score = prop.get("score", 0)
        # Normalize to 0-100 range based on max score in this batch
        normalized_score = int((semantic_score / max_score) * 100) if max_score > 0 else 0
        scored_results.append({"property": prop, "score": normalized_score, "type": "semantic"})
```

**Normalization Strategy:**
- Find max score in current batch
- Normalize all scores: `(score / max_score) * 100`
- Convert to integer for display: "Äiá»ƒm: 85/100"
- Top result always gets 100 points

**Why Normalize?**
- OpenSearch scores are floats (e.g., 0.8537, 12.45)
- Users understand percentages (0-100) better
- Consistent with existing UI expectations

---

### 3. Best Results Tracking Fix

**File**: `/Users/tmone/ree-ai/services/orchestrator/main.py:361-377`

**Problem:** Previously, orchestrator only kept `last_results` from final iteration. If iteration 1 found 5 results but iteration 2 found 0, clarification got 0 results.

**Solution:** Track `best_results` across all iterations.

```python
best_results = []  # Keep track of BEST results across iterations
best_evaluation = None

for iteration in range(max_iterations):
    results = await self._execute_search_internal(current_query)
    evaluation = await self._evaluate_results(results, requirements)

    # Keep track of best results (prefer more results if quality is similar)
    if (not best_results and results) or (results and len(results) > len(best_results)):
        best_results = results
        best_evaluation = evaluation
        self.logger.info(f"{LogEmoji.INFO} Updated best_results: {len(best_results)} properties")

    # ... evaluation and iteration logic ...

    # Use best results from all iterations, not just last one
    return await self._ask_clarification(requirements, best_evaluation or evaluation, best_results)
```

**Benefit:** Clarification always shows best available alternatives, even if later refinement fails.

---

## ğŸ“‹ Example Output

### Query
"TÃ¬m cÄƒn há»™ 3 phÃ²ng ngá»§ á»Ÿ quáº­n 2 gáº§n trÆ°á»ng quá»‘c táº¿"

### Response with Semantic Scores

```
TÃ´i tÃ¬m tháº¥y **150 cÄƒn há»™** á»Ÿ TP.HCM, nhÆ°ng **khÃ´ng cÃ³ cÄƒn nÃ o á»Ÿ quáº­n 2**.

**Báº¡n muá»‘n tÃ´i:**
- ğŸ” TÃ¬m thÃªm á»Ÿ **cÃ¡c quáº­n lÃ¢n cáº­n** (Quáº­n 9, Thá»§ Äá»©c, BÃ¬nh Tháº¡nh)
- ğŸŒ Má»Ÿ rá»™ng tÃ¬m kiáº¿m **toÃ n TP.HCM**
- ğŸ“ Cung cáº¥p thÃ´ng tin cá»¥ thá»ƒ hÆ¡n vá» "gáº§n trÆ°á»ng quá»‘c táº¿"
- ğŸ›ï¸ Äiá»u chá»‰nh sá»‘ phÃ²ng ngá»§ (3 Â± 1 phÃ²ng)

**DÆ°á»›i Ä‘Ã¢y lÃ  5 BÄS gáº§n nháº¥t cÃ³ thá»ƒ phÃ¹ há»£p:**

1. ğŸŸ¢ **CÄƒn há»™ The Sun Avenue 3PN** (Äiá»ƒm: 100/100)  â† Normalized semantic score
   ğŸ’° GiÃ¡: 4.5 tá»· | ğŸ“ 90mÂ² | ğŸ›ï¸ 3 PN
   ğŸ“ BÃ¬nh Tháº¡nh

2. ğŸŸ¡ **CÄƒn há»™ Masteri Tháº£o Äiá»n** (Äiá»ƒm: 87/100)
   ğŸ’° GiÃ¡: 5.2 tá»· | ğŸ“ 85mÂ² | ğŸ›ï¸ 3 PN
   ğŸ“ Quáº­n 2

3. ğŸŸ¡ **CÄƒn há»™ Vista Verde 3PN** (Äiá»ƒm: 73/100)
   ğŸ’° GiÃ¡: 4.8 tá»· | ğŸ“ 88mÂ² | ğŸ›ï¸ 3 PN
   ğŸ“ Thá»§ Äá»©c

4. ğŸŸ¡ **CÄƒn há»™ Gateway Tháº£o Äiá»n** (Äiá»ƒm: 65/100)
   ğŸ’° GiÃ¡: 6.0 tá»· | ğŸ“ 95mÂ² | ğŸ›ï¸ 3 PN
   ğŸ“ Quáº­n 2

5. ğŸ”´ **CÄƒn há»™ Palm Heights** (Äiá»ƒm: 42/100)
   ğŸ’° GiÃ¡: 5.5 tá»· | ğŸ“ 82mÂ² | ğŸ›ï¸ 2 PN
   ğŸ“ Quáº­n 2

ğŸ’¬ Báº¡n muá»‘n tÃ´i há»— trá»£ nhÆ° tháº¿ nÃ o?
```

---

## ğŸ” Logs Analysis

### Success Case (Semantic Scores)

```
2025-11-01 12:28:14,185 - orchestrator - INFO - âœ… [ReAct-Act] Found 5 results
2025-11-01 12:28:14,185 - orchestrator - INFO - â„¹ï¸ Updated best_results: 5 properties
2025-11-01 12:28:23,858 - orchestrator - INFO - â„¹ï¸ Using OpenSearch semantic scores (max: 12.45)
```

- Iteration 1: Found 5 results with semantic scores
- Max score: 12.45 (OpenSearch k-NN score)
- Normalized to 0-100 range

### Fallback Case (Rule-Based)

```
2025-11-01 12:28:23,858 - orchestrator - INFO - â„¹ï¸ Using rule-based match scores (no semantic scores found)
```

- No semantic scores in results
- Falls back to `_calculate_match_score()` method

---

## ğŸ¯ Why Semantic Scores Matter

### 1. **Query Understanding**

**Query:** "TÃ¬m cÄƒn há»™ gáº§n trÆ°á»ng quá»‘c táº¿"

**Semantic Search (OpenSearch):**
- Understands "gáº§n trÆ°á»ng quá»‘c táº¿" = proximity to international schools
- Finds properties mentioning: "British International School", "Australian International School"
- Even if exact phrase "trÆ°á»ng quá»‘c táº¿" not in listing

**Rule-Based:**
- Can only match exact keywords
- Misses semantic relationships

---

### 2. **Ranking Quality**

**OpenSearch Semantic Scores:**
- Based on vector similarity (embeddings)
- Considers:
  - Semantic meaning of query
  - Context of entire listing
  - Relationships between terms

**Rule-Based Scores:**
- Fixed weights: District 40%, Bedrooms 30%, Type 15%, Price 15%
- Doesn't adapt to different query types
- Misses nuanced preferences

---

### 3. **User Intent Matching**

**Example:**

Query: "CÄƒn há»™ view Ä‘áº¹p yÃªn tÄ©nh"

**Semantic Scores** will rank higher:
- "CÄƒn há»™ view sÃ´ng SÃ i GÃ²n, thoÃ¡ng mÃ¡t" (Score: 95/100)
- "CÄƒn há»™ hÆ°á»›ng cÃ´ng viÃªn, khÃ´ng á»“n" (Score: 88/100)

**Rule-Based** would ignore "view Ä‘áº¹p yÃªn tÄ©nh" entirely because it's not a structured field (district, bedrooms, etc).

---

## ğŸ“Š Performance Considerations

### Normalization Overhead
- **Impact**: Minimal (O(n) where n = number of results)
- **Typical n**: 5-10 properties
- **Time**: < 1ms

### Memory
- **Additional data**: One float per property (`score` field)
- **Impact**: Negligible (5 properties Ã— 8 bytes = 40 bytes)

### Accuracy Trade-off
- **Semantic scores**: Reflect actual search relevance
- **Rule-based scores**: Consistent but less accurate
- **Recommendation**: Use semantic when available

---

## ğŸš€ Testing

### Test Script

```bash
python3 test_semantic_scores.py
```

**Expected Output:**
```
âœ… Found 5 properties with scores: [100, 87, 73, 65, 42]
âœ… All scores are in 0-100 range (normalized)
âœ… Scores are sorted descending (best first)

ğŸ“Š Score Statistics:
  - Maximum: 100/100
  - Minimum: 42/100
  - Average: 73.4/100
  - Range: 58 points

âœ… SUCCESS: Semantic scores are being used!
```

---

## ğŸ”® Future Improvements

### 1. Hybrid Scoring

Combine semantic + rule-based:

```python
final_score = (semantic_score * 0.7) + (rule_based_score * 0.3)
```

**Benefits:**
- Semantic for query understanding
- Rule-based for hard requirements (e.g., budget)

### 2. Score Calibration

Instead of relative normalization (max = 100), use absolute thresholds:

```python
# Calibrate based on typical score ranges
if semantic_score > 10.0:  # Excellent
    normalized_score = 90 + (semantic_score - 10) * 2
elif semantic_score > 5.0:  # Good
    normalized_score = 70 + (semantic_score - 5) * 4
else:  # Poor
    normalized_score = semantic_score * 14
```

### 3. User Feedback Learning

Collect user clicks/selections to refine scoring:

```python
# If user consistently selects properties with score < 70
# Adjust normalization to show more diversity
```

---

## âœ… Production Readiness

- [x] Semantic scores from OpenSearch
- [x] Normalization to 0-100 range
- [x] Logging for debugging
- [x] Fallback to rule-based scoring
- [x] Best results tracking across iterations
- [x] Visual indicators (ğŸŸ¢ğŸŸ¡ğŸ”´)
- [ ] Hybrid scoring (TODO)
- [ ] Score calibration (TODO)
- [ ] User feedback integration (TODO)

---

## ğŸ“š Related Files

- **Orchestrator**: `/Users/tmone/ree-ai/services/orchestrator/main.py`
- **DB Gateway**: `/Users/tmone/ree-ai/services/db_gateway/main.py`
- **Models**: `/Users/tmone/ree-ai/shared/models/db_gateway.py`
- **Test Script**: `/Users/tmone/ree-ai/test_semantic_scores.py`
- **V2 Documentation**: `/Users/tmone/ree-ai/docs/REACT_CLARIFICATION_V2_IMPROVED.md`

---

**Last Updated**: 2025-11-01
**Implementation Status**: âœ… Complete & Testing
**Next Step**: Verify semantic scores display correctly in production
