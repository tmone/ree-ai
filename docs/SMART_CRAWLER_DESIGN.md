# Smart Crawler - Production Design

## Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t

**Challenge**: Trong production, lÃ m sao biáº¿t page nÃ o Ä‘Ã£ crawl Ä‘á»ƒ trÃ¡nh lÃ£ng phÃ­?

## Giáº£i phÃ¡p: Multi-Layer Auto-Resume

### Layer 1: Database State Tracking

Sá»­ dá»¥ng báº£ng `crawl_state` Ä‘á»ƒ track tá»«ng URL Ä‘Ã£ crawl:

```sql
CREATE TABLE crawl_state (
    id SERIAL PRIMARY KEY,
    site_domain VARCHAR(255) NOT NULL,
    url TEXT NOT NULL,
    url_hash VARCHAR(64) NOT NULL,  -- MD5 hash for quick lookup
    status VARCHAR(50) DEFAULT 'active',
    first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(site_domain, url_hash)
);
```

**Benefits:**
- âœ… Track chÃ­nh xÃ¡c tá»«ng URL
- âœ… Detect duplicates real-time
- âœ… Support incremental crawling
- âœ… Resume tá»« báº¥t ká»³ Ä‘iá»ƒm nÃ o

### Layer 2: Auto-Detect Resume Point

SmartCrawler tá»± Ä‘á»™ng tÃ¬m page Ä‘á»ƒ resume theo thá»© tá»± Æ°u tiÃªn:

```python
def get_resume_page(self) -> int:
    """
    Strategy:
    1. Check crawl_jobs table for last successful page
    2. Estimate from properties count (count / 20 properties per page)
    3. Default to page 1 if no data
    """
```

**Example:**
```bash
# Láº§n cháº¡y Ä‘áº§u tiÃªn
>>> SmartCrawler('batdongsan.com.vn').get_resume_page()
ğŸ†• No previous crawl found, starting from page 1
>>> 1

# Sau khi crawl 10,000 properties
>>> SmartCrawler('batdongsan.com.vn').get_resume_page()
ğŸ“Š Estimated last page from 10000 properties: ~500
>>> 500

# Láº§n cháº¡y tiáº¿p theo
>>> SmartCrawler('batdongsan.com.vn').get_resume_page()
ğŸ“Œ Found last crawled page: 505
>>> 506  # Tá»± Ä‘á»™ng resume tá»« page tiáº¿p theo
```

### Layer 3: Smart Duplicate Detection

Má»—i URL Ä‘Æ°á»£c check trÆ°á»›c khi crawl:

```python
def is_url_crawled(self, url: str) -> bool:
    """Check database náº¿u URL Ä‘Ã£ crawl"""
    url_hash = hashlib.md5(url.encode()).hexdigest()
    # Fast lookup using hash index
    return exists_in_crawl_state(url_hash)
```

**Performance:**
- O(1) lookup vá»›i hash index
- KhÃ´ng cáº§n crawl láº¡i page Ä‘Ã£ cÃ³
- Skip duplicates tá»± Ä‘á»™ng

### Layer 4: Auto-Stop on Exhaustion

Crawler tá»± Ä‘á»™ng dá»«ng khi háº¿t data má»›i:

```python
# Stop if 10 consecutive pages are all duplicates
if consecutive_duplicates >= 10:
    print("â¹ï¸  Stopping: No new properties found")
    break
```

## Usage

### Basic Usage

```bash
# Crawl 10,000 new properties (tá»± Ä‘á»™ng resume)
python services/crawler/smart_crawler.py batdongsan.com.vn 10000
```

**Output:**
```
ğŸš€ Starting incremental crawl for batdongsan.com.vn
ğŸ¯ Target: 10000 new properties
======================================================================

ğŸ“Œ Found last crawled page: 505
â–¶ï¸  Resuming from page 506

ğŸ”§ Initializing crawler...
âœ… Crawler ready!

ğŸ“„ Crawling page 506: https://batdongsan.com.vn/nha-dat-ban/p506
   âœ… Page 506: 20 total, 20 new, 0 duplicates
ğŸ“„ Crawling page 507: https://batdongsan.com.vn/nha-dat-ban/p507
   âœ… Page 507: 20 total, 18 new, 2 duplicates
...
ğŸ“Š Progress: 100/10000 new properties
...
ğŸ‰ Target reached! 10000 new properties crawled

======================================================================
âœ… Crawl completed!
   ğŸ“„ Pages crawled: 500
   ğŸ†• New properties: 10000
   ğŸ”„ Duplicates skipped: 234
======================================================================

ğŸ“Š Total properties in database: 20000
```

### Production Cron Job

```bash
# Cháº¡y má»—i ngÃ y Ä‘á»ƒ láº¥y data má»›i
0 2 * * * cd /app && python services/crawler/smart_crawler.py batdongsan.com.vn 5000
```

**Behavior:**
- Day 1: Crawl pages 1-250 â†’ 5,000 properties
- Day 2: Auto-resume from page 251 â†’ 5,000 more
- Day 3: Auto-resume from page 501 â†’ 5,000 more
- ...

## Key Features

### 1. Zero Configuration
```python
crawler = SmartCrawler('batdongsan.com.vn')
crawler.crawl_incremental(target_properties=10000)  # Tá»± Ä‘á»™ng resume!
```

### 2. Fault Tolerant
- Database transaction per batch
- Resume tá»« Ä‘iá»ƒm dá»«ng báº¥t ká»³
- KhÃ´ng máº¥t data khi crash

### 3. Performance Optimized
- Batch insert (20 properties/transaction)
- Hash-based duplicate detection
- Indexed lookups
- Rate limiting built-in

### 4. Production Ready
- Comprehensive logging
- Progress tracking
- Error handling
- Auto-stop when exhausted

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SmartCrawler                             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Auto-Detect Resume Point                        â”‚    â”‚
â”‚  â”‚    â”œâ”€ Check crawl_jobs (last_page metadata)       â”‚    â”‚
â”‚  â”‚    â”œâ”€ Estimate from properties count              â”‚    â”‚
â”‚  â”‚    â””â”€ Default to page 1                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. Crawl Loop                                      â”‚    â”‚
â”‚  â”‚    â”œâ”€ Fetch page HTML                              â”‚    â”‚
â”‚  â”‚    â”œâ”€ Extract properties                           â”‚    â”‚
â”‚  â”‚    â””â”€ Check duplicates via crawl_state            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 3. Batch Insert                                    â”‚    â”‚
â”‚  â”‚    â”œâ”€ Insert into properties table                 â”‚    â”‚
â”‚  â”‚    â”œâ”€ Mark URL in crawl_state                      â”‚    â”‚
â”‚  â”‚    â””â”€ Update job metadata                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. Smart Stop                                      â”‚    â”‚
â”‚  â”‚    â”œâ”€ Check if target reached                      â”‚    â”‚
â”‚  â”‚    â”œâ”€ Check consecutive duplicates (â‰¥10)          â”‚    â”‚
â”‚  â”‚    â””â”€ Stop and report                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Database Tables                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ crawl_configsâ”‚   â”‚  properties  â”‚   â”‚ crawl_state  â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Site config  â”‚â”€â”€â”€â”‚ Property dataâ”‚â”€â”€â”€â”‚ URL tracking â”‚   â”‚
â”‚  â”‚ Selectors    â”‚   â”‚ Title, price â”‚   â”‚ url_hash     â”‚   â”‚
â”‚  â”‚ Pagination   â”‚   â”‚ Location     â”‚   â”‚ status       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Database Schema

### crawl_state Table
```sql
-- Track tá»«ng URL Ä‘Ã£ crawl
INSERT INTO crawl_state (site_domain, url, url_hash, status)
VALUES ('batdongsan.com.vn', 'https://...', 'abc123...', 'active')
ON CONFLICT (site_domain, url_hash)
DO UPDATE SET last_seen = CURRENT_TIMESTAMP;
```

**Indexes:**
- `idx_crawl_state_url_hash` - Fast duplicate lookup
- `idx_crawl_state_domain` - Filter by site
- `idx_crawl_state_last_seen` - Find stale URLs

## Comparison: Old vs New

### Old Approach (crawl_and_store.py)
```python
# âŒ Problems:
- Hard-coded page numbers (page 506)
- No auto-resume
- Re-crawl duplicates
- Waste bandwidth

# Example:
python crawl_from_page_506.py 506 10000  # Manual page number!
```

### New Approach (SmartCrawler)
```python
# âœ… Solutions:
- Auto-detect resume point
- Skip duplicates automatically
- Stop when no new data
- Production ready

# Example:
python services/crawler/smart_crawler.py batdongsan.com.vn 10000
# Tá»± Ä‘á»™ng resume tá»« page cuá»‘i cÃ¹ng!
```

## Migration Guide

### Step 1: Update existing crawl script
```bash
# Old way
python tests/crawl_and_store.py 10000

# New way
python services/crawler/smart_crawler.py batdongsan.com.vn 10000
```

### Step 2: Setup cron job
```bash
# Add to crontab
0 2 * * * cd /app && python services/crawler/smart_crawler.py batdongsan.com.vn 5000 >> /var/log/crawler.log 2>&1
```

### Step 3: Monitor
```bash
# Check crawl status
psql -c "SELECT site_domain, COUNT(*) FROM crawl_state GROUP BY site_domain;"

# Check last crawl
psql -c "SELECT site_domain, metadata->>'last_page' FROM crawl_jobs ORDER BY completed_at DESC LIMIT 5;"
```

## Future Enhancements

### 1. Distributed Crawling
```python
# Multiple workers crawl different sites in parallel
workers = [
    SmartCrawler('batdongsan.com.vn'),
    SmartCrawler('alonhadat.com.vn'),
    SmartCrawler('mogi.vn'),
]
await asyncio.gather(*[w.crawl_incremental(5000) for w in workers])
```

### 2. Delta Updates
```python
# Chá»‰ crawl properties updated trong 24h qua
crawler.crawl_delta(hours=24)
```

### 3. Quality Metrics
```python
# Track data quality per site
crawler.report_quality_metrics()
```

## Summary

**SmartCrawler giáº£i quyáº¿t váº¥n Ä‘á» production:**

âœ… **Auto-Resume**: KhÃ´ng cáº§n manual page number
âœ… **Duplicate Detection**: Skip URLs Ä‘Ã£ crawl
âœ… **Performance**: Hash-based lookup, batch insert
âœ… **Fault Tolerant**: Resume tá»« báº¥t ká»³ Ä‘iá»ƒm nÃ o
âœ… **Production Ready**: Logging, monitoring, error handling

**One-liner usage:**
```python
SmartCrawler('batdongsan.com.vn').crawl_incremental(10000)
```

KhÃ´ng cáº§n biáº¿t page nÃ o Ä‘Ã£ crawl - há»‡ thá»‘ng tá»± Ä‘á»™ng xá»­ lÃ½! ğŸ‰
