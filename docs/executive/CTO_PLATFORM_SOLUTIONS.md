# üéØ PLATFORM SOLUTIONS - TR·∫¢ L·ªúI S∆† ƒê·ªí CTO

> **Document n√†y B√ÅM S√ÅT 100% s∆° ƒë·ªì g·ªëc CTO** (`REE AI-architecture.drawio.xml`)
> T√¨m platform MI·ªÑN PH√ç, PH·ªî BI·∫æN, C·ªòNG ƒê·ªíNG L·ªöN ƒë·ªÉ tri·ªÉn khai

---

## üìã T√ìM T·∫ÆT NHANH

### ‚úÖ 10 Services CTO ‚Üí Platform FREE

| # | Service CTO | Platform ƒê·ªÅ Xu·∫•t | L√Ω Do | GitHub Stars |
|---|-------------|-------------------|-------|--------------|
| 1 | **Orchestrator** (routing message) | FastAPI + gRPC | Async, Python native, gRPC built-in | 72K‚≠ê |
| 2 | **Hybrid Semantic Chunking** (6 b∆∞·ªõc) | Sentence-Transformers + NLTK | HuggingFace official, ƒë√∫ng 6 b∆∞·ªõc CTO | 13K‚≠ê |
| 3 | **Attribute Extraction** (LLM-driven) | GPT-4 mini + Pydantic | Structured output JSON, validation t·ª± ƒë·ªông | GPT API |
| 4 | **Classification Service** (3 modes) | FastAPI + GPT-4 mini | Classify: filter / semantic / both | 72K‚≠ê |
| 5 | **Completeness Feedback** | GPT-4 mini | Score 0-100, re-gen if <70 | GPT API |
| 6 | **Price Suggestion** | GPT-4 mini | Market analysis + reasoning | GPT API |
| 7 | **Rerank Service** | cross-encoder (HuggingFace) | Score normalization, Top-K | HuggingFace |
| 8 | **User Account Service** | FastAPI + PostgreSQL + JWT | Auth, users, roles | Millions |
| 9 | **Core Service** (Gateway Q3) | LiteLLM + Redis | Rate limit, cost tracking, cache | 10K‚≠ê |
| 10 | **Real Estate Crawler** | Crawl4AI + Playwright | 73% √≠t code, 47% nhanh h∆°n Scrapy | 4K‚≠ê |

### ‚úÖ TR·∫¢ L·ªúI 4 C√ÇU H·ªéI CTO

| C√¢u H·ªèi | Tr·∫£ L·ªùi | Platform | Chi Ti·∫øt |
|---------|---------|----------|----------|
| **Q1:** Context Memory - OpenAI API c√≥ qu·∫£n l√Ω kh√¥ng? | **KH√îNG** ‚ùå Ph·∫£i t·ª± qu·∫£n | PostgreSQL + conversation_id | L∆∞u users, conversations, messages table |
| **Q2:** Mapping ƒë·ªÉ OpenAI hi·ªÉu request c·ªßa user n√†o? | **Orchestrator gen UUID** | FastAPI + UUID library | Gen conversation_id ‚Üí G·ª≠i m·ªçi service |
| **Q3:** C√≥ c·∫ßn Core Service t·∫≠p trung OpenAI? | **C√ì** ‚úÖ B·∫Øt bu·ªôc | LiteLLM + Redis | Rate limit, cost tracking, caching |
| **Q4:** Conversation history khi user m·ªü l·∫°i? | **Load t·ª´ PostgreSQL** | PostgreSQL + SQLAlchemy | SELECT messages WHERE conversation_id ‚Üí Inject prompt |

### üí∞ Chi Ph√≠

```
PLATFORMS: $0 (ALL FREE)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚úÖ FastAPI: $0
‚úÖ Sentence-Transformers: $0
‚úÖ Crawl4AI: $0
‚úÖ LiteLLM: $0
‚úÖ OpenSearch: $0
‚úÖ PostgreSQL: $0
‚úÖ Redis: $0
‚úÖ Pydantic: $0
‚úÖ cross-encoder: $0

ONLY COST: OpenAI API
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- GPT-4 mini: $0.15/$0.60 per 1M tokens
- Embeddings: $0.02 per 1M tokens

Development: ~$100-200/month
Production: ~$300-1000/month
```

### ‚è±Ô∏è Timeline

```
Week 1-2: Core (14 days)
  - PostgreSQL + Users schema
  - Orchestrator + conversation_id
  - Core Gateway (LiteLLM)

Week 3-4: AI Services (14 days)
  - Semantic Chunking
  - Attribute Extraction
  - Classification (3 modes)
  - Completeness Feedback
  - Price Suggestion
  - Rerank

Week 5: Data & Deploy (7 days)
  - Crawler (Crawl4AI)
  - OpenSearch
  - Docker Compose
  - Testing

TOTAL: 5 weeks
```

---

## üèóÔ∏è KI·∫æN TR√öC CHI TI·∫æT

### 1. User Account Service

**S∆° ƒë·ªì CTO:** User Account Service
**Platform:** FastAPI + PostgreSQL + JWT + bcrypt

```python
# Stack:
FastAPI           # Web framework
PostgreSQL        # User database
SQLAlchemy        # ORM
PyJWT             # JWT tokens
bcrypt            # Password hashing

# Features:
- Register (email, password)
- Login ‚Üí JWT token
- Token refresh
- User profile management
- Role-based access (admin, user)

# API Endpoints:
POST /auth/register
POST /auth/login
POST /auth/refresh
GET  /auth/me
```

**L√Ω do ch·ªçn:**
- ‚úÖ FREE (MIT License)
- ‚úÖ 72K stars GitHub
- ‚úÖ Python native (team d·ªÖ maintain)
- ‚úÖ Auto OpenAPI docs
- ‚úÖ Async performance

---

### 2. Orchestrator (Routing Service)

**S∆° ƒë·ªì CTO:** Orchestrator - routing message: create RE / search RE / price suggestion
**Platform:** FastAPI + gRPC (grpcio)

```python
# Stack:
FastAPI           # HTTP interface
grpcio            # Inter-service communication
UUID              # conversation_id generation ‚Üê Q2 ANSWER

# Routing Logic:
User Request ‚Üí Orchestrator
  ‚Üì
Gen conversation_id (UUID v4)  ‚Üê Q2 ANSWER
  ‚Üì
Route to services:
  - "create RE" ‚Üí Attribute Extraction + Semantic Chunking
  - "search RE" ‚Üí Classification ‚Üí Search + Rerank
  - "price" ‚Üí Price Suggestion

# gRPC Services:
service Orchestrator {
  rpc RouteMessage(Request) returns (Response);
  rpc CreateRE(CreateRERequest) returns (CreateREResponse);
  rpc SearchRE(SearchRERequest) returns (SearchREResponse);
}
```

**L√Ω do ch·ªçn gRPC:**
- ‚úÖ Faster than REST (binary protocol)
- ‚úÖ Built-in load balancing
- ‚úÖ Strongly typed (Protobuf)
- ‚úÖ Bi-directional streaming

---

### 3. Hybrid Semantic Chunking Service

**S∆° ƒë·ªì CTO:** Hybrid Semantic Chunking - 6 steps (Notion doc link)
**Platform:** Sentence-Transformers + NLTK + NumPy

```python
# Stack:
sentence-transformers   # Embeddings
NLTK                    # Sentence segmentation
NumPy                   # Cosine similarity
FastAPI                 # Service wrapper

# 6 Steps ƒê√öNG Y√äU C·∫¶U CTO:

Step 1: Sentence Segmentation
  ‚Üí NLTK sent_tokenize()
  ‚Üí Input: "Nh√† 3 ph√≤ng ng·ªß. Gi√° 2 t·ª∑. View ƒë·∫πp."
  ‚Üí Output: ["Nh√† 3 ph√≤ng ng·ªß.", "Gi√° 2 t·ª∑.", "View ƒë·∫πp."]

Step 2: Generate Embedding cho t·ª´ng c√¢u
  ‚Üí sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
  ‚Üí Vietnamese support
  ‚Üí Output: [embed1, embed2, embed3]

Step 3: Cosine Similarity Calculation
  ‚Üí NumPy cosine_similarity()
  ‚Üí Matrix 3x3 similarities

Step 4: Combine Sentences v·ªõi threshold >0.75
  ‚Üí If similarity(sent1, sent2) > 0.75 ‚Üí Merge
  ‚Üí "Nh√† 3 ph√≤ng ng·ªß. Gi√° 2 t·ª∑." (if similar)

Step 5: Overlap
  ‚Üí Window overlap 1-2 sentences
  ‚Üí Ensure context continuity

Step 6: Create Embedding for Whole Chunk
  ‚Üí sentence-transformers encode(merged_text)
  ‚Üí Final chunk embedding for vector DB

# API:
POST /semantic-chunking
{
  "text": "long real estate description...",
  "threshold": 0.75,
  "overlap_sentences": 1
}
‚Üí Returns: [{"chunk": "...", "embedding": [...]}]
```

**L√Ω do ch·ªçn:**
- ‚úÖ FREE (Apache 2.0)
- ‚úÖ 13K stars, HuggingFace official
- ‚úÖ ƒê√∫ng 6 b∆∞·ªõc CTO
- ‚úÖ Vietnamese support
- ‚úÖ Research paper cited 1000+ times

**Ref CTO:** https://www.notion.so/.../Chunk-size-optimation-...

---

### 4. Attribute Extraction Service (LLM-driven)

**S∆° ƒë·ªì CTO:** Attribute Extraction Service - LLM-driven
**Platform:** GPT-4 mini + Pydantic (structured output)

```python
# Stack:
OpenAI GPT-4 mini   # LLM extraction
Pydantic            # Schema validation
FastAPI             # Service wrapper

# JSON Schema (Pydantic):
class RealEstateAttributes(BaseModel):
    price: Optional[float] = None
    price_unit: str = "VND"
    location: str
    district: Optional[str] = None
    city: str
    property_type: str  # "apartment", "house", "land"
    bedrooms: Optional[int] = None
    bathrooms: Optional[int] = None
    area: Optional[float] = None
    area_unit: str = "m2"
    floor: Optional[int] = None
    direction: Optional[str] = None  # "ƒê√¥ng", "T√¢y"...
    legal_status: Optional[str] = None  # "S·ªï ƒë·ªè", "S·ªï h·ªìng"
    furniture: Optional[str] = None
    description: str

# GPT-4 Prompt:
system_prompt = """
B·∫°n l√† chuy√™n gia b·∫•t ƒë·ªông s·∫£n. Tr√≠ch xu·∫•t th√¥ng tin t·ª´ text th√†nh JSON.
Schema: {RealEstateAttributes.schema_json()}
"""

user_prompt = """
Text: {input_text}
Extract to JSON following schema.
"""

# API:
POST /attribute-extraction
{
  "text": "B√°n nh√† 3 ph√≤ng ng·ªß, 2WC, DT 80m2, gi√° 2 t·ª∑, Qu·∫≠n 1 HCM"
}
‚Üí Returns:
{
  "price": 2000000000,
  "location": "Qu·∫≠n 1, HCM",
  "bedrooms": 3,
  "bathrooms": 2,
  "area": 80.0,
  ...
}
```

**L√Ω do ch·ªçn:**
- ‚úÖ GPT-4 mini r·∫ª ($0.15 input / $0.60 output per 1M tokens)
- ‚úÖ Pydantic FREE + auto validation
- ‚úÖ Structured output = reliable
- ‚úÖ Vietnamese support

---

### 5. Classification Service (3 Modes CTO)

**S∆° ƒë·ªì CTO:** Classification Service ‚Üí 3 modes: filter / semantic / both
**Platform:** FastAPI + GPT-4 mini

```python
# Stack:
GPT-4 mini          # Query classification
FastAPI             # Service wrapper

# 3 Modes THEO CTO:

Mode 1: FILTER (Structured Filtering)
  ‚Üí Query c√≥ structured attributes r√µ r√†ng
  ‚Üí Example: "Nh√† 3 ph√≤ng ng·ªß, gi√° d∆∞·ªõi 2 t·ª∑, Qu·∫≠n 1"
  ‚Üí Route: SQL WHERE bedrooms=3 AND price<2000000000 AND district='Qu·∫≠n 1'

Mode 2: SEMANTIC (Vector Search)
  ‚Üí Query m∆° h·ªì, semantic
  ‚Üí Example: "T√¨m nh√† view ƒë·∫πp, y√™n tƒ©nh, g·∫ßn c√¥ng vi√™n"
  ‚Üí Route: OpenSearch vector search v·ªõi embedding

Mode 3: BOTH (Hybrid Retrieval)
  ‚Üí Query k·∫øt h·ª£p structured + semantic
  ‚Üí Example: "Nh√† 3 ph√≤ng ng·ªß, view s√¥ng, y√™n tƒ©nh"
  ‚Üí Route: Hybrid (filter bedrooms=3 + semantic "view s√¥ng y√™n tƒ©nh")

# GPT-4 Prompt:
system_prompt = """
Classify query into 3 modes:
1. filter: Has clear structured attributes (price, bedrooms, location)
2. semantic: Vague, descriptive (beautiful, quiet, modern)
3. both: Mix of structured + semantic

Return JSON: {"mode": "filter|semantic|both", "reasoning": "..."}
"""

# API:
POST /classification
{
  "query": "Nh√† 3 ph√≤ng ng·ªß view ƒë·∫πp Qu·∫≠n 1"
}
‚Üí Returns:
{
  "mode": "both",
  "structured": {"bedrooms": 3, "district": "Qu·∫≠n 1"},
  "semantic": "view ƒë·∫πp"
}
```

**L√Ω do ch·ªçn:**
- ‚úÖ GPT-4 mini classification ch√≠nh x√°c
- ‚úÖ 3 modes ƒë√∫ng y√™u c·∫ßu CTO
- ‚úÖ Flexible routing

---

### 6. Completeness Feedback Service

**S∆° ƒë·ªì CTO:** Completeness Feedback Service
**Platform:** GPT-4 mini (completeness evaluation)

```python
# Stack:
GPT-4 mini          # Evaluate response completeness
FastAPI             # Service wrapper

# Logic:
1. User query ‚Üí System generates response
2. Send (query + response) to Completeness Service
3. GPT-4 scores 0-100
4. If score < 70 ‚Üí Trigger re-generation with feedback

# GPT-4 Prompt:
system_prompt = """
B·∫°n l√† chuy√™n gia QA. ƒê√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa c√¢u tr·∫£ l·ªùi.

Criteria:
- Tr·∫£ l·ªùi ƒë√∫ng c√¢u h·ªèi? (40 points)
- ƒê·∫ßy ƒë·ªß th√¥ng tin? (30 points)
- Ch√≠nh x√°c? (20 points)
- Clear & concise? (10 points)

Return JSON:
{
  "score": 0-100,
  "missing": ["what's missing"],
  "suggestion": "how to improve"
}
"""

# API:
POST /completeness-feedback
{
  "query": "T√¨m nh√† 3 ph√≤ng ng·ªß Qu·∫≠n 1",
  "response": "C√≥ 5 cƒÉn nh√† ph√π h·ª£p: ..."
}
‚Üí Returns:
{
  "score": 85,
  "is_complete": true,  // score >= 70
  "missing": [],
  "suggestion": "Good!"
}

# Orchestrator Logic:
if score < 70:
    re_generate_with_feedback(missing, suggestion)
```

**L√Ω do ch·ªçn:**
- ‚úÖ GPT-4 mini t·ªët nh·∫•t cho reasoning
- ‚úÖ Quality control t·ª± ƒë·ªông
- ‚úÖ Feedback loop c·∫£i thi·ªán response

---

### 7. Price Suggestion Service

**S∆° ƒë·ªì CTO:** Price Suggestion Service
**Platform:** GPT-4 mini + Market Data

```python
# Stack:
GPT-4 mini          # Price reasoning
OpenSearch          # Similar properties
FastAPI             # Service wrapper

# Logic:
1. Get property attributes (from Attribute Extraction)
2. Search similar properties in OpenSearch
3. Send (attributes + similar_properties) to GPT-4
4. GPT-4 analyzes market ‚Üí Suggest price range

# GPT-4 Prompt:
system_prompt = """
B·∫°n l√† chuy√™n gia ƒë·ªãnh gi√° BƒêS. D·ª±a v√†o:
1. Property attributes
2. Similar properties ƒë√£ b√°n
3. Market trends

Suggest gi√° h·ª£p l√Ω v·ªõi reasoning.

Return JSON:
{
  "suggested_price_min": float,
  "suggested_price_max": float,
  "reasoning": "...",
  "market_trend": "up|stable|down",
  "confidence": 0-1
}
"""

# API:
POST /price-suggestion
{
  "attributes": {
    "bedrooms": 3,
    "area": 80,
    "location": "Qu·∫≠n 1"
  }
}
‚Üí Returns:
{
  "suggested_price_min": 1800000000,
  "suggested_price_max": 2200000000,
  "reasoning": "Based on 15 similar properties...",
  "confidence": 0.85
}
```

**L√Ω do ch·ªçn:**
- ‚úÖ GPT-4 mini c√≥ market reasoning t·ªët
- ‚úÖ K·∫øt h·ª£p similar properties = ch√≠nh x√°c

---

### 8. Rerank Service

**S∆° ƒë·ªì CTO:** Rerank Service
**Platform:** cross-encoder (HuggingFace)

```python
# Stack:
sentence-transformers   # cross-encoder model
FastAPI                 # Service wrapper

# Model:
cross-encoder/ms-marco-MiniLM-L-6-v2
  ‚Üí Trained for semantic similarity ranking
  ‚Üí Vietnamese support via multilingual

# Logic:
1. OpenSearch returns top 50 results
2. cross-encoder re-scores (query, doc) pairs
3. Sort by score ‚Üí Return top 10

# Code:
from sentence_transformers import CrossEncoder

model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

pairs = [(query, doc) for doc in search_results]
scores = model.predict(pairs)

# Sort and return top-K
ranked = sorted(zip(search_results, scores),
                key=lambda x: x[1], reverse=True)[:10]

# API:
POST /rerank
{
  "query": "Nh√† view ƒë·∫πp Qu·∫≠n 1",
  "candidates": [
    {"id": 1, "text": "..."},
    {"id": 2, "text": "..."}
  ],
  "top_k": 10
}
‚Üí Returns: [{"id": 5, "score": 0.92}, ...]
```

**L√Ω do ch·ªçn:**
- ‚úÖ FREE (HuggingFace)
- ‚úÖ Proven research (MS MARCO dataset)
- ‚úÖ Better than vector similarity alone

---

### 9. Core Service (OpenAI Gateway) - Q3 ANSWER

**S∆° ƒë·ªì CTO:** Q3 - C√≥ c·∫ßn Core Service t·∫≠p trung request l√™n OpenAI?
**C√¢u tr·∫£ l·ªùi:** **C√ì** ‚úÖ B·∫Øt bu·ªôc
**Platform:** LiteLLM + Redis + FastAPI

```python
# Stack:
LiteLLM             # Universal LLM gateway
Redis               # Cache + rate limiting
FastAPI             # Wrapper service

# Features:

1. RATE LIMITING (protect API key)
   ‚Üí Redis-based token bucket
   ‚Üí 1000 requests/user/hour

2. COST TRACKING (per user/conversation)
   ‚Üí Log: user_id, conversation_id, tokens, cost
   ‚Üí PostgreSQL analytics table

3. RESPONSE CACHING (Redis)
   ‚Üí Cache key: hash(model + prompt)
   ‚Üí TTL: 1 hour
   ‚Üí Save ~30% API cost

4. CENTRALIZED MONITORING
   ‚Üí All OpenAI requests go through gateway
   ‚Üí Track: latency, errors, usage

# Code:
from litellm import completion

async def call_gpt(prompt, user_id, conversation_id):
    # 1. Check rate limit
    if not check_rate_limit(user_id):
        raise RateLimitError()

    # 2. Check cache
    cache_key = hash(prompt)
    cached = redis.get(cache_key)
    if cached:
        return cached

    # 3. Call OpenAI via LiteLLM
    response = await completion(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    # 4. Track cost
    cost = calculate_cost(response.usage)
    log_usage(user_id, conversation_id, cost)

    # 5. Cache response
    redis.setex(cache_key, 3600, response)

    return response

# API:
POST /gateway/chat
{
  "user_id": "uuid",
  "conversation_id": "uuid",  ‚Üê Q2 mapping
  "messages": [...]
}
‚Üí Returns: GPT response + cost info
```

**L√Ω do ch·ªçn LiteLLM:**
- ‚úÖ FREE (MIT License) 10K‚≠ê
- ‚úÖ Unified API (OpenAI, Anthropic, Cohere...)
- ‚úÖ Built-in: rate limit, retry, fallback
- ‚úÖ Cost tracking built-in
- ‚úÖ Easy migration to other LLMs

**Tr·∫£ l·ªùi Q3 CTO:** ‚úÖ C√ì c·∫ßn Core Service - B·∫ÆTT BU·ªòC ƒë·ªÉ:
- Protect API key (rate limiting)
- Track cost per user
- Cache expensive calls
- Centralized monitoring
- **Model routing (Ollama vs OpenAI)** ‚Üê M·ªöI

---

### 9.1. Model Routing Strategy - Ollama vs OpenAI

**Platform:** LiteLLM h·ªó tr·ª£ c·∫£ Ollama v√† OpenAI
**M·ª•c ƒë√≠ch:** Ti·∫øt ki·ªám chi ph√≠ b·∫±ng c√°ch d√πng Ollama (FREE) cho tasks ƒë∆°n gi·∫£n

#### üìä Ph√¢n Lu·ªìng Model:

| Task | Complexity | Model | Cost | L√Ω Do |
|------|-----------|-------|------|-------|
| **Attribute Extraction** | Medium | **Ollama (llama3.1:8b)** | $0 | Structured extraction, schema r√µ r√†ng |
| **Classification (3 modes)** | Low | **Ollama (llama3.1:8b)** | $0 | Simple classification task |
| **Completeness Feedback** | High | **OpenAI (GPT-4 mini)** | $$ | C·∫ßn reasoning t·ªët |
| **Price Suggestion** | High | **OpenAI (GPT-4 mini)** | $$ | Market analysis ph·ª©c t·∫°p |
| **Semantic Chunking** | N/A | **Sentence-Transformers** | $0 | Kh√¥ng d√πng LLM |
| **Rerank** | N/A | **cross-encoder** | $0 | Kh√¥ng d√πng LLM |

#### üéØ Chi·∫øn L∆∞·ª£c:

```python
# Core Gateway - Model Router

TASK_MODEL_MAP = {
    "attribute_extraction": {
        "provider": "ollama",
        "model": "llama3.1:8b",
        "cost_per_1m_tokens": 0.0,
        "fallback": "gpt-4o-mini"  # N·∫øu Ollama fail
    },
    "classification": {
        "provider": "ollama",
        "model": "llama3.1:8b",
        "cost_per_1m_tokens": 0.0,
        "fallback": "gpt-4o-mini"
    },
    "completeness_feedback": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "cost_per_1m_tokens": 0.15,  # input
        "fallback": "ollama/llama3.1:8b"
    },
    "price_suggestion": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "cost_per_1m_tokens": 0.15,
        "fallback": "ollama/llama3.1:70b"  # Larger Ollama model
    },
    "chat": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "cost_per_1m_tokens": 0.15,
        "fallback": "ollama/llama3.1:8b"
    }
}

async def call_llm(task: str, prompt: str, user_id: str):
    config = TASK_MODEL_MAP.get(task)

    # Try primary model
    try:
        if config["provider"] == "ollama":
            response = await completion(
                model=f"ollama/{config['model']}",
                messages=[{"role": "user", "content": prompt}],
                api_base="http://ollama:11434"
            )
        else:  # openai
            response = await completion(
                model=config["model"],
                messages=[{"role": "user", "content": prompt}]
            )

        return response

    except Exception as e:
        # Fallback to secondary model
        logger.warning(f"Primary model failed: {e}, using fallback")
        response = await completion(
            model=config["fallback"],
            messages=[{"role": "user", "content": prompt}]
        )
        return response
```

#### üêã Docker Compose - Th√™m Ollama:

```yaml
services:
  # ... existing services ...

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]  # Optional: GPU support
    command: |
      sh -c "ollama serve &
             sleep 5 &&
             ollama pull llama3.1:8b &&
             ollama pull llama3.1:70b &&
             wait"

  core_gateway:
    build: ./services/core_gateway
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OLLAMA_BASE_URL=http://ollama:11434
      - MODEL_ROUTING_ENABLED=true
    depends_on:
      - ollama
      - redis

volumes:
  ollama_models:
```

#### üí∞ Cost Comparison:

```
SCENARIO: 1M tokens/month

Option 1: ALL OpenAI GPT-4 mini
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Attribute Extraction: 200K tokens √ó $0.15 = $30
Classification:       100K tokens √ó $0.15 = $15
Completeness:         300K tokens √ó $0.60 = $180 (output heavy)
Price Suggestion:     400K tokens √ó $0.60 = $240
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL: $465/month

Option 2: Hybrid (Ollama + OpenAI)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Attribute Extraction: 200K tokens √ó $0    = $0   ‚Üê Ollama
Classification:       100K tokens √ó $0    = $0   ‚Üê Ollama
Completeness:         300K tokens √ó $0.60 = $180 ‚Üê OpenAI
Price Suggestion:     400K tokens √ó $0.60 = $240 ‚Üê OpenAI
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL: $420/month

SAVINGS: $45/month (10% reduction)

+ Ollama server cost: $20/month (DigitalOcean 8GB RAM)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
NET SAVINGS: $25/month

At scale (10M tokens/month):
- Option 1: $4,650/month
- Option 2: $4,220/month
SAVINGS: $430/month
```

#### üéØ Quality vs Cost Trade-off:

```python
# A/B Testing Strategy

# Week 1-2: Baseline (ALL OpenAI)
config = {"default_provider": "openai"}

# Week 3-4: Hybrid Test
config = {
    "attribute_extraction": "ollama",  # Test Ollama
    "classification": "ollama",
    "others": "openai"
}

# Metrics to track:
metrics = {
    "extraction_accuracy": 0.95,  # Target: >90%
    "classification_accuracy": 0.92,  # Target: >85%
    "user_satisfaction": 4.2,  # Target: >4.0/5
    "cost_savings": 0.10  # 10% reduction
}

# Decision criteria:
if extraction_accuracy < 0.90:
    # Rollback to OpenAI
    switch_to_openai("attribute_extraction")
else:
    # Keep Ollama (save money)
    pass
```

#### üìã Model Selection Guide:

**D√πng Ollama khi:**
- ‚úÖ Structured output (JSON schema)
- ‚úÖ Simple classification (2-5 classes)
- ‚úÖ Template-based generation
- ‚úÖ Low latency required (self-hosted = faster)
- ‚úÖ Privacy concerns (data kh√¥ng ra kh·ªèi server)

**D√πng OpenAI khi:**
- ‚úÖ Complex reasoning (price suggestion, market analysis)
- ‚úÖ Creative generation
- ‚úÖ Multi-step logic
- ‚úÖ High accuracy critical (completeness feedback)
- ‚úÖ Vietnamese nuances important

#### üöÄ LiteLLM Router Config:

```python
# LiteLLM supports automatic routing

from litellm import Router

router = Router(
    model_list=[
        {
            "model_name": "cheap-model",
            "litellm_params": {
                "model": "ollama/llama3.1:8b",
                "api_base": "http://ollama:11434"
            }
        },
        {
            "model_name": "smart-model",
            "litellm_params": {
                "model": "gpt-4o-mini"
            }
        }
    ],
    routing_strategy="cost-based",  # Route to cheapest first
    fallbacks=[
        {"cheap-model": ["smart-model"]}  # Fallback if Ollama fails
    ]
)

# Usage:
response = await router.acompletion(
    model="cheap-model",  # Try Ollama first
    messages=[...]
)
```

#### üîß Implementation Steps:

```bash
Week 1: Infrastructure
  1. Deploy Ollama container
  2. Pull llama3.1:8b, llama3.1:70b models
  3. Test Ollama API connectivity

Week 2: Integration
  4. Update Core Gateway with routing logic
  5. Implement fallback mechanism
  6. Add cost tracking per model

Week 3-4: Testing
  7. A/B test: Attribute Extraction (Ollama vs OpenAI)
  8. Measure: accuracy, latency, cost
  9. Tune prompts for Ollama if needed

Week 5: Rollout
  10. Enable Ollama for attribute_extraction
  11. Enable Ollama for classification
  12. Monitor quality metrics
  13. Adjust routing if quality drops
```

#### üìä Monitoring Dashboard:

```python
# Track per-model metrics

SELECT
    task,
    provider,
    model,
    COUNT(*) as requests,
    AVG(tokens) as avg_tokens,
    SUM(cost) as total_cost,
    AVG(latency_ms) as avg_latency,
    SUM(CASE WHEN error THEN 1 ELSE 0 END) as errors
FROM llm_usage_logs
WHERE created_at > NOW() - INTERVAL '7 days'
GROUP BY task, provider, model
ORDER BY total_cost DESC;

Example output:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ task                ‚îÇ provider ‚îÇ model        ‚îÇ requests ‚îÇ cost   ‚îÇ errors‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ price_suggestion    ‚îÇ openai   ‚îÇ gpt-4o-mini  ‚îÇ 50,000   ‚îÇ $150   ‚îÇ 12    ‚îÇ
‚îÇ completeness        ‚îÇ openai   ‚îÇ gpt-4o-mini  ‚îÇ 80,000   ‚îÇ $120   ‚îÇ 8     ‚îÇ
‚îÇ attribute_extract   ‚îÇ ollama   ‚îÇ llama3.1:8b  ‚îÇ 100,000  ‚îÇ $0     ‚îÇ 45    ‚îÇ
‚îÇ classification      ‚îÇ ollama   ‚îÇ llama3.1:8b  ‚îÇ 60,000   ‚îÇ $0     ‚îÇ 20    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Alert if:
- Ollama error rate > 5% ‚Üí Switch to OpenAI
- OpenAI cost > $500/week ‚Üí Expand Ollama usage
```

---

### 10. Real Estate Crawler

**S∆° ƒë·ªì CTO:** Real Estate Crawler
**Platform:** Crawl4AI + Playwright

```python
# Stack:
Crawl4AI            # AI-optimized crawler
Playwright          # JS rendering
Celery Beat         # Scheduling (every 6h)
FastAPI             # API wrapper

# Crawl Flow:
1. Crawl nhatot.vn, batdongsan.vn
2. JS render (Playwright)
3. Auto clean HTML (remove ads, scripts)
4. Extract LLM-friendly markdown
5. Send to Semantic Chunking ‚Üí Attribute Extraction
6. Index to OpenSearch

# Code:
from crawl4ai import WebCrawler

crawler = WebCrawler(
    headless=True,
    browser_type="chromium",
    markdown_generator=LLMFriendlyMarkdown()
)

# Crawl property listing
result = await crawler.arun(
    url="https://nhatot.vn/mua-ban-bat-dong-san",
    css_selector=".property-item",
    extraction_strategy="JsonCssExtractionStrategy",
    schema={
        "name": "title.text",
        "price": ".price.text",
        "location": ".location.text",
        "description": ".description.text"
    }
)

# Celery Task:
@celery_app.task
def crawl_properties():
    sites = ["nhatot.vn", "batdongsan.vn", "alonhadat.com.vn"]
    for site in sites:
        properties = await crawl_site(site)
        for prop in properties:
            # Send to processing pipeline
            process_property.delay(prop)

# Schedule (every 6 hours):
celery_beat_schedule = {
    'crawl-properties': {
        'task': 'crawl_properties',
        'schedule': crontab(minute=0, hour='*/6'),
    }
}
```

**L√Ω do ch·ªçn Crawl4AI (thay v√¨ Scrapy):**
- ‚úÖ 73% √≠t code h∆°n Scrapy
- ‚úÖ 47% nhanh h∆°n
- ‚úÖ LLM-friendly markdown built-in
- ‚úÖ Auto clean HTML (no manual parsing)
- ‚úÖ Playwright JS rendering built-in
- ‚úÖ 4K stars (hot, growing community)

---

### 11. Storage Layer

#### A. PostgreSQL (Context Memory - Q1, Q4)

**S∆° ƒë·ªì CTO:** Q1 - Context Memory: OpenAI c√≥ qu·∫£n l√Ω kh√¥ng?
**C√¢u tr·∫£ l·ªùi:** **KH√îNG** ‚ùå Ph·∫£i t·ª± qu·∫£n PostgreSQL
**Platform:** PostgreSQL + SQLAlchemy

```sql
-- Q1 ANSWER: PostgreSQL Schema

-- Users table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    full_name VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Conversations table ‚Üê Q1, Q4
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),  ‚Üê Q2 mapping
    user_id UUID REFERENCES users(id),
    title VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Messages table ‚Üê Q4 (history loading)
CREATE TABLE messages (
    id BIGSERIAL PRIMARY KEY,
    conversation_id UUID REFERENCES conversations(id),
    role VARCHAR(50),  -- 'user', 'assistant', 'system'
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Q4: Load history when user opens conversation
SELECT role, content
FROM messages
WHERE conversation_id = $1
ORDER BY created_at ASC;

-- Inject into GPT prompt:
messages = [
    {"role": "system", "content": "You are RE assistant"},
    *[{"role": msg.role, "content": msg.content} for msg in history],
    {"role": "user", "content": new_query}
]
```

**Tr·∫£ l·ªùi Q1 CTO:** ‚ùå OpenAI API KH√îNG c√≥ context memory built-in
**Tr·∫£ l·ªùi Q4 CTO:** ‚úÖ Load t·ª´ PostgreSQL messages table ‚Üí Inject v√†o prompt

#### B. OpenSearch (Vector DB + BM25)

```python
# Index Schema:
{
    "properties": {
        "id": {"type": "keyword"},
        "title": {"type": "text", "analyzer": "vietnamese"},
        "description": {"type": "text", "analyzer": "vietnamese"},
        "price": {"type": "float"},
        "location": {"type": "text"},
        "bedrooms": {"type": "integer"},
        "area": {"type": "float"},
        "embedding": {
            "type": "dense_vector",
            "dims": 384  # sentence-transformers output
        }
    }
}

# Hybrid Search (vector + BM25):
{
    "query": {
        "bool": {
            "must": [
                # BM25 keyword search
                {"match": {"description": "view ƒë·∫πp"}},

                # Structured filter
                {"range": {"price": {"lte": 2000000000}}},
                {"term": {"bedrooms": 3}}
            ],
            "should": [
                # Vector similarity search
                {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                            "params": {"query_vector": embedding}
                        }
                    }
                }
            ]
        }
    }
}
```

#### C. Redis (Cache + Queue)

```python
# Use cases:
1. Response caching (Core Gateway)
2. Rate limiting (token bucket)
3. Session management
4. Celery task queue (crawling)

# Example:
redis.setex(f"rate_limit:{user_id}", 3600, 1000)  # 1000 req/hour
redis.get(f"cache:{hash(prompt)}")
```

---

## üì¶ DOCKER COMPOSE

```yaml
version: '3.8'

services:
  # --- Databases ---
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: ree_ai
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: secret
    volumes:
      - postgres_data:/var/lib/postgresql/data

  opensearch:
    image: opensearchproject/opensearch:2.11.0
    environment:
      - discovery.type=single-node
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    volumes:
      - opensearch_data:/usr/share/opensearch/data

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

  # --- Services (FastAPI) ---
  orchestrator:
    build: ./services/orchestrator
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://admin:secret@postgres/ree_ai
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis

  semantic_chunking:
    build: ./services/semantic_chunking
    ports:
      - "8001:8000"

  attribute_extraction:
    build: ./services/attribute_extraction
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}

  classification:
    build: ./services/classification
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}

  completeness_feedback:
    build: ./services/completeness_feedback
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}

  price_suggestion:
    build: ./services/price_suggestion
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}

  rerank:
    build: ./services/rerank

  user_account:
    build: ./services/user_account
    environment:
      - DATABASE_URL=postgresql://admin:secret@postgres/ree_ai
      - JWT_SECRET=${JWT_SECRET}

  core_gateway:
    build: ./services/core_gateway
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis

  crawler:
    build: ./services/crawler
    environment:
      - DATABASE_URL=postgresql://admin:secret@postgres/ree_ai
      - OPENSEARCH_URL=http://opensearch:9200

  # --- Worker ---
  celery_worker:
    build: ./services/crawler
    command: celery -A tasks worker --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379

  celery_beat:
    build: ./services/crawler
    command: celery -A tasks beat --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379

volumes:
  postgres_data:
  opensearch_data:
  redis_data:
```

---

## üìä SO S√ÅNH: S∆† ƒê·ªí CTO vs PLATFORM ƒê·ªÄ XU·∫§T

| Service CTO | C√≥ trong platform? | Platform | Thay ƒë·ªïi g√¨? |
|-------------|-------------------|----------|--------------|
| ‚úÖ Orchestrator | ‚úÖ | FastAPI + gRPC | ƒê√∫ng 100% |
| ‚úÖ Hybrid Semantic Chunking | ‚úÖ | Sentence-Transformers | ƒê√∫ng 6 b∆∞·ªõc |
| ‚úÖ Attribute Extraction | ‚úÖ | GPT-4 mini + Pydantic | ƒê√∫ng LLM-driven |
| ‚úÖ Classification (3 modes) | ‚úÖ | FastAPI + GPT-4 | ƒê√∫ng filter/semantic/both |
| ‚úÖ Completeness Feedback | ‚úÖ | GPT-4 mini | ƒê√∫ng |
| ‚úÖ Price Suggestion | ‚úÖ | GPT-4 mini | Thay "3rd pricing service" = GPT (t·ªët h∆°n) |
| ‚úÖ Rerank | ‚úÖ | cross-encoder | ƒê√∫ng |
| ‚úÖ User Account | ‚úÖ | FastAPI + PostgreSQL | ƒê√∫ng |
| ‚úÖ Core/Gateway (Q3) | ‚úÖ | LiteLLM + Redis | ƒê√∫ng, TR·∫¢ L·ªúI Q3 |
| ‚úÖ Crawler | ‚úÖ | Crawl4AI | Thay Scrapy (t·ªët h∆°n) |
| ‚úÖ Context Memory (Q1, Q4) | ‚úÖ | PostgreSQL | ƒê√∫ng, TR·∫¢ L·ªúI Q1, Q4 |
| ‚úÖ conversation_id mapping (Q2) | ‚úÖ | UUID trong Orchestrator | ƒê√∫ng, TR·∫¢ L·ªúI Q2 |

### ‚ö†Ô∏è Kh√°c Bi·ªát:

1. **Price Suggestion:**
   - CTO: "3rd pricing service" (external cloud service)
   - Platform: GPT-4 mini + market data
   - **L√Ω do:** GPT-4 t·ªët h∆°n, flexible, kh√¥ng ph·ª• thu·ªôc external

2. **Crawler:**
   - CTO: Kh√¥ng ch·ªâ ƒë·ªãnh (c√≥ th·ªÉ Scrapy)
   - Platform: Crawl4AI
   - **L√Ω do:** 73% √≠t code, 47% nhanh h∆°n Scrapy

3. **Kh√¥ng c√≥ Open WebUI, LangChain:**
   - CTO kh√¥ng ƒë·ªÅ c·∫≠p ‚Üí Kh√¥ng d√πng
   - Platform: Microservices thu·∫ßn (FastAPI)

---

## ‚úÖ K·∫æT LU·∫¨N

### ƒê√É GI·∫¢I QUY·∫æT:

‚úÖ **10 Services CTO** ‚Üí T·∫•t c·∫£ c√≥ platform FREE, ph·ªï bi·∫øn
‚úÖ **4 C√¢u h·ªèi CTO** ‚Üí ƒê√£ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß (Q1, Q2, Q3, Q4)
‚úÖ **Chi ph√≠** ‚Üí $0 tools, ch·ªâ $100-300/month OpenAI API
‚úÖ **Timeline** ‚Üí 5 tu·∫ßn (realistic)
‚úÖ **C·ªông ƒë·ªìng** ‚Üí T·∫•t c·∫£ platform c√≥ 4K-72K stars GitHub

### KHUY·∫æN NGH·ªä:

1. **Review s∆° ƒë·ªì CTO l·∫ßn cu·ªëi** ƒë·ªÉ confirm
2. **B·∫Øt ƒë·∫ßu Week 1-2:** PostgreSQL + Orchestrator + Core Gateway
3. **Song song:** Setup Sentence-Transformers test v·ªõi Vietnamese text
4. **Monitoring:** Prometheus + Grafana (optional nh∆∞ng n√™n c√≥)

---

**Document n√†y l√† ƒë·ªÅ xu·∫•t platform - c·∫ßn CTO approve tr∆∞·ªõc khi implement.**

Generated: 2025-10-29
Version: 1.0
Status: ‚úÖ Ready for CTO review
