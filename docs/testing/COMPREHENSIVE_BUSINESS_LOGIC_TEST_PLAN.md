# üéØ Comprehensive Business Logic Test Plan
## REE AI Platform - CTO Requirements Validation

**Date:** 2025-11-01
**Status:** üöß In Progress
**Data Available:** 13,448 Vietnam real estate properties

---

## üìã Executive Summary

### Test Objectives
1. ‚úÖ **Validate CTO Requirements** - Test all 10 services against CTO diagram
2. ‚úÖ **Verify Business Logic** - Test 8 intent types with real data
3. ‚úÖ **Improve Prompts** - Optimize for Vietnamese real estate domain
4. ‚úÖ **End-to-End Testing** - Full user journey from query to response

### Current Status
- **Architecture:** 7-layer implementation (CTO diagram)
- **Services:** 10/10 mapped to platforms
- **Data:** 13,448 properties from Batdongsan.com.vn
- **Prompts:** Need alignment with CTO requirements

---

## üé® CTO Requirements vs Current Implementation

### Gap Analysis

| CTO Requirement | Current Implementation | Status | Gap |
|----------------|------------------------|--------|-----|
| **10 Services** | 10 services implemented | ‚úÖ | None |
| **Intent Types** | 8 intents in code vs prompts | ‚ö†Ô∏è | **MISMATCH** |
| **Vietnamese Support** | Prompts have Vietnamese | ‚úÖ | None |
| **RAG Pipeline** | Implemented but untested | ‚ö†Ô∏è | **Need testing** |
| **Context Memory (Q1)** | PostgreSQL + Open WebUI | ‚úÖ | None |
| **Conversation ID (Q2)** | UUID generation | ‚úÖ | None |
| **Core Gateway (Q3)** | LiteLLM implemented | ‚úÖ | None |
| **History Loading (Q4)** | LangChain Memory | ‚úÖ | None |

### Critical Gap Found! üö®

**orchestrator/main.py Intent Types:**
```python
SEARCH, CHAT, CLASSIFY, EXTRACT, PRICE_SUGGEST, COMPARE, RECOMMEND, UNKNOWN
```

**orchestrator/prompts.py Intent Types:**
```python
SEARCH, COMPARE, PRICE_ANALYSIS, INVESTMENT_ADVICE, LOCATION_INSIGHTS,
LEGAL_GUIDANCE, CHAT, UNKNOWN
```

**‚ö†Ô∏è Mismatch between code and prompts!**

---

## üß™ Test Plan - 8 Intent Types

### 1. SEARCH Intent (CTO Service #8: RAG)

**Business Logic:**
- User searches for properties with specific criteria
- System uses RAG (Vector + BM25) to find matching properties
- Returns top 10 results ranked by relevance

**Test Cases:**

```python
# TC1.1: Simple search - Location only
Query: "T√¨m cƒÉn h·ªô Qu·∫≠n 7"
Expected Intent: SEARCH
Expected Entities: {
    "property_type": "cƒÉn h·ªô",
    "location": "Qu·∫≠n 7"
}
Expected Service: rag_service
Expected Response: List of apartments in District 7

# TC1.2: Complex search - Multiple criteria
Query: "T√¨m nh√† 2 ph√≤ng ng·ªß Qu·∫≠n 2 d∆∞·ªõi 3 t·ª∑ g·∫ßn metro"
Expected Intent: SEARCH
Expected Entities: {
    "bedrooms": 2,
    "location": "Qu·∫≠n 2",
    "price_range": {"max": 3000000000},
    "amenities": ["metro"]
}
Expected Service: rag_service
Expected Response: Filtered results matching all criteria

# TC1.3: Search with Vietnamese variations
Queries to test:
- "C√≥ nh√† n√†o gi√° r·∫ª kh√¥ng?" ‚Üí SEARCH
- "C·∫ßn mua cƒÉn h·ªô g·∫•p" ‚Üí SEARCH
- "Find cheap apartments" ‚Üí SEARCH (English)
```

**Success Criteria:**
- ‚úÖ Intent detection accuracy ‚â• 90%
- ‚úÖ Entity extraction accuracy ‚â• 85%
- ‚úÖ RAG returns relevant results (P@10 ‚â• 0.7)
- ‚úÖ Response time < 2 seconds

---

### 2. COMPARE Intent (CTO Service #8: RAG + Analysis)

**Business Logic:**
- User wants to compare 2+ properties
- System retrieves properties from RAG
- LLM analyzes and creates comparison table

**Test Cases:**

```python
# TC2.1: Compare by IDs
Query: "So s√°nh cƒÉn h·ªô #123 v√† #456"
Expected Intent: COMPARE
Expected Entities: {
    "property_ids": ["123", "456"]
}
Expected Service: rag_service ‚Üí comparison_chain
Expected Response: Comparison table with pros/cons

# TC2.2: Compare by implicit reference
Query: "So s√°nh 2 cƒÉn h·ªô Vinhomes Grand Park"
Expected Intent: COMPARE
Expected Entities: {
    "count": 2,
    "location": "Vinhomes Grand Park"
}
Expected Service: rag_service (get top 2) ‚Üí comparison
Expected Response: Side-by-side comparison

# TC2.3: Multi-turn conversation context
Turn 1: "T√¨m cƒÉn h·ªô Q7" ‚Üí Returns results
Turn 2: "So s√°nh 2 cƒÉn ƒë·∫ßu" ‚Üí COMPARE using context
Expected: Use conversation history to identify properties
```

**Success Criteria:**
- ‚úÖ Comparison includes: price, size, location, pros/cons
- ‚úÖ Structured output (table format)
- ‚úÖ Recommendations based on comparison
- ‚úÖ Works with conversation context

---

### 3. PRICE_ANALYSIS Intent (CTO Service #7: Price Suggestion)

**Business Logic:**
- User asks if a price is reasonable
- System analyzes against market data (13K properties)
- Provides price recommendation with reasoning

**Test Cases:**

```python
# TC3.1: Price reasonability check
Query: "Gi√° 2.5 t·ª∑ cho cƒÉn h·ªô 70m¬≤ Q7 c√≥ h·ª£p l√Ω kh√¥ng?"
Expected Intent: PRICE_ANALYSIS
Expected Entities: {
    "price": 2500000000,
    "area": 70,
    "location": "Qu·∫≠n 7",
    "property_type": "cƒÉn h·ªô"
}
Expected Service: price_suggestion
Expected Response:
- Market average price for similar properties
- "H·ª£p l√Ω" or "Cao h∆°n th·ªã tr∆∞·ªùng X%"
- Reasoning with data

# TC3.2: Price range question
Query: "Bao nhi√™u ti·ªÅn mua ƒë∆∞·ª£c nh√† 100m¬≤ Qu·∫≠n 2?"
Expected Intent: PRICE_ANALYSIS
Expected Response: Price range from market data

# TC3.3: Investment ROI
Query: "CƒÉn n√†y 3 t·ª∑, sau 5 nƒÉm b√°n ƒë∆∞·ª£c bao nhi√™u?"
Expected Intent: PRICE_ANALYSIS (or INVESTMENT_ADVICE)
Expected Response: ROI projection based on trends
```

**Success Criteria:**
- ‚úÖ Uses real market data from 13K properties
- ‚úÖ Price analysis accuracy within 10% of market
- ‚úÖ Clear reasoning with data points
- ‚úÖ Vietnamese + English support

---

### 4. INVESTMENT_ADVICE Intent (Custom)

**Business Logic:**
- User asks for investment recommendations
- System analyzes market trends, location potential
- Provides data-driven investment advice

**Test Cases:**

```python
# TC4.1: Location comparison for investment
Query: "N√™n ƒë·∫ßu t∆∞ Q2 hay Q7 v·ªõi 5 t·ª∑?"
Expected Intent: INVESTMENT_ADVICE
Expected Entities: {
    "locations": ["Qu·∫≠n 2", "Qu·∫≠n 7"],
    "budget": 5000000000
}
Expected Response:
- Growth trends for both districts
- Price appreciation data
- Recommendation with reasoning

# TC4.2: Investment potential question
Query: "CƒÉn n√†y c√≥ ti·ªÅm nƒÉng sinh l·ªùi kh√¥ng?"
Expected Intent: INVESTMENT_ADVICE
Expected Response:
- Location analysis
- Market trends
- Rental yield potential
- Appreciation forecast

# TC4.3: Budget optimization
Query: "5 t·ª∑ n√™n mua 1 cƒÉn l·ªõn hay 2 cƒÉn nh·ªè?"
Expected Intent: INVESTMENT_ADVICE
Expected Response: Strategy comparison with ROI
```

**Success Criteria:**
- ‚úÖ Data-driven recommendations
- ‚úÖ Market trend analysis
- ‚úÖ Risk assessment
- ‚úÖ Clear reasoning

---

### 5. LOCATION_INSIGHTS Intent (Custom)

**Business Logic:**
- User asks about a specific area/district
- System provides insights: infrastructure, amenities, trends
- Uses both structured data + LLM knowledge

**Test Cases:**

```python
# TC5.1: General area question
Query: "Qu·∫≠n Th·ªß ƒê·ª©c c√≥ g√¨ hay?"
Expected Intent: LOCATION_INSIGHTS
Expected Entities: {
    "location": "Qu·∫≠n Th·ªß ƒê·ª©c"
}
Expected Response:
- Key developments (Tech Park, universities)
- Infrastructure (Metro, highways)
- Average prices
- Growth potential

# TC5.2: Amenities question
Query: "G·∫ßn ƒë√¢y c√≥ tr∆∞·ªùng h·ªçc n√†o kh√¥ng?"
Expected Intent: LOCATION_INSIGHTS (context-dependent)
Expected Response: Schools near current context location

# TC5.3: Comparison of locations
Query: "Q7 v√† Q2 kh√°c nhau th·∫ø n√†o?"
Expected Intent: LOCATION_INSIGHTS or COMPARE
Expected Response: Location comparison table
```

**Success Criteria:**
- ‚úÖ Accurate location data
- ‚úÖ Infrastructure insights
- ‚úÖ Price trends for area
- ‚úÖ Amenities list

---

### 6. LEGAL_GUIDANCE Intent (CTO Service: Core Gateway)

**Business Logic:**
- User asks legal/procedural questions
- System uses LLM knowledge (no RAG needed)
- Provides legal guidance with disclaimers

**Test Cases:**

```python
# TC6.1: Document question
Query: "S·ªï ƒë·ªè kh√°c s·ªï h·ªìng th·∫ø n√†o?"
Expected Intent: LEGAL_GUIDANCE
Expected Service: core_gateway (direct LLM)
Expected Response:
- Clear explanation of differences
- Legal disclaimer
- When to use which

# TC6.2: Process question
Query: "Th·ªß t·ª•c mua nh√† g·ªìm nh·ªØng g√¨?"
Expected Intent: LEGAL_GUIDANCE
Expected Response:
- Step-by-step process
- Required documents
- Timeline
- Costs involved

# TC6.3: Tax question
Query: "Ph·∫£i ƒë√≥ng thu·∫ø g√¨ khi b√°n nh√†?"
Expected Intent: LEGAL_GUIDANCE
Expected Response:
- Tax types (Capital gains, etc.)
- Calculation method
- Legal disclaimer
```

**Success Criteria:**
- ‚úÖ Accurate legal information
- ‚úÖ Clear disclaimers
- ‚úÖ Step-by-step guidance
- ‚úÖ Vietnamese legal context

---

### 7. CHAT Intent (CTO Service: Core Gateway)

**Business Logic:**
- General conversation, greetings, questions about system
- Uses Core Gateway for simple LLM chat
- No RAG needed

**Test Cases:**

```python
# TC7.1: Greetings
Queries:
- "Xin ch√†o" ‚Üí CHAT
- "Hello" ‚Üí CHAT
- "B·∫°n l√† ai?" ‚Üí CHAT
Expected Response: Friendly introduction of REE AI

# TC7.2: System capabilities
Query: "B·∫°n c√≥ th·ªÉ l√†m g√¨?"
Expected Intent: CHAT
Expected Response: List of capabilities (search, compare, price analysis, etc.)

# TC7.3: Chitchat
Query: "H√¥m nay th·ªùi ti·∫øt th·∫ø n√†o?"
Expected Intent: CHAT or UNKNOWN
Expected Response: Polite deflection to real estate topics
```

**Success Criteria:**
- ‚úÖ Friendly, helpful tone
- ‚úÖ Introduces capabilities
- ‚úÖ Deflects off-topic gracefully
- ‚úÖ Fast response (< 1s)

---

### 8. UNKNOWN Intent (Fallback)

**Business Logic:**
- Query doesn't match any intent
- System asks for clarification
- Suggests valid intent types

**Test Cases:**

```python
# TC8.1: Gibberish
Query: "asdf qwer zxcv"
Expected Intent: UNKNOWN
Expected Response: "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ..."

# TC8.2: Off-topic
Query: "T√≠nh 2+2 b·∫±ng m·∫•y?"
Expected Intent: UNKNOWN
Expected Response: Redirect to real estate topics

# TC8.3: Ambiguous
Query: "N√≥ nh∆∞ th·∫ø n√†o?"
Expected Intent: UNKNOWN (needs context)
Expected Response: "B·∫°n ƒëang h·ªèi v·ªÅ b·∫•t ƒë·ªông s·∫£n n√†o?"
```

**Success Criteria:**
- ‚úÖ Polite error handling
- ‚úÖ Suggests valid queries
- ‚úÖ No hallucination
- ‚úÖ Graceful degradation

---

## üîß Prompt Improvement Strategy

### Current Issues

1. **Intent Mismatch:**
   - Code has: CLASSIFY, EXTRACT, RECOMMEND
   - Prompts have: PRICE_ANALYSIS, INVESTMENT_ADVICE, LOCATION_INSIGHTS
   - **Action:** Sync code with prompts (use prompts.py definitions)

2. **Prompt Quality:**
   - prompts.py has excellent Vietnamese real estate context
   - main.py has basic English prompts
   - **Action:** Replace main.py prompts with prompts.py

3. **Few-Shot Examples:**
   - prompts.py has 6 excellent examples
   - main.py has zero
   - **Action:** Import few-shot examples to improve accuracy

### Improvement Plan

#### Step 1: Sync Intent Types
```python
# Update shared/models/orchestrator.py
class IntentType(str, Enum):
    SEARCH = "search"
    COMPARE = "compare"
    PRICE_ANALYSIS = "price_analysis"      # NEW
    INVESTMENT_ADVICE = "investment_advice" # NEW
    LOCATION_INSIGHTS = "location_insights" # NEW
    LEGAL_GUIDANCE = "legal_guidance"       # NEW
    CHAT = "chat"
    UNKNOWN = "unknown"
    # REMOVE: classify, extract, recommend
```

#### Step 2: Replace Prompt in main.py
```python
# OLD (main.py line 48-73)
self.intent_prompt = ChatPromptTemplate.from_messages([...])

# NEW (use prompts.py)
from .prompts import get_intent_detection_prompt
self.intent_prompt = get_intent_detection_prompt()
```

#### Step 3: Add Few-Shot Learning
```python
# Add to intent detection
from .prompts import OrchestratorPrompts

examples_text = OrchestratorPrompts.get_few_shot_examples_text()

# Include in prompt
prompt = self.intent_prompt.format(
    query=query,
    examples=examples_text  # NEW
)
```

#### Step 4: Improve Routing Logic
```python
# Expand _decide_routing() with all 8 intents
def _decide_routing(self, intent_result: IntentDetectionResult):
    routing_map = {
        IntentType.SEARCH: ("rag_service", "/rag", True),
        IntentType.COMPARE: ("rag_service", "/compare", True),
        IntentType.PRICE_ANALYSIS: ("price_suggestion", "/analyze", False),
        IntentType.INVESTMENT_ADVICE: ("rag_service", "/investment", True),
        IntentType.LOCATION_INSIGHTS: ("rag_service", "/location", True),
        IntentType.LEGAL_GUIDANCE: ("core_gateway", "/chat/completions", False),
        IntentType.CHAT: ("core_gateway", "/chat/completions", False),
        IntentType.UNKNOWN: ("core_gateway", "/chat/completions", False),
    }

    service, endpoint, use_rag = routing_map.get(intent_result.intent)
    return RoutingDecision(...)
```

---

## üìä Test Execution Plan

### Phase 1: Unit Tests (Intent Detection)
**Duration:** 2 hours
**Test File:** `tests/test_orchestrator_intents.py`

```python
import pytest
from services.orchestrator.main import Orchestrator

@pytest.mark.asyncio
async def test_search_intent():
    orch = Orchestrator()
    result = await orch._detect_intent("T√¨m cƒÉn h·ªô 2PN Q7")

    assert result.intent == IntentType.SEARCH
    assert result.confidence >= 0.8
    assert "bedrooms" in result.extracted_entities
    assert result.extracted_entities["bedrooms"] == 2

# 50+ test cases for all 8 intents
```

### Phase 2: Integration Tests (End-to-End)
**Duration:** 4 hours
**Test File:** `tests/test_business_logic_e2e.py`

```python
@pytest.mark.asyncio
async def test_search_with_real_data():
    """Test SEARCH intent with 13K properties"""

    # Setup: Ensure database has 13K+ properties
    assert get_property_count() >= 10000

    # Execute: Search query
    request = OrchestrationRequest(
        user_id="test_user",
        query="T√¨m cƒÉn h·ªô 2 ph√≤ng ng·ªß Qu·∫≠n 7 d∆∞·ªõi 3 t·ª∑"
    )

    response = await orchestrator.orchestrate(request)

    # Verify: Intent detection
    assert response.intent == IntentType.SEARCH
    assert response.confidence >= 0.8

    # Verify: Response quality
    assert "cƒÉn h·ªô" in response.response.lower()
    assert "qu·∫≠n 7" in response.response.lower()

    # Verify: Performance
    assert response.execution_time_ms < 2000
```

### Phase 3: Prompt Optimization
**Duration:** 3 hours
**Method:** A/B Testing

```python
# Test old prompt vs new prompt
old_prompt_accuracy = test_with_prompt(old_prompt, test_cases)
new_prompt_accuracy = test_with_prompt(new_prompt, test_cases)

assert new_prompt_accuracy > old_prompt_accuracy + 0.05  # 5% improvement
```

### Phase 4: Load Testing
**Duration:** 2 hours
**Tool:** Locust

```python
# Simulate 100 concurrent users
# Test all 8 intent types
# Verify response times < 2s at p95
```

---

## üìà Success Metrics

### Accuracy Metrics
- **Intent Detection:** ‚â• 90% accuracy across 8 types
- **Entity Extraction:** ‚â• 85% accuracy (F1 score)
- **RAG Relevance:** P@10 ‚â• 0.7 (top 10 results relevant)
- **Price Analysis:** Within 10% of market average

### Performance Metrics
- **Response Time:** p95 < 2 seconds
- **Throughput:** 100 req/sec (orchestrator)
- **Availability:** 99.9% uptime

### Business Metrics
- **User Satisfaction:** Positive response to 80% queries
- **Conversation Success:** 85% complete without clarification
- **Coverage:** Handle 95% of real user queries

---

## üéØ Test Data

### Real Estate Queries Dataset (100 samples)

**SEARCH (25 queries):**
1. "T√¨m cƒÉn h·ªô 2 ph√≤ng ng·ªß Qu·∫≠n 7"
2. "C√≥ nh√† n√†o gi√° r·∫ª kh√¥ng?"
3. "Find apartments near metro"
4. "C·∫ßn mua bi·ªát th·ª± Th·ªß ƒê·ª©c"
5. "Nh√† m·∫∑t ti·ªÅn ƒë∆∞·ªùng l·ªõn Q1"
... (20 more)

**COMPARE (15 queries):**
1. "So s√°nh 2 cƒÉn h·ªô Vinhomes"
2. "CƒÉn n√†o t·ªët h∆°n?"
3. "Q2 vs Q7 cho gia ƒë√¨nh tr·∫ª"
... (12 more)

**PRICE_ANALYSIS (15 queries):**
1. "Gi√° 2.5 t·ª∑ cho 70m¬≤ Q7 h·ª£p l√Ω kh√¥ng?"
2. "Bao nhi√™u ti·ªÅn mua nh√† Q2?"
3. "Is 3 billion too expensive?"
... (12 more)

**INVESTMENT_ADVICE (15 queries):**
1. "N√™n ƒë·∫ßu t∆∞ Q2 hay Q7?"
2. "CƒÉn n√†y c√≥ ti·ªÅm nƒÉng kh√¥ng?"
3. "5 t·ª∑ mua 1 hay 2 cƒÉn?"
... (12 more)

**LOCATION_INSIGHTS (10 queries):**
1. "Qu·∫≠n Th·ªß ƒê·ª©c c√≥ g√¨ hay?"
2. "Q7 ph√°t tri·ªÉn th·∫ø n√†o?"
... (8 more)

**LEGAL_GUIDANCE (10 queries):**
1. "S·ªï ƒë·ªè kh√°c s·ªï h·ªìng th·∫ø n√†o?"
2. "Th·ªß t·ª•c mua nh√† g·ªìm g√¨?"
... (8 more)

**CHAT (5 queries):**
1. "Xin ch√†o"
2. "B·∫°n l√† ai?"
... (3 more)

**UNKNOWN (5 queries):**
1. "asdf qwer"
2. "T√≠nh 2+2"
... (3 more)

---

## üìù Next Steps

1. ‚úÖ **Sync Intent Types** (main.py ‚Üê prompts.py)
2. ‚úÖ **Replace Prompts** (use high-quality Vietnamese prompts)
3. ‚úÖ **Create Test Suite** (100 test cases)
4. ‚úÖ **Run Tests** (Unit ‚Üí Integration ‚Üí E2E)
5. ‚úÖ **Measure Metrics** (Accuracy, Performance, Business)
6. ‚úÖ **Generate Report** (Share with CTO)

---

## üìä Expected Outcomes

After testing and prompt improvements:

- **Intent Accuracy:** 85% ‚Üí **95%** (+10%)
- **Entity Extraction:** 75% ‚Üí **90%** (+15%)
- **Response Quality:** 70% ‚Üí **88%** (+18%)
- **User Satisfaction:** Unknown ‚Üí **85%+**

**Timeline:** 11 hours total (1.5 days)

**Resources:**
- 13,448 real properties for testing
- CTO prompts with Vietnamese expertise
- LangChain framework for improvements

---

**Status:** üöß Ready to Execute
**Next:** Start with Phase 1 - Unit Tests

