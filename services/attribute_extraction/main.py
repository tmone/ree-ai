"""
Attribute Extraction Service - CTO Service #4 (Layer 3)
Extracts structured entities from raw user queries using enhanced 3-layer pipeline:
1. NLP Pre-processing - Rule-based entity extraction
2. RAG Context Retrieval - Get similar properties for context
3. Enhanced LLM Extraction - LLM with NLP + RAG hints
4. Post-Validation - Validate against DB distribution
"""
import httpx
import json
import re
from typing import Dict, Any, Optional
from fastapi import HTTPException
from pydantic import BaseModel

from core.base_service import BaseService
from services.attribute_extraction.prompts import AttributeExtractionPrompts, PropertyAttributes
from services.attribute_extraction.nlp_processor import VietnameseNLPProcessor
from services.attribute_extraction.rag_enhancer import RAGContextEnhancer
from services.attribute_extraction.validator import AttributeValidator
from shared.config import settings
from shared.utils.logger import LogEmoji
from shared.i18n import get_multilingual_mapper


class QueryExtractionRequest(BaseModel):
    """Request to extract entities from user query"""
    query: str
    intent: Optional[str] = None  # SEARCH, COMPARE, etc.


class QueryExtractionResponse(BaseModel):
    """Response with extracted entities"""
    entities: Dict[str, Any]
    confidence: float
    extracted_from: str  # "query"


class EnhancedExtractionResponse(BaseModel):
    """Response with enhanced extraction using NLP + RAG + LLM pipeline"""
    entities: Dict[str, Any]
    confidence: float
    extracted_from: str
    nlp_entities: Dict[str, Any]  # Entities from NLP layer
    rag_retrieved_count: int  # Number of similar properties used for context
    warnings: list[str]  # Validation warnings
    validation_details: Dict[str, Any]  # Detailed validation info


class AttributeExtractionService(BaseService):
    """
    Attribute Extraction Service - Layer 3
    Extracts structured entities from raw queries and property descriptions
    """

    def __init__(self):
        super().__init__(
            name="attribute_extraction",
            version="2.0.0",  # Enhanced with NLP + RAG
            capabilities=["entity_extraction", "attribute_extraction", "query_parsing", "nlp_preprocessing", "rag_enhanced"],
            port=8080
        )

        self.http_client = httpx.AsyncClient(timeout=60.0)
        self.core_gateway_url = settings.get_core_gateway_url()
        self.db_gateway_url = settings.get_db_gateway_url()

        # Initialize enhanced components
        self.nlp_processor = VietnameseNLPProcessor()
        self.rag_enhancer = RAGContextEnhancer(self.db_gateway_url)
        self.validator = AttributeValidator()
        self.multilingual_mapper = get_multilingual_mapper()

        self.logger.info(f"{LogEmoji.INFO} Using Core Gateway at: {self.core_gateway_url}")
        self.logger.info(f"{LogEmoji.INFO} Using DB Gateway at: {self.db_gateway_url}")
        self.logger.info(f"{LogEmoji.SUCCESS} Enhanced NLP + RAG pipeline initialized")
        self.logger.info(f"{LogEmoji.SUCCESS} Multilingual mapper initialized (vi/en/zh ‚Üí English)")

    def setup_routes(self):
        """Setup API routes"""

        @self.app.post("/extract-query-enhanced", response_model=EnhancedExtractionResponse)
        async def extract_from_query_enhanced(request: QueryExtractionRequest):
            """
            **NEW ENHANCED ENDPOINT** - Extract entities using 3-layer pipeline:
            1. NLP Pre-processing (rule-based)
            2. RAG Context Retrieval (similar properties)
            3. Enhanced LLM Extraction (with NLP + RAG context)
            4. Post-Validation (against DB distribution)

            This is the RECOMMENDED endpoint for production use!
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} Enhanced extraction for: '{request.query}'")

                # LAYER 1: NLP Pre-processing
                self.logger.info(f"{LogEmoji.AI} Layer 1: NLP Pre-processing...")
                nlp_entities = self.nlp_processor.extract_entities(request.query)
                nlp_confidence = self.nlp_processor.get_extraction_confidence(nlp_entities)
                self.logger.info(f"{LogEmoji.SUCCESS} NLP extracted {len(nlp_entities)} entities (confidence: {nlp_confidence:.2f})")

                # LAYER 2: RAG Context Retrieval
                self.logger.info(f"{LogEmoji.AI} Layer 2: RAG Context Retrieval...")
                rag_context = await self.rag_enhancer.get_context(
                    query=request.query,
                    nlp_entities=nlp_entities,
                    limit=5
                )
                rag_count = rag_context.get("retrieved_count", 0)
                self.logger.info(f"{LogEmoji.SUCCESS} RAG retrieved {rag_count} similar properties")

                # LAYER 3: Enhanced LLM Extraction
                self.logger.info(f"{LogEmoji.AI} Layer 3: Enhanced LLM Extraction...")
                llm_entities = await self._enhanced_llm_extraction(
                    query=request.query,
                    nlp_entities=nlp_entities,
                    rag_context=rag_context,
                    intent=request.intent
                )
                self.logger.info(f"{LogEmoji.SUCCESS} LLM extracted {len(llm_entities)} entities")

                # LAYER 4: Post-Validation
                self.logger.info(f"{LogEmoji.AI} Layer 4: Post-Validation...")
                validation_result = self.validator.validate(
                    entities=llm_entities,
                    nlp_entities=nlp_entities,
                    rag_context=rag_context
                )

                validated_entities = validation_result["validated_entities"]
                confidence = validation_result["confidence"]
                warnings = validation_result["warnings"]
                validation_details = validation_result["validation_details"]

                # CRITICAL: Normalize entities to English master data standard
                # This converts multilingual input (vi/zh) ‚Üí English for database storage
                self.logger.info(f"{LogEmoji.AI} Normalizing entities to English master data...")
                normalized_entities = self.multilingual_mapper.normalize_entities(
                    validated_entities,
                    source_lang="vi"  # Default to Vietnamese, can be auto-detected
                )
                self.logger.info(
                    f"{LogEmoji.SUCCESS} Entities normalized to English: {normalized_entities}"
                )

                self.logger.info(
                    f"{LogEmoji.SUCCESS} Extraction complete! "
                    f"Confidence: {confidence:.2f}, Warnings: {len(warnings)}"
                )

                return EnhancedExtractionResponse(
                    entities=normalized_entities,  # Return English-normalized entities
                    confidence=confidence,
                    extracted_from="enhanced_pipeline",
                    nlp_entities=nlp_entities,
                    rag_retrieved_count=rag_count,
                    warnings=warnings,
                    validation_details=validation_details
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Enhanced extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-query", response_model=QueryExtractionResponse)
        async def extract_from_query(request: QueryExtractionRequest):
            """
            Extract entities from user search query
            This is what Orchestrator should call for SEARCH intent!
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} Extracting entities from query: '{request.query}'")

                # Build specialized prompt for query extraction (not full property description)
                prompt = self._build_query_extraction_prompt(request.query, request.intent)

                # Call LLM via Core Gateway
                entities = await self._call_llm_for_extraction(prompt)

                # Normalize to English master data
                self.logger.info(f"{LogEmoji.AI} Normalizing query entities to English...")
                normalized_entities = self.multilingual_mapper.normalize_entities(
                    entities,
                    source_lang="vi"
                )

                confidence = self._calculate_confidence(normalized_entities)

                self.logger.info(f"{LogEmoji.SUCCESS} Extracted entities (normalized): {normalized_entities}")

                return QueryExtractionResponse(
                    entities=normalized_entities,
                    confidence=confidence,
                    extracted_from="query"
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Entity extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-property", response_model=QueryExtractionResponse)
        async def extract_from_property_description(request: QueryExtractionRequest):
            """
            Extract full property attributes from property description/listing
            Used for data enrichment, not for user queries
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} Extracting from property description (length: {len(request.query)})")

                # Use full extraction prompt for property descriptions
                prompt = AttributeExtractionPrompts.build_extraction_prompt(request.query, include_examples=True)

                entities = await self._call_llm_for_extraction(prompt)
                confidence = self._calculate_confidence(entities)

                return QueryExtractionResponse(
                    entities=entities,
                    confidence=confidence,
                    extracted_from="property_description"
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Property extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

    def _build_query_extraction_prompt(self, query: str, intent: Optional[str] = None) -> str:
        """
        Build specialized prompt for extracting entities from USER QUERIES
        Simpler than full property extraction - focuses on search criteria
        """
        return f"""B·∫°n l√† chuy√™n gia tr√≠ch xu·∫•t th√¥ng tin t√¨m ki·∫øm b·∫•t ƒë·ªông s·∫£n t·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

üéØ NHI·ªÜM V·ª§:
ƒê·ªçc c√¢u h·ªèi c·ªßa user v√† tr√≠ch xu·∫•t C√ÅC TI√äU CH√ç T√åM KI·∫æM th√†nh JSON.

üìä ENTITIES C·∫¶N TR√çCH XU·∫§T (ch·ªâ tr√≠ch xu·∫•t nh·ªØng g√¨ c√≥ trong c√¢u h·ªèi):

**1. PROPERTY TYPE**
- property_type: cƒÉn h·ªô | nh√† ph·ªë | bi·ªát th·ª± | ƒë·∫•t | chung c∆∞ | commercial

**2. LOCATION**
- district: Qu·∫≠n/Huy·ªán (chu·∫©n h√≥a)
  * "Q7", "Q.7", "qu·∫≠n 7" ‚Üí "Qu·∫≠n 7"
  * "Q2" ‚Üí "Qu·∫≠n 2"
  * "B√¨nh Th·∫°nh" ‚Üí "Qu·∫≠n B√¨nh Th·∫°nh"
  * "Th·ªß ƒê·ª©c" ‚Üí "Qu·∫≠n Th·ªß ƒê·ª©c"
- ward: Ph∆∞·ªùng (n·∫øu c√≥)
- project_name: T√™n d·ª± √°n (Vinhomes, Masteri, etc.)

**3. PHYSICAL ATTRIBUTES**
- bedrooms: S·ªë ph√≤ng ng·ªß
  * "2PN" ‚Üí 2
  * "3 ph√≤ng ng·ªß" ‚Üí 3
  * "2 ph√≤ng" ‚Üí 2
- bathrooms: S·ªë ph√≤ng t·∫Øm/WC
- area: Di·ªán t√≠ch (m¬≤)
- min_area: Di·ªán t√≠ch t·ªëi thi·ªÉu
- max_area: Di·ªán t√≠ch t·ªëi ƒëa

**4. PRICE**
- price: Gi√° c·ª• th·ªÉ (VND)
- min_price: Gi√° t·ªëi thi·ªÉu (VND)
- max_price: Gi√° t·ªëi ƒëa (VND)

CHU·∫®N H√ìA GI√Å:
  * "d∆∞·ªõi 3 t·ª∑" ‚Üí max_price: 3000000000
  * "t·ª´ 2 ƒë·∫øn 5 t·ª∑" ‚Üí min_price: 2000000000, max_price: 5000000000
  * "kho·∫£ng 3 t·ª∑" ‚Üí price: 3000000000
  * "25 tri·ªáu/th√°ng" ‚Üí price: 25000000

**5. FEATURES**
- furniture: full | c∆° b·∫£n | kh√¥ng
- direction: ƒê√¥ng | T√¢y | Nam | B·∫Øc | etc.

**6. AMENITIES**
- parking: true n·∫øu c√≥ y√™u c·∫ßu ch·ªó ƒë·∫≠u xe
- elevator: true n·∫øu c√≥ y√™u c·∫ßu thang m√°y
- swimming_pool: true n·∫øu c√≥ y√™u c·∫ßu h·ªì b∆°i
- gym: true n·∫øu c√≥ y√™u c·∫ßu gym

üîç EXTRACTION RULES:

1. **Ch·ªâ tr√≠ch xu·∫•t th√¥ng tin C√ì TRONG C√ÇU H·ªéI** - ƒë·ª´ng b·ªãa th√™m
2. **Chu·∫©n h√≥a ƒë·ªãa danh** v·ªÅ format chu·∫©n
3. **Chu·∫©n h√≥a gi√°** v·ªÅ s·ªë VND
4. **N·∫øu kh√¥ng c√≥ th√¥ng tin** ‚Üí kh√¥ng ƒë∆∞a v√†o JSON (not null, just omit)
5. **∆Øu ti√™n t·ª´ kh√≥a r√µ r√†ng** h∆°n t·ª´ kh√≥a m∆° h·ªì

üìù FEW-SHOT EXAMPLES:

Example 1:
Input: "T√¨m cƒÉn h·ªô 2 ph√≤ng ng·ªß qu·∫≠n 7 d∆∞·ªõi 3 t·ª∑"
Output: {{"property_type": "cƒÉn h·ªô", "bedrooms": 2, "district": "Qu·∫≠n 7", "max_price": 3000000000}}

Example 2:
Input: "C·∫ßn mua nh√† ph·ªë B√¨nh Th·∫°nh kho·∫£ng 100m2"
Output: {{"property_type": "nh√† ph·ªë", "district": "Qu·∫≠n B√¨nh Th·∫°nh", "area": 100}}

Example 3:
Input: "T√¨m chung c∆∞ Vinhomes c√≥ h·ªì b∆°i"
Output: {{"property_type": "chung c∆∞", "project_name": "Vinhomes", "swimming_pool": true}}

Example 4:
Input: "Bi·ªát th·ª± Q2 t·ª´ 10 ƒë·∫øn 20 t·ª∑, c√≥ garage"
Output: {{"property_type": "bi·ªát th·ª±", "district": "Qu·∫≠n 2", "min_price": 10000000000, "max_price": 20000000000, "parking": true}}

üì• USER QUERY:
{query}

Intent: {intent or "SEARCH"}

üì§ OUTPUT (ch·ªâ JSON, kh√¥ng gi·∫£i th√≠ch):
"""

    async def _call_llm_for_extraction(self, prompt: str) -> Dict[str, Any]:
        """Call LLM via Core Gateway to extract entities"""
        try:
            llm_request = {
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "You are a structured data extraction expert. Always return valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 500,
                "temperature": 0.2  # Low temperature for consistent extraction
            }

            response = await self.http_client.post(
                f"{self.core_gateway_url}/chat/completions",
                json=llm_request,
                timeout=30.0
            )

            if response.status_code == 200:
                data = response.json()
                content = data.get("content", "").strip()

                # Clean up markdown code blocks
                content = re.sub(r'^```(?:json)?\s*\n?', '', content)
                content = re.sub(r'\n?```\s*$', '', content)
                content = content.strip()

                # Parse JSON
                entities = json.loads(content)
                return entities
            else:
                self.logger.warning(f"{LogEmoji.WARNING} Core Gateway returned {response.status_code}")
                return {}

        except json.JSONDecodeError as e:
            self.logger.error(f"{LogEmoji.ERROR} Failed to parse JSON: {e}")
            self.logger.error(f"Raw content: {content}")
            return {}
        except Exception as e:
            self.logger.error(f"{LogEmoji.ERROR} LLM call failed: {e}")
            return {}

    def _calculate_confidence(self, entities: Dict[str, Any]) -> float:
        """Calculate confidence score based on extracted entities"""
        if not entities:
            return 0.0

        # Simple confidence based on number of entities extracted
        num_entities = len(entities)
        if num_entities >= 4:
            return 0.95
        elif num_entities >= 3:
            return 0.85
        elif num_entities >= 2:
            return 0.75
        else:
            return 0.65

    async def _enhanced_llm_extraction(
        self,
        query: str,
        nlp_entities: Dict[str, Any],
        rag_context: Dict[str, Any],
        intent: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Enhanced LLM extraction with NLP hints and RAG context.

        This builds a rich prompt that includes:
        1. NLP pre-extracted entities (as hints)
        2. Real property examples from RAG
        3. Value ranges and patterns from DB
        """
        # Build enhanced prompt
        prompt = self._build_enhanced_prompt(query, nlp_entities, rag_context, intent)

        # Call LLM
        entities = await self._call_llm_for_extraction(prompt)

        return entities

    def _build_enhanced_prompt(
        self,
        query: str,
        nlp_entities: Dict[str, Any],
        rag_context: Dict[str, Any],
        intent: Optional[str] = None
    ) -> str:
        """
        Build enhanced prompt with NLP hints and RAG context.
        """
        # Get RAG components
        examples = rag_context.get("examples", [])
        patterns = rag_context.get("patterns", {})
        value_ranges = rag_context.get("value_ranges", {})

        # Format examples
        examples_text = ""
        if examples:
            examples_text = "üìö REAL PROPERTY EXAMPLES FROM DATABASE (similar to this query):\n"
            for i, ex in enumerate(examples[:3], 1):
                examples_text += f"\nExample {i}:\n{json.dumps(ex, indent=2, ensure_ascii=False)}\n"

        # Format patterns
        patterns_text = ""
        if patterns:
            patterns_text = "üìä COMMON PATTERNS IN SIMILAR PROPERTIES:\n"
            if "common_districts" in patterns:
                districts = [d["value"] for d in patterns["common_districts"][:3]]
                patterns_text += f"- Common districts: {', '.join(districts)}\n"
            if "common_property_types" in patterns:
                types = [t["value"] for t in patterns["common_property_types"][:3]]
                patterns_text += f"- Common property types: {', '.join(types)}\n"

        # Format value ranges
        ranges_text = ""
        if value_ranges:
            ranges_text = "üìà VALUE RANGES FROM SIMILAR PROPERTIES:\n"
            if "price" in value_ranges:
                pr = value_ranges["price"]
                ranges_text += f"- Price range: {pr['min']:,.0f} - {pr['max']:,.0f} VND (avg: {pr['avg']:,.0f})\n"
            if "area" in value_ranges:
                ar = value_ranges["area"]
                ranges_text += f"- Area range: {ar['min']:.0f} - {ar['max']:.0f} m¬≤ (avg: {ar['avg']:.0f})\n"

        # Format NLP hints
        nlp_hints_text = ""
        if nlp_entities:
            nlp_hints_text = f"üí° NLP PRE-EXTRACTED HINTS:\n{json.dumps(nlp_entities, indent=2, ensure_ascii=False)}\n"

        prompt = f"""B·∫°n l√† chuy√™n gia tr√≠ch xu·∫•t th√¥ng tin b·∫•t ƒë·ªông s·∫£n.

üéØ NHI·ªÜM V·ª§: Tr√≠ch xu·∫•t entities t·ª´ query, S·ª¨ D·ª§NG HINTS t·ª´ NLP v√† EXAMPLES t·ª´ database.

{nlp_hints_text}

{examples_text}

{patterns_text}

{ranges_text}

üîç EXTRACTION RULES:
1. **USE NLP hints as starting point** - ∆Øu ti√™n th√¥ng tin t·ª´ NLP layer
2. **FOLLOW patterns from real examples** - Tham kh·∫£o format t·ª´ DB
3. **STAY within typical value ranges** - Ki·ªÉm tra v·ªõi ranges t·ª´ DB
4. **DON'T hallucinate** - Ch·ªâ tr√≠ch xu·∫•t th√¥ng tin c√≥ trong query
5. **Chu·∫©n h√≥a format** - S·ª≠ d·ª•ng format gi·ªëng examples

üìä ENTITIES C·∫¶N TR√çCH XU·∫§T (ch·ªâ tr√≠ch xu·∫•t nh·ªØng g√¨ c√≥ trong c√¢u h·ªèi):

**1. PROPERTY TYPE**
- property_type: cƒÉn h·ªô | nh√† ph·ªë | bi·ªát th·ª± | ƒë·∫•t | chung c∆∞ | vƒÉn ph√≤ng

**2. LOCATION**
- district: Qu·∫≠n/Huy·ªán (chu·∫©n h√≥a nh∆∞ examples)
- ward: Ph∆∞·ªùng (n·∫øu c√≥)
- project_name: T√™n d·ª± √°n

**3. PHYSICAL ATTRIBUTES**
- bedrooms: S·ªë ph√≤ng ng·ªß
- bathrooms: S·ªë ph√≤ng t·∫Øm
- area: Di·ªán t√≠ch (m¬≤)
- floors: S·ªë t·∫ßng

**4. PRICE**
- price: Gi√° c·ª• th·ªÉ (VND)
- min_price: Gi√° t·ªëi thi·ªÉu (VND)
- max_price: Gi√° t·ªëi ƒëa (VND)

**5. FEATURES & AMENITIES**
- furniture: full | c∆° b·∫£n | kh√¥ng
- direction: H∆∞·ªõng nh√†
- parking: true/false
- elevator: true/false
- swimming_pool: true/false
- gym: true/false

üì• USER QUERY:
{query}

Intent: {intent or "SEARCH"}

üì§ OUTPUT (ch·ªâ JSON, kh√¥ng gi·∫£i th√≠ch):
"""
        return prompt

    async def on_shutdown(self):
        """Cleanup on shutdown"""
        await self.http_client.aclose()
        await self.rag_enhancer.close()
        await super().on_shutdown()


# Create service instance at module level for uvicorn
service = AttributeExtractionService()
app = service.app

if __name__ == "__main__":
    service.run()
