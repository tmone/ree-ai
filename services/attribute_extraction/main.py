"""
Attribute Extraction Service - CTO Service #4 (Layer 3)
Extracts structured entities from raw user queries using enhanced 3-layer pipeline:
1. NLP Pre-processing - Rule-based entity extraction
2. RAG Context Retrieval - Get similar properties for context
3. Enhanced LLM Extraction - LLM with NLP + RAG hints
4. Post-Validation - Validate against DB distribution
"""
import httpx
import json
import re
from typing import Dict, Any, Optional
from fastapi import HTTPException
from pydantic import BaseModel

from core.base_service import BaseService
from services.attribute_extraction.prompts import AttributeExtractionPrompts, PropertyAttributes
from services.attribute_extraction.nlp_processor import VietnameseNLPProcessor
from services.attribute_extraction.rag_enhancer import RAGContextEnhancer
from services.attribute_extraction.validator import AttributeValidator
from services.attribute_extraction.master_data_validator import MasterDataValidator
from services.attribute_extraction.master_data_extractor import MasterDataExtractor
from services.attribute_extraction.admin_routes import AdminRoutes
from shared.config import settings
from shared.utils.logger import LogEmoji
from shared.i18n import get_multilingual_mapper
from shared.models.attribute_extraction import ExtractionRequest, ExtractionResponse


class QueryExtractionRequest(BaseModel):
    """Request to extract entities from user query"""
    query: str
    intent: Optional[str] = None  # SEARCH, COMPARE, etc.


class ImageExtractionRequest(BaseModel):
    """Request to extract entities from property images"""
    images: List[str]  # Base64 encoded images
    text_context: Optional[str] = None  # Optional text context to enhance extraction


class QueryExtractionResponse(BaseModel):
    """Response with extracted entities"""
    entities: Dict[str, Any]
    confidence: float
    extracted_from: str  # "query"


class EnhancedExtractionResponse(BaseModel):
    """Response with enhanced extraction using NLP + RAG + LLM pipeline"""
    entities: Dict[str, Any]
    confidence: float
    extracted_from: str
    nlp_entities: Dict[str, Any]  # Entities from NLP layer
    rag_retrieved_count: int  # Number of similar properties used for context
    warnings: list[str]  # Validation warnings
    validation_details: Dict[str, Any]  # Detailed validation info
    # NEW: Confidence-based clarification
    needs_clarification: bool = False  # True if confidence too low
    clarification_questions: Optional[List[str]] = None  # Questions to ask user
    suggestions: Optional[List[Dict[str, Any]]] = None  # Suggested values


class AttributeExtractionService(BaseService):
    """
    Attribute Extraction Service - Layer 3
    Extracts structured entities from raw queries and property descriptions
    """

    def __init__(self):
        super().__init__(
            name="attribute_extraction",
            version="2.0.0",  # Enhanced with NLP + RAG
            capabilities=["entity_extraction", "attribute_extraction", "query_parsing", "nlp_preprocessing", "rag_enhanced"],
            port=8080
        )

        self.http_client = httpx.AsyncClient(timeout=60.0)
        self.core_gateway_url = settings.get_core_gateway_url()
        self.db_gateway_url = settings.get_db_gateway_url()

        # Initialize enhanced components
        self.nlp_processor = VietnameseNLPProcessor()
        self.rag_enhancer = RAGContextEnhancer(self.db_gateway_url)
        self.validator = AttributeValidator()
        self.multilingual_mapper = get_multilingual_mapper()
        self.master_data_validator = MasterDataValidator()

        # NEW: Initialize master data extractor (v2.0)
        self.master_data_extractor = MasterDataExtractor(
            core_gateway_url=self.core_gateway_url,
            llm_extractor=self._call_llm_for_extraction
        )

        # NEW: Initialize admin routes
        self.admin_routes = AdminRoutes()

        self.logger.info(f"{LogEmoji.INFO} Using Core Gateway at: {self.core_gateway_url}")
        self.logger.info(f"{LogEmoji.INFO} Using DB Gateway at: {self.db_gateway_url}")
        self.logger.info(f"{LogEmoji.SUCCESS} Enhanced NLP + RAG pipeline initialized")
        self.logger.info(f"{LogEmoji.SUCCESS} Multilingual mapper initialized (vi/en/zh â†’ English)")
        self.logger.info(f"{LogEmoji.SUCCESS} Master data validator initialized (PostgreSQL)")
        self.logger.info(f"{LogEmoji.SUCCESS} Master data extractor v2.0 initialized (full pipeline)")
        self.logger.info(f"{LogEmoji.SUCCESS} Admin routes initialized (pending item review)")

    def setup_routes(self):
        """Setup API routes"""

        @self.app.get("/master-data/districts")
        async def get_districts():
            """Get list of all districts from master data"""
            try:
                districts = await self.master_data_validator.get_districts_list()
                return {"districts": districts, "count": len(districts)}
            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Failed to get districts: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.get("/master-data/property-types")
        async def get_property_types():
            """Get list of all property types from master data"""
            try:
                property_types = await self.master_data_validator.get_property_types_list()
                return {"property_types": property_types, "count": len(property_types)}
            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Failed to get property types: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.get("/master-data/amenities")
        async def get_amenities(category: Optional[str] = None):
            """Get list of all amenities from master data"""
            try:
                amenities = await self.master_data_validator.get_amenities_list(category=category)
                return {"amenities": amenities, "count": len(amenities)}
            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Failed to get amenities: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-with-master-data", response_model=ExtractionResponse)
        async def extract_with_master_data(request: ExtractionRequest):
            """
            âœ¨ **NEW V2.0 ENDPOINT** âœ¨

            Complete extraction pipeline with master data integration:
            1. Auto-detect language from input text
            2. Extract attributes using LLM
            3. Fuzzy match against PostgreSQL master data with translations
            4. Translate unmatched items to English using LLM
            5. Return 3-tier response: raw/mapped/new

            This is the RECOMMENDED endpoint for production use!

            **Response Structure**:
            - `raw`: Numeric and free-form attributes (bedrooms, area, price)
            - `mapped`: Successfully matched attributes with master data IDs + translations
            - `new`: Unmatched attributes requiring admin review

            **Example**:
            Input: "CÄƒn há»™ 2PN Quáº­n 1 cÃ³ há»“ bÆ¡i"
            Output:
            {
              "raw": {"bedrooms": 2},
              "mapped": [
                {
                  "property_name": "property_type",
                  "id": 1,
                  "value": "apartment",
                  "value_translated": "CÄƒn há»™",
                  "confidence": 0.98
                }
              ],
              "new": []
            }
            """
            try:
                # Initialize extractor if needed
                if not hasattr(self, '_extractor_initialized'):
                    self.logger.info(f"{LogEmoji.INFO} Initializing master data extractor...")
                    await self.master_data_extractor.initialize()
                    self._extractor_initialized = True

                self.logger.info(
                    f"{LogEmoji.TARGET} Extraction request: "
                    f"text='{request.text[:50]}...', lang={request.language}"
                )

                # Execute extraction
                response = await self.master_data_extractor.extract(request)

                self.logger.info(
                    f"{LogEmoji.SUCCESS} Extraction complete: "
                    f"{len(response.mapped)} mapped, {len(response.new)} new "
                    f"({response.processing_time_ms:.0f}ms)"
                )

                return response

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Extraction with master data failed: {e}")
                import traceback
                traceback.print_exc()
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.get("/admin/pending-items")
        async def get_pending_items(
            status: str = "pending",
            limit: int = 50,
            offset: int = 0
        ):
            """
            Get list of pending master data items for admin review

            **Query Parameters**:
            - status: Filter by status ('pending', 'approved', 'rejected')
            - limit: Max items to return (default: 50)
            - offset: Pagination offset (default: 0)

            **Returns**:
            - items: List of pending items
            - total_count: Total number of items
            - high_frequency_items: Items that appear frequently (priority review)
            """
            try:
                return await self.admin_routes.get_pending_items(status, limit, offset)
            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Failed to get pending items: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/admin/approve-item")
        async def approve_pending_item(
            pending_id: int,
            translations: dict,
            admin_user_id: str,
            admin_notes: Optional[str] = None
        ):
            """
            Approve a pending item and add to master data

            **Body Parameters**:
            - pending_id: ID of pending item
            - translations: Dict of {lang_code: translated_text}
              Example: {"vi": "Háº§m rÆ°á»£u", "en": "Wine cellar", "zh": "é…’çª–"}
            - admin_user_id: ID of admin approving
            - admin_notes: Optional notes

            **Returns**:
            - success: True if successful
            - master_data_id: ID of newly created master data
            - table: Which master data table it was added to
            """
            try:
                return await self.admin_routes.approve_pending_item(
                    pending_id, translations, admin_user_id, admin_notes
                )
            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Failed to approve item: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/admin/reject-item")
        async def reject_pending_item(
            pending_id: int,
            admin_user_id: str,
            admin_notes: Optional[str] = None
        ):
            """
            Reject a pending item

            **Body Parameters**:
            - pending_id: ID of pending item
            - admin_user_id: ID of admin rejecting
            - admin_notes: Reason for rejection

            **Returns**:
            - success: True if successful
            """
            try:
                return await self.admin_routes.reject_pending_item(
                    pending_id, admin_user_id, admin_notes
                )
            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Failed to reject item: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-query-enhanced", response_model=EnhancedExtractionResponse)
        async def extract_from_query_enhanced(request: QueryExtractionRequest):
            """
            **NEW ENHANCED ENDPOINT** - Extract entities using 3-layer pipeline:
            1. NLP Pre-processing (rule-based)
            2. RAG Context Retrieval (similar properties)
            3. Enhanced LLM Extraction (with NLP + RAG context)
            4. Post-Validation (against DB distribution)

            This is the RECOMMENDED endpoint for production use!
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} Enhanced extraction for: '{request.query}'")

                # LAYER 1: NLP Pre-processing
                self.logger.info(f"{LogEmoji.AI} Layer 1: NLP Pre-processing...")
                nlp_entities = self.nlp_processor.extract_entities(request.query)
                nlp_confidence = self.nlp_processor.get_extraction_confidence(nlp_entities)
                self.logger.info(f"{LogEmoji.SUCCESS} NLP extracted {len(nlp_entities)} entities (confidence: {nlp_confidence:.2f})")

                # LAYER 2: RAG Context Retrieval
                self.logger.info(f"{LogEmoji.AI} Layer 2: RAG Context Retrieval...")
                rag_context = await self.rag_enhancer.get_context(
                    query=request.query,
                    nlp_entities=nlp_entities,
                    limit=5
                )
                rag_count = rag_context.get("retrieved_count", 0)
                self.logger.info(f"{LogEmoji.SUCCESS} RAG retrieved {rag_count} similar properties")

                # LAYER 3: Enhanced LLM Extraction
                self.logger.info(f"{LogEmoji.AI} Layer 3: Enhanced LLM Extraction...")
                llm_entities = await self._enhanced_llm_extraction(
                    query=request.query,
                    nlp_entities=nlp_entities,
                    rag_context=rag_context,
                    intent=request.intent
                )
                self.logger.info(f"{LogEmoji.SUCCESS} LLM extracted {len(llm_entities)} entities")

                # LAYER 4: Post-Validation
                self.logger.info(f"{LogEmoji.AI} Layer 4: Post-Validation...")
                validation_result = self.validator.validate(
                    entities=llm_entities,
                    nlp_entities=nlp_entities,
                    rag_context=rag_context
                )

                validated_entities = validation_result["validated_entities"]
                confidence = validation_result["confidence"]
                warnings = validation_result["warnings"]
                validation_details = validation_result["validation_details"]

                # CRITICAL: Normalize entities to English master data standard using PostgreSQL
                # This converts multilingual input (vi/zh) â†’ English for database storage
                self.logger.info(f"{LogEmoji.AI} Normalizing entities using PostgreSQL master data...")
                master_data_result = await self.master_data_validator.normalize_and_validate(
                    validated_entities
                )
                normalized_entities = master_data_result["normalized_entities"]
                master_data_warnings = master_data_result["warnings"]
                master_data_confidence = master_data_result["confidence"]

                # Combine warnings from both validators
                all_warnings = warnings + master_data_warnings

                # Update confidence (weighted average)
                final_confidence = (confidence * 0.5) + (master_data_confidence * 0.5)

                self.logger.info(
                    f"{LogEmoji.SUCCESS} Entities normalized using master data: {normalized_entities}"
                )
                self.logger.info(
                    f"{LogEmoji.INFO} Master data confidence: {master_data_confidence:.2f}, "
                    f"Final confidence: {final_confidence:.2f}"
                )

                self.logger.info(
                    f"{LogEmoji.SUCCESS} Extraction complete! "
                    f"Final confidence: {final_confidence:.2f}, Total warnings: {len(all_warnings)}"
                )

                # NEW: Confidence-based clarification
                # If confidence too low, generate clarification questions
                needs_clarification = final_confidence < 0.7
                clarification_questions = []
                suggestions = []

                if needs_clarification:
                    self.logger.info(f"{LogEmoji.WARNING} Low confidence! Generating clarification questions...")
                    clarification_result = self._generate_clarification(
                        query=request.query,
                        entities=normalized_entities,
                        confidence=final_confidence,
                        rag_context=rag_context
                    )
                    clarification_questions = clarification_result["questions"]
                    suggestions = clarification_result["suggestions"]

                return EnhancedExtractionResponse(
                    entities=normalized_entities,  # Return master-data-normalized entities
                    confidence=final_confidence,
                    extracted_from="enhanced_pipeline_with_master_data",
                    nlp_entities=nlp_entities,
                    rag_retrieved_count=rag_count,
                    warnings=all_warnings,
                    validation_details=validation_details,
                    needs_clarification=needs_clarification,
                    clarification_questions=clarification_questions if needs_clarification else None,
                    suggestions=suggestions if needs_clarification else None
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Enhanced extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-query", response_model=QueryExtractionResponse)
        async def extract_from_query(request: QueryExtractionRequest):
            """
            Extract entities from user search query
            This is what Orchestrator should call for SEARCH intent!
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} Extracting entities from query: '{request.query}'")

                # Build specialized prompt for query extraction (not full property description)
                prompt = self._build_query_extraction_prompt(request.query, request.intent)

                # Call LLM via Core Gateway
                entities = await self._call_llm_for_extraction(prompt)

                # Normalize to English master data
                self.logger.info(f"{LogEmoji.AI} Normalizing query entities to English...")
                normalized_entities = self.multilingual_mapper.normalize_entities(
                    entities,
                    source_lang="vi"
                )

                confidence = self._calculate_confidence(normalized_entities)

                self.logger.info(f"{LogEmoji.SUCCESS} Extracted entities (normalized): {normalized_entities}")

                return QueryExtractionResponse(
                    entities=normalized_entities,
                    confidence=confidence,
                    extracted_from="query"
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Entity extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-property", response_model=QueryExtractionResponse)
        async def extract_from_property_description(request: QueryExtractionRequest):
            """
            Extract full property attributes from property description/listing
            Used for data enrichment, not for user queries
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} Extracting from property description (length: {len(request.query)})")

                # Use full extraction prompt for property descriptions
                prompt = AttributeExtractionPrompts.build_extraction_prompt(request.query, include_examples=True)

                entities = await self._call_llm_for_extraction(prompt)
                confidence = self._calculate_confidence(entities)

                return QueryExtractionResponse(
                    entities=entities,
                    confidence=confidence,
                    extracted_from="property_description"
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Property extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/extract-from-images", response_model=QueryExtractionResponse)
        async def extract_from_images(request: ImageExtractionRequest):
            """
            NEW ENDPOINT: Extract property attributes from images using GPT-4o Vision

            This enables users to upload property photos and automatically extract:
            - Property type (apartment, house, villa)
            - Room count (bedrooms, bathrooms)
            - Amenities (pool, gym, parking, garden)
            - Style (modern, classic, luxury)
            - Condition (new, renovated, needs work)

            Example use cases:
            - User uploads property photos â†’ auto-populate listing
            - User uploads photos while searching â†’ find similar properties
            - Agent uploads photos â†’ AI suggests price range
            """
            try:
                self.logger.info(
                    f"{LogEmoji.TARGET} Extracting from {len(request.images)} images "
                    f"(with context: {bool(request.text_context)})"
                )

                # Build vision prompt for property analysis
                vision_prompt = self._build_vision_extraction_prompt(request.text_context)

                # Call GPT-4o Vision via Core Gateway
                entities = await self._call_vision_for_extraction(
                    images=request.images,
                    prompt=vision_prompt
                )

                # Merge with text context if provided
                if request.text_context:
                    self.logger.info(f"{LogEmoji.AI} Merging vision extraction with text context...")
                    text_entities = await self._call_llm_for_extraction(
                        self._build_query_extraction_prompt(request.text_context)
                    )
                    # Merge entities (vision takes precedence for visual attributes)
                    entities = {**text_entities, **entities}

                confidence = self._calculate_confidence(entities)
                self.logger.info(f"{LogEmoji.SUCCESS} Extracted {len(entities)} entities from images")

                return QueryExtractionResponse(
                    entities=entities,
                    confidence=confidence,
                    extracted_from="images_vision"
                )

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} Image extraction failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))

    def _build_query_extraction_prompt(self, query: str, intent: Optional[str] = None) -> str:
        """
        Build specialized prompt for extracting entities from USER QUERIES
        Simpler than full property extraction - focuses on search criteria
        """
        return f"""Báº¡n lÃ  chuyÃªn gia trÃ­ch xuáº¥t thÃ´ng tin tÃ¬m kiáº¿m báº¥t Ä‘á»™ng sáº£n tá»« cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.

ðŸŽ¯ NHIá»†M Vá»¤:
Äá»c cÃ¢u há»i cá»§a user vÃ  trÃ­ch xuáº¥t CÃC TIÃŠU CHÃ TÃŒM KIáº¾M thÃ nh JSON.

ðŸ“Š ENTITIES Cáº¦N TRÃCH XUáº¤T (chá»‰ trÃ­ch xuáº¥t nhá»¯ng gÃ¬ cÃ³ trong cÃ¢u há»i):

**1. PROPERTY TYPE**
- property_type: cÄƒn há»™ | nhÃ  phá»‘ | biá»‡t thá»± | Ä‘áº¥t | chung cÆ° | commercial

**2. LOCATION**
- district: Quáº­n/Huyá»‡n (chuáº©n hÃ³a)
  * "Q7", "Q.7", "quáº­n 7" â†’ "Quáº­n 7"
  * "Q2" â†’ "Quáº­n 2"
  * "BÃ¬nh Tháº¡nh" â†’ "Quáº­n BÃ¬nh Tháº¡nh"
  * "Thá»§ Äá»©c" â†’ "Quáº­n Thá»§ Äá»©c"
- ward: PhÆ°á»ng (náº¿u cÃ³)
- project_name: TÃªn dá»± Ã¡n (Vinhomes, Masteri, etc.)

**3. PHYSICAL ATTRIBUTES**
- bedrooms: Sá»‘ phÃ²ng ngá»§
  * "2PN" â†’ 2
  * "3 phÃ²ng ngá»§" â†’ 3
  * "2 phÃ²ng" â†’ 2
- bathrooms: Sá»‘ phÃ²ng táº¯m/WC
- area: Diá»‡n tÃ­ch (mÂ²)
- min_area: Diá»‡n tÃ­ch tá»‘i thiá»ƒu
- max_area: Diá»‡n tÃ­ch tá»‘i Ä‘a

**4. PRICE**
- price: GiÃ¡ cá»¥ thá»ƒ (VND)
- min_price: GiÃ¡ tá»‘i thiá»ƒu (VND)
- max_price: GiÃ¡ tá»‘i Ä‘a (VND)

CHUáº¨N HÃ“A GIÃ:
  * "dÆ°á»›i 3 tá»·" â†’ max_price: 3000000000
  * "tá»« 2 Ä‘áº¿n 5 tá»·" â†’ min_price: 2000000000, max_price: 5000000000
  * "khoáº£ng 3 tá»·" â†’ price: 3000000000
  * "25 triá»‡u/thÃ¡ng" â†’ price: 25000000

**5. FEATURES**
- furniture: full | cÆ¡ báº£n | khÃ´ng
- direction: ÄÃ´ng | TÃ¢y | Nam | Báº¯c | etc.

**6. AMENITIES**
- parking: true náº¿u cÃ³ yÃªu cáº§u chá»— Ä‘áº­u xe
- elevator: true náº¿u cÃ³ yÃªu cáº§u thang mÃ¡y
- swimming_pool: true náº¿u cÃ³ yÃªu cáº§u há»“ bÆ¡i
- gym: true náº¿u cÃ³ yÃªu cáº§u gym

ðŸ” EXTRACTION RULES:

1. **Chá»‰ trÃ­ch xuáº¥t thÃ´ng tin CÃ“ TRONG CÃ‚U Há»ŽI** - Ä‘á»«ng bá»‹a thÃªm
2. **Chuáº©n hÃ³a Ä‘á»‹a danh** vá» format chuáº©n
3. **Chuáº©n hÃ³a giÃ¡** vá» sá»‘ VND
4. **Náº¿u khÃ´ng cÃ³ thÃ´ng tin** â†’ khÃ´ng Ä‘Æ°a vÃ o JSON (not null, just omit)
5. **Æ¯u tiÃªn tá»« khÃ³a rÃµ rÃ ng** hÆ¡n tá»« khÃ³a mÆ¡ há»“

ðŸ“ FEW-SHOT EXAMPLES:

Example 1:
Input: "TÃ¬m cÄƒn há»™ 2 phÃ²ng ngá»§ quáº­n 7 dÆ°á»›i 3 tá»·"
Output: {{"property_type": "cÄƒn há»™", "bedrooms": 2, "district": "Quáº­n 7", "max_price": 3000000000}}

Example 2:
Input: "Cáº§n mua nhÃ  phá»‘ BÃ¬nh Tháº¡nh khoáº£ng 100m2"
Output: {{"property_type": "nhÃ  phá»‘", "district": "Quáº­n BÃ¬nh Tháº¡nh", "area": 100}}

Example 3:
Input: "TÃ¬m chung cÆ° Vinhomes cÃ³ há»“ bÆ¡i"
Output: {{"property_type": "chung cÆ°", "project_name": "Vinhomes", "swimming_pool": true}}

Example 4:
Input: "Biá»‡t thá»± Q2 tá»« 10 Ä‘áº¿n 20 tá»·, cÃ³ garage"
Output: {{"property_type": "biá»‡t thá»±", "district": "Quáº­n 2", "min_price": 10000000000, "max_price": 20000000000, "parking": true}}

ðŸ“¥ USER QUERY:
{query}

Intent: {intent or "SEARCH"}

ðŸ“¤ OUTPUT (chá»‰ JSON, khÃ´ng giáº£i thÃ­ch):
"""

    async def _call_llm_for_extraction(self, prompt: str) -> Dict[str, Any]:
        """Call LLM via Core Gateway to extract entities"""
        try:
            llm_request = {
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "You are a structured data extraction expert. Always return valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 500,
                "temperature": 0.2  # Low temperature for consistent extraction
            }

            response = await self.http_client.post(
                f"{self.core_gateway_url}/chat/completions",
                json=llm_request,
                timeout=30.0
            )

            if response.status_code == 200:
                data = response.json()
                content = data.get("content", "").strip()

                # Clean up markdown code blocks
                content = re.sub(r'^```(?:json)?\s*\n?', '', content)
                content = re.sub(r'\n?```\s*$', '', content)
                content = content.strip()

                # Parse JSON
                entities = json.loads(content)
                return entities
            else:
                self.logger.warning(f"{LogEmoji.WARNING} Core Gateway returned {response.status_code}")
                return {}

        except json.JSONDecodeError as e:
            self.logger.error(f"{LogEmoji.ERROR} Failed to parse JSON: {e}")
            self.logger.error(f"Raw content: {content}")
            return {}
        except Exception as e:
            self.logger.error(f"{LogEmoji.ERROR} LLM call failed: {e}")
            return {}

    def _calculate_confidence(self, entities: Dict[str, Any]) -> float:
        """Calculate confidence score based on extracted entities"""
        if not entities:
            return 0.0

        # Simple confidence based on number of entities extracted
        num_entities = len(entities)
        if num_entities >= 4:
            return 0.95
        elif num_entities >= 3:
            return 0.85
        elif num_entities >= 2:
            return 0.75
        else:
            return 0.65

    async def _enhanced_llm_extraction(
        self,
        query: str,
        nlp_entities: Dict[str, Any],
        rag_context: Dict[str, Any],
        intent: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Enhanced LLM extraction with NLP hints and RAG context.

        This builds a rich prompt that includes:
        1. NLP pre-extracted entities (as hints)
        2. Real property examples from RAG
        3. Value ranges and patterns from DB
        """
        # Build enhanced prompt
        prompt = self._build_enhanced_prompt(query, nlp_entities, rag_context, intent)

        # Call LLM
        entities = await self._call_llm_for_extraction(prompt)

        return entities

    def _build_enhanced_prompt(
        self,
        query: str,
        nlp_entities: Dict[str, Any],
        rag_context: Dict[str, Any],
        intent: Optional[str] = None
    ) -> str:
        """
        Build enhanced prompt with NLP hints and RAG context.
        """
        # Get RAG components
        examples = rag_context.get("examples", [])
        patterns = rag_context.get("patterns", {})
        value_ranges = rag_context.get("value_ranges", {})

        # Format examples
        examples_text = ""
        if examples:
            examples_text = "ðŸ“š REAL PROPERTY EXAMPLES FROM DATABASE (similar to this query):\n"
            for i, ex in enumerate(examples[:3], 1):
                examples_text += f"\nExample {i}:\n{json.dumps(ex, indent=2, ensure_ascii=False)}\n"

        # Format patterns
        patterns_text = ""
        if patterns:
            patterns_text = "ðŸ“Š COMMON PATTERNS IN SIMILAR PROPERTIES:\n"
            if "common_districts" in patterns:
                districts = [d["value"] for d in patterns["common_districts"][:3]]
                patterns_text += f"- Common districts: {', '.join(districts)}\n"
            if "common_property_types" in patterns:
                types = [t["value"] for t in patterns["common_property_types"][:3]]
                patterns_text += f"- Common property types: {', '.join(types)}\n"

        # Format value ranges
        ranges_text = ""
        if value_ranges:
            ranges_text = "ðŸ“ˆ VALUE RANGES FROM SIMILAR PROPERTIES:\n"
            if "price" in value_ranges:
                pr = value_ranges["price"]
                ranges_text += f"- Price range: {pr['min']:,.0f} - {pr['max']:,.0f} VND (avg: {pr['avg']:,.0f})\n"
            if "area" in value_ranges:
                ar = value_ranges["area"]
                ranges_text += f"- Area range: {ar['min']:.0f} - {ar['max']:.0f} mÂ² (avg: {ar['avg']:.0f})\n"

        # Format NLP hints
        nlp_hints_text = ""
        if nlp_entities:
            nlp_hints_text = f"ðŸ’¡ NLP PRE-EXTRACTED HINTS:\n{json.dumps(nlp_entities, indent=2, ensure_ascii=False)}\n"

        prompt = f"""Báº¡n lÃ  chuyÃªn gia trÃ­ch xuáº¥t thÃ´ng tin báº¥t Ä‘á»™ng sáº£n.

ðŸŽ¯ NHIá»†M Vá»¤: TrÃ­ch xuáº¥t entities tá»« query, Sá»¬ Dá»¤NG HINTS tá»« NLP vÃ  EXAMPLES tá»« database.

{nlp_hints_text}

{examples_text}

{patterns_text}

{ranges_text}

ðŸ” EXTRACTION RULES:
1. **USE NLP hints as starting point** - Æ¯u tiÃªn thÃ´ng tin tá»« NLP layer
2. **FOLLOW patterns from real examples** - Tham kháº£o format tá»« DB
3. **STAY within typical value ranges** - Kiá»ƒm tra vá»›i ranges tá»« DB
4. **DON'T hallucinate** - Chá»‰ trÃ­ch xuáº¥t thÃ´ng tin cÃ³ trong query
5. **Chuáº©n hÃ³a format** - Sá»­ dá»¥ng format giá»‘ng examples

ðŸ“Š ENTITIES Cáº¦N TRÃCH XUáº¤T (chá»‰ trÃ­ch xuáº¥t nhá»¯ng gÃ¬ cÃ³ trong cÃ¢u há»i):

**1. PROPERTY TYPE**
- property_type: cÄƒn há»™ | nhÃ  phá»‘ | biá»‡t thá»± | Ä‘áº¥t | chung cÆ° | vÄƒn phÃ²ng

**2. LOCATION**
- district: Quáº­n/Huyá»‡n (chuáº©n hÃ³a nhÆ° examples)
- ward: PhÆ°á»ng (náº¿u cÃ³)
- project_name: TÃªn dá»± Ã¡n

**3. PHYSICAL ATTRIBUTES**
- bedrooms: Sá»‘ phÃ²ng ngá»§
- bathrooms: Sá»‘ phÃ²ng táº¯m
- area: Diá»‡n tÃ­ch (mÂ²)
- floors: Sá»‘ táº§ng

**4. PRICE**
- price: GiÃ¡ cá»¥ thá»ƒ (VND)
- min_price: GiÃ¡ tá»‘i thiá»ƒu (VND)
- max_price: GiÃ¡ tá»‘i Ä‘a (VND)

**5. FEATURES & AMENITIES**
- furniture: full | cÆ¡ báº£n | khÃ´ng
- direction: HÆ°á»›ng nhÃ 
- parking: true/false
- elevator: true/false
- swimming_pool: true/false
- gym: true/false

ðŸ“¥ USER QUERY:
{query}

Intent: {intent or "SEARCH"}

ðŸ“¤ OUTPUT (chá»‰ JSON, khÃ´ng giáº£i thÃ­ch):
"""
        return prompt

    def _build_vision_extraction_prompt(self, text_context: Optional[str] = None) -> str:
        """
        Build prompt for GPT-4o Vision to extract property attributes from images.

        Args:
            text_context: Optional text context to enhance extraction

        Returns:
            Vision prompt
        """
        prompt = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch hÃ¬nh áº£nh báº¥t Ä‘á»™ng sáº£n.

ðŸŽ¯ NHIá»†M Vá»¤: PhÃ¢n tÃ­ch áº£nh property vÃ  trÃ­ch xuáº¥t thÃ´ng tin thÃ nh JSON.

ðŸ“Š ENTITIES Cáº¦N TRÃCH XUáº¤T (chá»‰ trÃ­ch xuáº¥t nhá»¯ng gÃ¬ THáº¤Y ÄÆ¯á»¢C trong áº£nh):

**1. PROPERTY TYPE (quan trá»ng nháº¥t)**
- property_type: cÄƒn há»™ | nhÃ  phá»‘ | biá»‡t thá»± | Ä‘áº¥t | studio | penthouse
  * NhÃ¬n vÃ o: Cáº¥u trÃºc, quy mÃ´, kiáº¿n trÃºc
  * CÄƒn há»™: Trong chung cÆ°, cÃ³ ban cÃ´ng nhá»
  * NhÃ  phá»‘: Nhiá»u táº§ng, máº·t tiá»n háº¹p
  * Biá»‡t thá»±: Rá»™ng rÃ£i, cÃ³ sÃ¢n vÆ°á»n

**2. ROOMS & SPACES**
- bedrooms: Sá»‘ phÃ²ng ngá»§ (Ä‘áº¿m tá»« áº£nh - giÆ°á»ng, tá»§ quáº§n Ã¡o)
- bathrooms: Sá»‘ phÃ²ng táº¯m (nhÃ¬n tháº¥y toilet, bá»“n táº¯m)
- living_rooms: Sá»‘ phÃ²ng khÃ¡ch
- kitchens: CÃ³ báº¿p khÃ´ng
- balconies: CÃ³ ban cÃ´ng khÃ´ng

**3. AMENITIES (Tiá»‡n Ã­ch nhÃ¬n tháº¥y)**
- swimming_pool: true náº¿u tháº¥y há»“ bÆ¡i
- gym: true náº¿u tháº¥y phÃ²ng gym
- parking: true náº¿u tháº¥y chá»— Ä‘áº­u xe/garage
- garden: true náº¿u tháº¥y sÃ¢n vÆ°á»n
- elevator: true náº¿u tháº¥y thang mÃ¡y
- security: CÃ³ há»‡ thá»‘ng báº£o vá»‡ khÃ´ng (camera, cá»•ng báº£o vá»‡)

**4. STYLE & CONDITION**
- style: modern | classic | luxury | minimalist | industrial
  * Modern: ÄÆ°á»ng nÃ©t sáº¡ch sáº½, tá»‘i giáº£n, mÃ u tráº¯ng/ghi
  * Classic: Gá»—, há»a tiáº¿t cá»• Ä‘iá»ƒn
  * Luxury: Sang trá»ng, Ä‘Ã¨n chÃ¹m, Ä‘Ã¡ marble
- condition: new | excellent | good | needs_renovation
  * New: Má»›i tinh, sáº¡ch sáº½
  * Excellent: Tá»‘t, Ä‘Æ°á»£c maintain tá»‘t
  * Good: á»”n, cÃ³ dáº¥u hiá»‡u sá»­ dá»¥ng nháº¹
  * Needs renovation: CÅ©, cáº§n sá»­a chá»¯a
- furnished: full | basic | unfurnished
  * Full: Äáº§y Ä‘á»§ ná»™i tháº¥t (giÆ°á»ng, bÃ n, gháº¿, TV, tá»§ láº¡nh)
  * Basic: CÃ³ má»™t sá»‘ ná»™i tháº¥t cÆ¡ báº£n
  * Unfurnished: Trá»‘ng, khÃ´ng ná»™i tháº¥t

**5. VISUAL FEATURES**
- view: sea | city | garden | mountain | river (náº¿u tháº¥y qua cá»­a sá»•)
- direction: HÆ°á»›ng ban cÃ´ng/cá»­a sá»• chÃ­nh
- natural_light: high | medium | low (lÆ°á»£ng Ã¡nh sÃ¡ng tá»± nhiÃªn)
- floor_material: wood | tile | marble | carpet (váº­t liá»‡u sÃ n nhÃ )

**6. ESTIMATE (Æ¯á»›c lÆ°á»£ng tá»« áº£nh)**
- estimated_area_m2: Æ¯á»›c lÆ°á»£ng diá»‡n tÃ­ch (náº¿u cÃ³ thá»ƒ)
- estimated_floors: Sá»‘ táº§ng (náº¿u tháº¥y)

ðŸ” EXTRACTION RULES:

1. **CHá»ˆ trÃ­ch xuáº¥t thÃ´ng tin THáº¤Y RÃ• TRONG áº¢NH** - Ä‘á»«ng Ä‘oÃ¡n
2. **Æ¯u tiÃªn Ä‘á»™ chÃ­nh xÃ¡c** hÆ¡n lÃ  extract nhiá»u
3. **Náº¿u khÃ´ng cháº¯c â†’ khÃ´ng Ä‘Æ°a vÃ o JSON** (omit field, don't guess)
4. **Äáº¿m phÃ²ng cáº©n tháº­n** - Ä‘á»«ng nháº§m láº«n
5. **PhÃ¢n tÃ­ch táº¥t cáº£ cÃ¡c áº£nh** náº¿u cÃ³ nhiá»u áº£nh

ðŸ“ EXAMPLES:

Example 1 (Luxury apartment):
Input: áº¢nh cÄƒn há»™ cao cáº¥p, phÃ²ng khÃ¡ch rá»™ng, sofa tráº¯ng, view thÃ nh phá»‘
Output: {
  "property_type": "cÄƒn há»™",
  "living_rooms": 1,
  "style": "luxury",
  "furnished": "full",
  "view": "city",
  "natural_light": "high",
  "floor_material": "marble",
  "condition": "excellent"
}

Example 2 (Villa with pool):
Input: áº¢nh biá»‡t thá»± 2 táº§ng, há»“ bÆ¡i ngoÃ i trá»i, sÃ¢n vÆ°á»n rá»™ng
Output: {
  "property_type": "biá»‡t thá»±",
  "swimming_pool": true,
  "garden": true,
  "estimated_floors": 2,
  "style": "modern",
  "natural_light": "high",
  "condition": "new"
}

Example 3 (Simple bedroom):
Input: áº¢nh phÃ²ng ngá»§ Ä‘Æ¡n giáº£n, giÆ°á»ng Ä‘Æ¡n, tá»§ quáº§n Ã¡o
Output: {
  "bedrooms": 1,
  "furnished": "basic",
  "style": "minimalist",
  "condition": "good",
  "floor_material": "wood"
}
"""

        if text_context:
            prompt += f"\n\nðŸ“ TEXT CONTEXT (from user):\n{text_context}\n\nUse this context to enhance extraction, but PRIORITIZE what you SEE in images.\n"

        prompt += "\nðŸ“¤ OUTPUT (chá»‰ JSON, khÃ´ng giáº£i thÃ­ch):"

        return prompt

    async def _call_vision_for_extraction(
        self,
        images: List[str],
        prompt: str
    ) -> Dict[str, Any]:
        """
        Call GPT-4o Vision via Core Gateway to extract entities from images.

        Args:
            images: List of base64-encoded images
            prompt: Extraction prompt

        Returns:
            Extracted entities dict
        """
        try:
            from shared.models.core_gateway import FileAttachment

            # Build vision request
            files = [
                FileAttachment(
                    filename=f"property_{i}.jpg",
                    mime_type="image/jpeg",
                    base64_data=img
                )
                for i, img in enumerate(images)
            ]

            # Use Core Gateway's vision endpoint
            llm_request = {
                "model": "gpt-4o",  # GPT-4o for vision
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a property image analysis expert. Always return valid JSON."
                    },
                    {
                        "role": "user",
                        "content": prompt,
                        "files": [f.dict() for f in files]
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.2
            }

            response = await self.http_client.post(
                f"{self.core_gateway_url}/chat/completions",
                json=llm_request,
                timeout=60.0
            )

            if response.status_code == 200:
                data = response.json()
                content = data.get("content", "").strip()

                # Clean up markdown code blocks
                import re
                content = re.sub(r'^```(?:json)?\s*\n?', '', content)
                content = re.sub(r'\n?```\s*$', '', content)
                content = content.strip()

                # Parse JSON
                import json
                entities = json.loads(content)
                return entities
            else:
                self.logger.warning(f"{LogEmoji.WARNING} Vision API returned {response.status_code}")
                return {}

        except json.JSONDecodeError as e:
            self.logger.error(f"{LogEmoji.ERROR} Failed to parse vision response: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"{LogEmoji.ERROR} Vision call failed: {e}")
            return {}

    def _generate_clarification(
        self,
        query: str,
        entities: Dict[str, Any],
        confidence: float,
        rag_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Generate clarification questions and suggestions when confidence is low.

        Args:
            query: User query
            entities: Extracted entities
            confidence: Confidence score
            rag_context: RAG context with similar properties

        Returns:
            Dict with clarification questions and suggestions
        """
        questions = []
        suggestions = []

        # Check what's missing or unclear
        has_property_type = "property_type" in entities
        has_location = "district" in entities or "ward" in entities
        has_price = "price" in entities or "min_price" in entities or "max_price" in entities
        has_bedrooms = "bedrooms" in entities

        # Generate questions based on missing info
        if not has_property_type:
            questions.append("Báº¡n muá»‘n tÃ¬m loáº¡i báº¥t Ä‘á»™ng sáº£n nÃ o? (cÄƒn há»™, nhÃ  phá»‘, biá»‡t thá»±)")
            # Get suggestions from RAG
            if rag_context.get("patterns", {}).get("common_property_types"):
                property_type_suggestions = [
                    {
                        "value": pt["value"],
                        "count": pt["count"],
                        "label": f"{pt['value']} ({pt['count']} cÄƒn)"
                    }
                    for pt in rag_context["patterns"]["common_property_types"][:3]
                ]
                suggestions.append({
                    "field": "property_type",
                    "question": "Loáº¡i báº¥t Ä‘á»™ng sáº£n",
                    "options": property_type_suggestions
                })

        if not has_location:
            questions.append("Báº¡n muá»‘n tÃ¬m á»Ÿ khu vá»±c nÃ o? (Quáº­n/Huyá»‡n)")
            # Get suggestions from RAG
            if rag_context.get("patterns", {}).get("common_districts"):
                district_suggestions = [
                    {
                        "value": d["value"],
                        "count": d["count"],
                        "label": f"{d['value']} ({d['count']} cÄƒn)"
                    }
                    for d in rag_context["patterns"]["common_districts"][:5]
                ]
                suggestions.append({
                    "field": "district",
                    "question": "Khu vá»±c",
                    "options": district_suggestions
                })

        if not has_price:
            questions.append("NgÃ¢n sÃ¡ch cá»§a báº¡n lÃ  bao nhiÃªu?")
            # Get price range from RAG
            if rag_context.get("value_ranges", {}).get("price"):
                price_range = rag_context["value_ranges"]["price"]
                price_suggestions = [
                    {
                        "value": "low",
                        "min": price_range["min"],
                        "max": price_range["avg"] * 0.7,
                        "label": f"DÆ°á»›i {price_range['avg'] * 0.7 / 1_000_000_000:.1f} tá»·"
                    },
                    {
                        "value": "medium",
                        "min": price_range["avg"] * 0.7,
                        "max": price_range["avg"] * 1.3,
                        "label": f"{price_range['avg'] * 0.7 / 1_000_000_000:.1f} - {price_range['avg'] * 1.3 / 1_000_000_000:.1f} tá»·"
                    },
                    {
                        "value": "high",
                        "min": price_range["avg"] * 1.3,
                        "max": price_range["max"],
                        "label": f"TrÃªn {price_range['avg'] * 1.3 / 1_000_000_000:.1f} tá»·"
                    }
                ]
                suggestions.append({
                    "field": "price",
                    "question": "NgÃ¢n sÃ¡ch",
                    "options": price_suggestions
                })

        if not has_bedrooms:
            questions.append("Báº¡n cáº§n bao nhiÃªu phÃ²ng ngá»§?")
            suggestions.append({
                "field": "bedrooms",
                "question": "Sá»‘ phÃ²ng ngá»§",
                "options": [
                    {"value": 1, "label": "1 phÃ²ng ngá»§ (Studio)"},
                    {"value": 2, "label": "2 phÃ²ng ngá»§"},
                    {"value": 3, "label": "3 phÃ²ng ngá»§"},
                    {"value": 4, "label": "4+ phÃ²ng ngá»§"}
                ]
            })

        # If entities were extracted but confidence still low, ask for confirmation
        if entities and confidence < 0.6:
            questions.append(
                f"TÃ´i hiá»ƒu báº¡n Ä‘ang tÃ¬m: {self._format_entities_for_display(entities)}. ÄÃºng khÃ´ng?"
            )

        return {
            "questions": questions,
            "suggestions": suggestions,
            "reason": f"Confidence tháº¥p ({confidence:.2f}), cáº§n lÃ m rÃµ thÃªm thÃ´ng tin"
        }

    def _format_entities_for_display(self, entities: Dict[str, Any]) -> str:
        """Format entities for user-friendly display."""
        parts = []
        if "property_type" in entities:
            parts.append(entities["property_type"])
        if "bedrooms" in entities:
            parts.append(f"{entities['bedrooms']} phÃ²ng ngá»§")
        if "district" in entities:
            parts.append(f"táº¡i {entities['district']}")
        if "max_price" in entities:
            parts.append(f"dÆ°á»›i {entities['max_price'] / 1_000_000_000:.1f} tá»·")
        elif "price" in entities:
            parts.append(f"khoáº£ng {entities['price'] / 1_000_000_000:.1f} tá»·")

        return " ".join(parts) if parts else "báº¥t Ä‘á»™ng sáº£n"

    async def on_shutdown(self):
        """Cleanup on shutdown"""
        await self.http_client.aclose()
        await self.rag_enhancer.close()
        await self.master_data_validator.close()
        await self.master_data_extractor.close()
        await self.admin_routes.close()  # NEW: Cleanup admin routes
        await super().on_shutdown()


# Create service instance at module level for uvicorn
service = AttributeExtractionService()
app = service.app

if __name__ == "__main__":
    service.run()
