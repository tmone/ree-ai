"""
RAG Service - Enhanced with Modular RAG + Memory + Multi-Agent
Layer 6 - Production-Ready Advanced RAG Pipeline

NEW FEATURES (vs basic main.py):
- Modular RAG operators (grading, reranking, query rewriting)
- Agentic memory system (episodic, semantic, procedural)
- Multi-agent coordination (supervisor + specialists)
- Self-reflection and quality control
- Advanced operators (HyDE, decomposition, reflection)

BACKWARD COMPATIBLE: Falls back to basic pipeline if advanced features disabled
"""
import httpx
from typing import Dict, Any, List, Optional
from fastapi import HTTPException
from pydantic import BaseModel
import os

from core.base_service import BaseService
from shared.config import settings
from shared.utils.logger import LogEmoji

# Import new modular RAG components
try:
    from shared.rag_operators.operators import (
        HybridRetrievalOperator,
        DocumentGraderOperator,
        RerankOperator,
        QueryRewriterOperator,
        GenerationOperator
    )
    from shared.rag_operators.operators.hyde import HyDEOperator
    from shared.rag_operators.operators.query_decomposition import QueryDecompositionOperator
    from shared.rag_operators.operators.reflection import ReflectionOperator
    from shared.memory import MemoryManager
    from shared.agents import SupervisorAgent
    ADVANCED_FEATURES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Advanced features not available: {e}")
    ADVANCED_FEATURES_AVAILABLE = False


class RAGQueryRequest(BaseModel):
    """Request for RAG query"""
    query: str
    filters: Optional[Dict[str, Any]] = None
    limit: int = 5
    user_id: Optional[str] = None  # NEW: For memory personalization
    use_advanced_rag: bool = True  # NEW: Enable/disable advanced features


class RAGQueryResponse(BaseModel):
    """Response from RAG query"""
    response: str
    retrieved_count: int
    confidence: float
    sources: List[Dict[str, Any]] = []
    # NEW: Advanced metadata
    pipeline_used: Optional[str] = None
    quality_score: Optional[float] = None
    operators_executed: Optional[List[str]] = None
    memory_context_used: Optional[bool] = None


class EnhancedRAGService(BaseService):
    """
    Enhanced RAG Service with Modular RAG + Memory + Multi-Agent

    Features:
    1. Modular RAG Pipeline:
       - Query rewriting (fix typos, ambiguity)
       - HyDE or Query Decomposition (for complex queries)
       - Hybrid retrieval (vector + BM25)
       - Document grading (filter irrelevant)
       - Semantic reranking (optimize order)
       - LLM generation (natural language)
       - Reflection (self-critique)

    2. Agentic Memory:
       - Episodic: Learn from past interactions
       - Semantic: Pre-loaded domain knowledge
       - Procedural: Learned skills/strategies

    3. Multi-Agent System:
       - Supervisor coordination
       - Specialist agents (search, grade, rerank, critique)
       - Performance tracking
    """

    def __init__(self):
        super().__init__(
            name="enhanced_rag_service",
            version="2.0.0",
            capabilities=[
                "retrieval_augmented_generation",
                "modular_rag",
                "agentic_memory",
                "multi_agent",
                "self_reflection"
            ],
            port=8091
        )

        self.http_client = httpx.AsyncClient(timeout=90.0)
        self.db_gateway_url = settings.get_db_gateway_url()
        self.core_gateway_url = settings.get_core_gateway_url()

        # Initialize advanced components if available
        self.advanced_enabled = ADVANCED_FEATURES_AVAILABLE and os.getenv("USE_ADVANCED_RAG", "true").lower() == "true"

        if self.advanced_enabled:
            self.logger.info(f"{LogEmoji.SUCCESS} Advanced RAG features enabled")
            self._setup_advanced_components()
        else:
            self.logger.warning(f"{LogEmoji.WARNING} Advanced RAG features disabled (fallback to basic)")
            self.memory_manager = None
            self.supervisor = None

        self.logger.info(f"{LogEmoji.INFO} DB Gateway: {self.db_gateway_url}")
        self.logger.info(f"{LogEmoji.INFO} Core Gateway: {self.core_gateway_url}")

    def _setup_advanced_components(self):
        """Initialize advanced RAG components"""
        try:
            # Memory system
            self.memory_manager = MemoryManager()
            self.logger.info(f"{LogEmoji.SUCCESS} Memory system initialized")

            # Multi-agent system
            self.supervisor = SupervisorAgent()
            self.logger.info(f"{LogEmoji.SUCCESS} Multi-agent system initialized")

            # Individual operators (for custom pipelines)
            self.query_rewriter = QueryRewriterOperator(core_gateway_url=self.core_gateway_url)
            self.hyde_operator = HyDEOperator(core_gateway_url=self.core_gateway_url)
            self.decomposition_operator = QueryDecompositionOperator(core_gateway_url=self.core_gateway_url)
            self.document_grader = DocumentGraderOperator()
            self.reranker = RerankOperator()
            self.reflection_operator = ReflectionOperator(core_gateway_url=self.core_gateway_url)
            self.generation_operator = GenerationOperator(core_gateway_url=self.core_gateway_url)

            self.logger.info(f"{LogEmoji.SUCCESS} All operators initialized")

        except Exception as e:
            self.logger.error(f"{LogEmoji.ERROR} Failed to setup advanced components: {e}")
            self.advanced_enabled = False
            self.memory_manager = None
            self.supervisor = None

    def setup_routes(self):
        """Setup RAG API routes"""

        @self.app.post("/query", response_model=RAGQueryResponse)
        async def rag_query(request: RAGQueryRequest):
            """
            Main RAG endpoint - Intelligent Retrieval-Augmented Generation

            Automatically selects best pipeline:
            - Advanced: Modular RAG + Memory + Multi-Agent (if enabled)
            - Basic: Simple retrieve â†’ augment â†’ generate (fallback)
            """
            try:
                self.logger.info(f"{LogEmoji.TARGET} RAG Query: '{request.query}'")

                # Decide pipeline based on availability and request
                use_advanced = self.advanced_enabled and request.use_advanced_rag

                if use_advanced:
                    self.logger.info(f"{LogEmoji.AI} Using ADVANCED pipeline (Modular RAG + Memory + Agents)")
                    return await self._advanced_rag_pipeline(request)
                else:
                    self.logger.info(f"{LogEmoji.INFO} Using BASIC pipeline (Simple RAG)")
                    return await self._basic_rag_pipeline(request)

            except Exception as e:
                self.logger.error(f"{LogEmoji.ERROR} RAG query failed: {e}")
                raise HTTPException(status_code=500, detail=f"RAG query failed: {str(e)}")

        @self.app.get("/stats")
        async def get_stats():
            """Get RAG service statistics"""
            if not self.advanced_enabled:
                return {"advanced_features": False, "message": "Basic RAG only"}

            stats = {
                "advanced_features": True,
                "memory_stats": self.memory_manager.get_memory_stats() if self.memory_manager else {},
                "agent_stats": self.supervisor.get_all_agent_stats() if self.supervisor else {}
            }
            return stats

    async def _advanced_rag_pipeline(self, request: RAGQueryRequest) -> RAGQueryResponse:
        """
        ADVANCED PIPELINE: Full Modular RAG + Memory + Multi-Agent

        Flow:
        1. Retrieve memory context (if user_id provided)
        2. Check procedural memory for applicable skills
        3. Query enhancement (rewrite/HyDE/decomposition if needed)
        4. Multi-agent search (supervisor delegates to specialists)
        5. Reflection and quality check
        6. Record interaction in memory
        """
        operators_executed = []
        memory_context_used = False

        # STEP 1: Memory retrieval
        memory_context = None
        if request.user_id and self.memory_manager:
            self.logger.info(f"{LogEmoji.AI} Retrieving memory context for user: {request.user_id}")
            memory_context = await self.memory_manager.retrieve_context_for_query(
                request.user_id,
                request.query
            )
            memory_context_used = True
            operators_executed.append("memory_retrieval")

            # Log memory insights
            if memory_context.get("user_preferences"):
                prefs = memory_context["user_preferences"]
                self.logger.info(f"{LogEmoji.INFO} User preferences: {prefs}")

            if memory_context.get("applicable_skills"):
                skills = memory_context["applicable_skills"]
                self.logger.info(f"{LogEmoji.SUCCESS} Found {len(skills)} applicable skills")

        # STEP 2: Query enhancement (if needed)
        enhanced_query = request.query

        # Check if query is complex enough for decomposition
        if self._is_complex_query(request.query):
            self.logger.info(f"{LogEmoji.AI} Complex query detected, using decomposition")
            decomp_result = await self.decomposition_operator.execute({"query": request.query})
            if decomp_result.success:
                sub_queries = decomp_result.data.sub_queries
                self.logger.info(f"{LogEmoji.SUCCESS} Decomposed into {len(sub_queries)} sub-queries")
                operators_executed.append("query_decomposition")
                # For now, use first sub-query (in production, could search all and merge)
                enhanced_query = sub_queries[0] if sub_queries else request.query

        # STEP 3: Multi-agent search
        self.logger.info(f"{LogEmoji.AI} Executing multi-agent search pipeline")
        search_task = {
            "query": enhanced_query,
            "filters": request.filters or {},
            "limit": request.limit
        }

        supervisor_result = await self.supervisor.execute(search_task)
        operators_executed.append("multi_agent_search")

        if not supervisor_result.success or not supervisor_result.data:
            # Fallback to basic search
            self.logger.warning(f"{LogEmoji.WARNING} Multi-agent search failed, falling back to basic")
            return await self._basic_rag_pipeline(request)

        retrieved_properties = supervisor_result.data
        self.logger.info(f"{LogEmoji.SUCCESS} Multi-agent pipeline retrieved {len(retrieved_properties)} properties")

        # STEP 4: Generate response
        context = self._build_context(retrieved_properties, request.query)
        generation_result = await self.generation_operator.execute({
            "query": request.query,
            "documents": [{"content": context}],
            "system_prompt": "Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n báº¥t Ä‘á»™ng sáº£n REE AI. HÃ£y táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn, há»¯u Ã­ch dá»±a trÃªn dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p."
        })
        operators_executed.append("generation")

        if not generation_result.success:
            self.logger.error(f"{LogEmoji.ERROR} Generation failed")
            return await self._basic_rag_pipeline(request)

        generated_response = generation_result.data.response

        # STEP 5: Reflection (quality check)
        self.logger.info(f"{LogEmoji.AI} Performing self-reflection on response")
        reflection_result = await self.reflection_operator.execute({
            "query": request.query,
            "response": generated_response,
            "sources": retrieved_properties
        })
        operators_executed.append("reflection")

        quality_score = 0.8  # Default
        if reflection_result.success:
            quality_score = reflection_result.data.quality_score
            self.logger.info(f"{LogEmoji.SUCCESS} Quality score: {quality_score:.2f}")

            # If quality is low and needs improvement, could retry here
            if reflection_result.data.needs_improvement:
                self.logger.warning(f"{LogEmoji.WARNING} Quality below threshold, consider retry")
                # In production: implement retry logic

        # STEP 6: Record in memory
        if request.user_id and self.memory_manager:
            self.logger.info(f"{LogEmoji.AI} Recording interaction in memory")
            await self.memory_manager.record_interaction(
                user_id=request.user_id,
                query=request.query,
                results=retrieved_properties,
                success=True,
                applied_skills=[],
                metadata={
                    "confidence": supervisor_result.confidence,
                    "quality_score": quality_score
                }
            )
            operators_executed.append("memory_storage")

        return RAGQueryResponse(
            response=generated_response,
            retrieved_count=len(retrieved_properties),
            confidence=supervisor_result.confidence,
            sources=[
                {
                    "property_id": prop.get("property_id"),
                    "title": prop.get("title"),
                    "price": prop.get("price")
                }
                for prop in retrieved_properties[:3]
            ],
            pipeline_used="advanced_modular_rag",
            quality_score=quality_score,
            operators_executed=operators_executed,
            memory_context_used=memory_context_used
        )

    async def _basic_rag_pipeline(self, request: RAGQueryRequest) -> RAGQueryResponse:
        """
        BASIC PIPELINE: Simple retrieve â†’ augment â†’ generate (fallback)
        Same as original main.py implementation
        """
        # STEP 1: RETRIEVE
        retrieved_properties = await self._retrieve_basic(
            request.query,
            request.filters,
            request.limit
        )

        if not retrieved_properties:
            return RAGQueryResponse(
                response="Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y báº¥t Ä‘á»™ng sáº£n phÃ¹ há»£p vá»›i yÃªu cáº§u cá»§a báº¡n.",
                retrieved_count=0,
                confidence=0.0,
                sources=[],
                pipeline_used="basic"
            )

        # STEP 2: AUGMENT
        context = self._build_context(retrieved_properties, request.query)

        # STEP 3: GENERATE
        generated_response = await self._generate_basic(
            query=request.query,
            context=context,
            retrieved_properties=retrieved_properties
        )

        return RAGQueryResponse(
            response=generated_response,
            retrieved_count=len(retrieved_properties),
            confidence=0.9,
            sources=[
                {
                    "property_id": prop.get("property_id"),
                    "title": prop.get("title"),
                    "price": prop.get("price")
                }
                for prop in retrieved_properties[:3]
            ],
            pipeline_used="basic"
        )

    async def _retrieve_basic(self, query: str, filters: Optional[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
        """Basic retrieval via DB Gateway"""
        try:
            search_request = {
                "query": query,
                "filters": filters or {},
                "limit": limit
            }

            response = await self.http_client.post(
                f"{self.db_gateway_url}/search",
                json=search_request,
                timeout=30.0
            )

            if response.status_code == 200:
                data = response.json()
                results = data.get("results", [])
                self.logger.info(f"{LogEmoji.SUCCESS} Retrieved {len(results)} results")
                return results
            else:
                self.logger.warning(f"{LogEmoji.WARNING} DB Gateway returned {response.status_code}")
                return []

        except Exception as e:
            self.logger.error(f"{LogEmoji.ERROR} Retrieval failed: {e}")
            return []

    def _build_context(self, properties: List[Dict[str, Any]], query: str) -> str:
        """Build context from properties (same as original)"""
        if not properties:
            return "KhÃ´ng cÃ³ dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n phÃ¹ há»£p."

        context_parts = [
            "# Dá»® LIá»†U Báº¤T Äá»˜NG Sáº¢N TÃŒM ÄÆ¯á»¢C\n",
            f"TÃ¬m tháº¥y {len(properties)} báº¥t Ä‘á»™ng sáº£n phÃ¹ há»£p vá»›i yÃªu cáº§u: '{query}'\n\n"
        ]

        for i, prop in enumerate(properties, 1):
            context_parts.append(f"## Báº¤T Äá»˜NG Sáº¢N #{i}\n")
            context_parts.append(f"- **TiÃªu Ä‘á»**: {prop.get('title', 'N/A')}\n")

            # Price
            price_display = prop.get('price_display')
            if price_display:
                context_parts.append(f"- **GiÃ¡**: {price_display}\n")
            else:
                price = prop.get('price', 0)
                if isinstance(price, (int, float)) and price > 0:
                    price_str = f"{price/1_000_000_000:.1f} tá»· VNÄ" if price >= 1_000_000_000 else f"{price/1_000_000:.0f} triá»‡u VNÄ"
                    context_parts.append(f"- **GiÃ¡**: {price_str}\n")

            # Location
            city = prop.get('city', '')
            district = prop.get('district', '')
            ward = prop.get('ward', '')
            location_parts = [p for p in [ward, district, city] if p]
            location_str = ', '.join(location_parts) if location_parts else prop.get('location', 'N/A')
            context_parts.append(f"- **Vá»‹ trÃ­**: {location_str}\n")

            # Attributes
            if prop.get('bedrooms'):
                context_parts.append(f"- **PhÃ²ng ngá»§**: {prop['bedrooms']}\n")
            if prop.get('bathrooms'):
                context_parts.append(f"- **PhÃ²ng táº¯m**: {prop['bathrooms']}\n")

            # Area
            area_display = prop.get('area_display')
            if area_display:
                context_parts.append(f"- **Diá»‡n tÃ­ch**: {area_display}\n")
            elif prop.get('area'):
                area = prop['area']
                area_str = f"{area} mÂ²" if isinstance(area, (int, float)) else str(area)
                context_parts.append(f"- **Diá»‡n tÃ­ch**: {area_str}\n")

            # Description excerpt
            if prop.get('description'):
                desc = prop['description'][:200]
                context_parts.append(f"- **MÃ´ táº£**: {desc}...\n")

            context_parts.append("\n")

        return "".join(context_parts)

    async def _generate_basic(self, query: str, context: str, retrieved_properties: List[Dict[str, Any]]) -> str:
        """Basic generation via Core Gateway"""
        try:
            system_prompt = """Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n báº¥t Ä‘á»™ng sáº£n REE AI.

NHIá»†M Vá»¤:
Dá»±a vÃ o dá»¯ liá»‡u báº¥t Ä‘á»™ng sáº£n Ä‘Æ°á»£c cung cáº¥p, hÃ£y táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn, há»¯u Ã­ch cho khÃ¡ch hÃ ng.

QUY Táº®C:
1. Giá»›i thiá»‡u ngáº¯n gá»n sá»‘ lÆ°á»£ng báº¥t Ä‘á»™ng sáº£n tÃ¬m tháº¥y
2. NÃªu báº­t 2-3 báº¥t Ä‘á»™ng sáº£n phÃ¹ há»£p nháº¥t
3. Cung cáº¥p thÃ´ng tin chi tiáº¿t: giÃ¡, vá»‹ trÃ­, diá»‡n tÃ­ch, sá»‘ phÃ²ng
4. TÆ° váº¥n thÃªm náº¿u phÃ¹ há»£p
5. Káº¿t thÃºc vá»›i cÃ¢u há»i hoáº·c lá»i má»i hÃ nh Ä‘á»™ng

PHONG CÃCH:
- ThÃ¢n thiá»‡n, chuyÃªn nghiá»‡p
- Sá»­ dá»¥ng tiáº¿ng Viá»‡t tá»± nhiÃªn
- KhÃ´ng copy nguyÃªn vÄƒn dá»¯ liá»‡u"""

            user_prompt = f"""CÃ¢u há»i cá»§a khÃ¡ch hÃ ng: {query}

{context}

HÃ£y táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn, há»¯u Ã­ch cho khÃ¡ch hÃ ng dá»±a trÃªn dá»¯ liá»‡u trÃªn."""

            llm_request = {
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "max_tokens": 500,
                "temperature": 0.7
            }

            response = await self.http_client.post(
                f"{self.core_gateway_url}/chat/completions",
                json=llm_request,
                timeout=30.0
            )

            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("content", "").strip()
                return generated_text
            else:
                return self._format_simple_response(retrieved_properties)

        except Exception as e:
            self.logger.error(f"{LogEmoji.ERROR} Generation failed: {e}")
            return self._format_simple_response(retrieved_properties)

    def _format_simple_response(self, properties: List[Dict[str, Any]]) -> str:
        """Fallback: Simple formatting without LLM"""
        if not properties:
            return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y báº¥t Ä‘á»™ng sáº£n phÃ¹ há»£p vá»›i yÃªu cáº§u cá»§a báº¡n."

        response_parts = [f"TÃ´i Ä‘Ã£ tÃ¬m tháº¥y {len(properties)} báº¥t Ä‘á»™ng sáº£n phÃ¹ há»£p:\n\n"]

        for i, prop in enumerate(properties, 1):
            price_str = prop.get('price_display')
            if not price_str:
                price = prop.get('price', 0)
                if isinstance(price, (int, float)) and price > 0:
                    price_str = f"{price/1_000_000_000:.1f} tá»·"
                else:
                    price_str = "GiÃ¡ thá»a thuáº­n"

            location_str = prop.get('district', prop.get('location', 'N/A'))

            response_parts.append(f"{i}. **{prop.get('title', 'N/A')}**\n")
            response_parts.append(f"   - ðŸ’° GiÃ¡: {price_str}\n")
            response_parts.append(f"   - ðŸ“ Vá»‹ trÃ­: {location_str}\n")

            if prop.get('bedrooms'):
                response_parts.append(f"   - ðŸ›ï¸ {prop['bedrooms']} phÃ²ng ngá»§\n")

            area_str = prop.get('area_display')
            if not area_str and prop.get('area'):
                area = prop['area']
                area_str = f"{area} mÂ²" if isinstance(area, (int, float)) else str(area)

            if area_str:
                response_parts.append(f"   - ðŸ“ Diá»‡n tÃ­ch: {area_str}\n")

            response_parts.append("\n")

        return "".join(response_parts)

    def _is_complex_query(self, query: str) -> bool:
        """Check if query is complex enough for decomposition"""
        constraint_keywords = ["vÃ ", "gáº§n", "giÃ¡", "dÆ°á»›i", "trÃªn", "cÃ³", "vá»›i"]
        count = sum(1 for kw in constraint_keywords if kw in query.lower())
        return count >= 2

    async def on_shutdown(self):
        """Cleanup on shutdown"""
        await self.http_client.aclose()

        # Cleanup operators
        if self.advanced_enabled:
            if hasattr(self.query_rewriter, 'cleanup'):
                await self.query_rewriter.cleanup()
            if hasattr(self.hyde_operator, 'cleanup'):
                await self.hyde_operator.cleanup()
            if hasattr(self.decomposition_operator, 'cleanup'):
                await self.decomposition_operator.cleanup()
            if hasattr(self.reflection_operator, 'cleanup'):
                await self.reflection_operator.cleanup()

        await super().on_shutdown()


# Create service instance at module level for uvicorn
service = EnhancedRAGService()
app = service.app

if __name__ == "__main__":
    service.run()
