"""
AI User Simulator - Automated Conversation Evaluation
Uses LLM to role-play different user personas and test system responses
"""
import asyncio
import httpx
import json
import time
from datetime import datetime
from typing import List, Dict, Any
import random

# 100 User Personas with Detailed Characteristics
USER_PERSONAS = [
    # First-time buyers (10 personas)
    *[{
        "id": f"first_buyer_{i}",
        "type": "C·∫∑p v·ª£ ch·ªìng tr·∫ª, l·∫ßn ƒë·∫ßu mua nh√†",
        "age_range": "25-35",
        "budget": random.choice(["2-3 t·ª∑", "3-4 t·ª∑", "1.5-2.5 t·ª∑"]),
        "district_preference": random.choice(["qu·∫≠n 7", "qu·∫≠n 2", "Th·ªß ƒê·ª©c", "qu·∫≠n 9", "B√¨nh Th·∫°nh"]),
        "requirements": random.choice([
            "2 ph√≤ng ng·ªß, g·∫ßn tr∆∞·ªùng h·ªçc",
            "c√≥ ch·ªó ƒë·∫≠u xe, view ƒë·∫πp",
            "g·∫ßn trung t√¢m, thu·∫≠n ti·ªán ƒëi l√†m",
            "khu an ninh t·ªët, d√¢n tr√≠ cao"
        ]),
        "characteristics": "H·ªèi nhi·ªÅu, c·∫ßn t∆∞ v·∫•n chi ti·∫øt, quan t√¢m th·ªß t·ª•c",
        "conversation_turns": random.randint(6, 10)
    } for i in range(1, 11)],

    # Experienced investors (10 personas)
    *[{
        "id": f"investor_{i}",
        "type": "Nh√† ƒë·∫ßu t∆∞ c√≥ kinh nghi·ªám",
        "age_range": "35-50",
        "budget": random.choice(["5-10 t·ª∑", "10-20 t·ª∑", "tr√™n 20 t·ª∑"]),
        "district_preference": random.choice(["qu·∫≠n 1", "qu·∫≠n 2", "qu·∫≠n 7", "Th·ªß ƒê·ª©c"]),
        "requirements": random.choice([
            "ROI cao, ti·ªÅm nƒÉng tƒÉng gi√°",
            "v·ªã tr√≠ ƒë·∫Øc ƒë·ªãa, d·ªÖ cho thu√™",
            "d·ª± √°n m·ªõi, ch·ªß ƒë·∫ßu t∆∞ uy t√≠n",
            "ph√°p l√Ω r√µ r√†ng, s·ªï h·ªìng s·∫µn"
        ]),
        "characteristics": "Ph√¢n t√≠ch k·ªπ, so s√°nh nhi·ªÅu, t·∫≠p trung l·ª£i nhu·∫≠n",
        "conversation_turns": random.randint(5, 8)
    } for i in range(1, 11)],

    # Families with children (10 personas)
    *[{
        "id": f"family_{i}",
        "type": "Gia ƒë√¨nh c√≥ con nh·ªè",
        "age_range": "30-45",
        "budget": random.choice(["3-5 t·ª∑", "4-6 t·ª∑", "5-7 t·ª∑"]),
        "district_preference": random.choice(["Th·ªß ƒê·ª©c", "qu·∫≠n 2", "qu·∫≠n 7", "qu·∫≠n 9"]),
        "requirements": random.choice([
            "g·∫ßn tr∆∞·ªùng qu·ªëc t·∫ø, c√¥ng vi√™n",
            "khu compound an to√†n",
            "nhi·ªÅu ti·ªán √≠ch cho tr·∫ª em",
            "kh√¥ng kh√≠ trong l√†nh"
        ]),
        "characteristics": "∆Øu ti√™n m√¥i tr∆∞·ªùng s·ªëng, ti·ªán √≠ch gi√°o d·ª•c",
        "conversation_turns": random.randint(6, 9)
    } for i in range(1, 11)],

    # Retirees (10 personas)
    *[{
        "id": f"retiree_{i}",
        "type": "Ng∆∞·ªùi v·ªÅ h∆∞u",
        "age_range": "55-70",
        "budget": random.choice(["2-3 t·ª∑", "3-4 t·ª∑", "1.5-2 t·ª∑"]),
        "district_preference": random.choice(["qu·∫≠n 3", "qu·∫≠n 10", "B√¨nh Th·∫°nh", "Ph√∫ Nhu·∫≠n"]),
        "requirements": random.choice([
            "y√™n tƒ©nh, g·∫ßn b·ªánh vi·ªán",
            "t·∫ßng th·∫•p, c√≥ thang m√°y",
            "g·∫ßn ch·ª£, g·∫ßn c√¥ng vi√™n",
            "khu d√¢n c∆∞ ·ªïn ƒë·ªãnh"
        ]),
        "characteristics": "Th·∫≠n tr·ªçng, kh√¥ng v·ªôi, quan t√¢m y t·∫ø",
        "conversation_turns": random.randint(5, 8)
    } for i in range(1, 11)],

    # Students/Renters (10 personas)
    *[{
        "id": f"student_{i}",
        "type": "Sinh vi√™n / Ng∆∞·ªùi thu√™ nh√†",
        "age_range": "18-25",
        "budget": random.choice(["3-5 tri·ªáu/th√°ng", "5-7 tri·ªáu/th√°ng", "2-4 tri·ªáu/th√°ng"]),
        "district_preference": random.choice(["Th·ªß ƒê·ª©c", "qu·∫≠n 9", "B√¨nh Th·∫°nh", "qu·∫≠n 7"]),
        "requirements": random.choice([
            "g·∫ßn tr∆∞·ªùng ƒë·∫°i h·ªçc",
            "c√≥ wifi, ƒëi·ªÅu h√≤a",
            "gi√° r·∫ª, ti·ªán √≠ch ƒë·∫ßy ƒë·ªß",
            "g·∫ßn khu ƒÉn u·ªëng, si√™u th·ªã"
        ]),
        "characteristics": "Ng√¢n s√°ch th·∫•p, c·∫ßn nhanh, ∆∞u ti√™n ti·ªán l·ª£i",
        "conversation_turns": random.randint(4, 6)
    } for i in range(1, 11)],

    # Foreign buyers (10 personas)
    *[{
        "id": f"expat_{i}",
        "type": "Ng∆∞·ªùi n∆∞·ªõc ngo√†i",
        "age_range": "30-50",
        "budget": random.choice(["$150k-200k", "$200k-300k", "$100k-150k"]),
        "district_preference": random.choice(["qu·∫≠n 1", "qu·∫≠n 2", "qu·∫≠n 7", "B√¨nh Th·∫°nh"]),
        "requirements": random.choice([
            "khu expat, g·∫ßn tr∆∞·ªùng qu·ªëc t·∫ø",
            "cƒÉn h·ªô d·ªãch v·ª•, n·ªôi th·∫•t ƒë·∫ßy ƒë·ªß",
            "g·∫ßn c√¥ng ty, trung t√¢m th∆∞∆°ng m·∫°i",
            "ph√°p l√Ω r√µ r√†ng cho ng∆∞·ªùi n∆∞·ªõc ngo√†i"
        ]),
        "characteristics": "Quan t√¢m th·ªß t·ª•c, c·∫ßn h·ªó tr·ª£ ng√¥n ng·ªØ",
        "conversation_turns": random.randint(5, 8)
    } for i in range(1, 11)],

    # Buy-to-let investors (10 personas)
    *[{
        "id": f"landlord_{i}",
        "type": "Ng∆∞·ªùi mua ƒë·ªÉ cho thu√™",
        "age_range": "30-50",
        "budget": random.choice(["4-6 t·ª∑", "6-8 t·ª∑", "3-5 t·ª∑"]),
        "district_preference": random.choice(["qu·∫≠n 1", "qu·∫≠n 2", "qu·∫≠n 7", "B√¨nh Th·∫°nh"]),
        "requirements": random.choice([
            "l·ª£i su·∫•t cho thu√™ cao",
            "v·ªã tr√≠ trung t√¢m, d·ªÖ cho thu√™",
            "d·ª± √°n m·ªõi, ch·∫•t l∆∞·ª£ng t·ªët",
            "qu·∫£n l√Ω chuy√™n nghi·ªáp"
        ]),
        "characteristics": "T√≠nh to√°n k·ªπ ROI, quan t√¢m cash flow",
        "conversation_turns": random.randint(5, 7)
    } for i in range(1, 11)],

    # Sellers (10 personas)
    *[{
        "id": f"seller_{i}",
        "type": "Ng∆∞·ªùi b√°n nh√†",
        "age_range": "35-60",
        "budget": "N/A",
        "district_preference": random.choice(["qu·∫≠n 2", "qu·∫≠n 7", "Th·ªß ƒê·ª©c", "B√¨nh Th·∫°nh"]),
        "requirements": random.choice([
            "ƒë·ªãnh gi√° h·ª£p l√Ω",
            "b√°n nhanh, c·∫ßn ti·ªÅn",
            "t√¨m kh√°ch h√†ng ti·ªÅm nƒÉng",
            "th·ªß t·ª•c b√°n nh√†"
        ]),
        "characteristics": "C·∫ßn ƒë·ªãnh gi√°, marketing, t∆∞ v·∫•n th·ªß t·ª•c",
        "conversation_turns": random.randint(4, 6)
    } for i in range(1, 11)],

    # Window shoppers (10 personas)
    *[{
        "id": f"browser_{i}",
        "type": "Ng∆∞·ªùi ch·ªâ t√¨m hi·ªÉu",
        "age_range": "25-40",
        "budget": "Ch∆∞a x√°c ƒë·ªãnh",
        "district_preference": random.choice(["nhi·ªÅu qu·∫≠n", "ch∆∞a quy·∫øt ƒë·ªãnh", "t√πy t√¨nh h√¨nh"]),
        "requirements": random.choice([
            "kh·∫£o s√°t th·ªã tr∆∞·ªùng",
            "t√¨m hi·ªÉu xu h∆∞·ªõng",
            "ch∆∞a c√≥ k·∫ø ho·∫°ch c·ª• th·ªÉ",
            "so s√°nh gi√° th·ªã tr∆∞·ªùng"
        ]),
        "characteristics": "C√¢u h·ªèi chung chung, ch∆∞a quy·∫øt ƒë·ªãnh, kh√°m ph√°",
        "conversation_turns": random.randint(3, 5)
    } for i in range(1, 11)],

    # Complex scenarios (10 personas)
    *[{
        "id": f"complex_{i}",
        "type": "K·ªãch b·∫£n ph·ª©c t·∫°p",
        "age_range": "Varied",
        "budget": "Varied",
        "district_preference": "Multiple",
        "requirements": random.choice([
            "ƒë·ªïi nh√†, b√°n c≈© mua m·ªõi",
            "t√≠ch l≈©y t√†i s·∫£n, ƒëa d·∫°ng h√≥a",
            "nhi·ªÅu ti√™u ch√≠ kh√°c nhau",
            "chuy·ªÉn ƒë·ªïi context li√™n t·ª•c"
        ]),
        "characteristics": "Test kh·∫£ nƒÉng x·ª≠ l√Ω context ph·ª©c t·∫°p, ƒëa d·∫°ng nhu c·∫ßu",
        "conversation_turns": random.randint(8, 12)
    } for i in range(1, 11)]
]

class AIUserSimulator:
    """Simulates realistic user conversations using LLM"""

    def __init__(self,
                 user_model: str = "llama3.2:latest",  # Model for user simulation (upgraded to 2GB)
                 orchestrator_url: str = "http://localhost:8090"):
        self.user_model = user_model
        self.orchestrator_url = orchestrator_url
        self.ollama_url = "http://localhost:11434"

    async def generate_user_query(self, persona: Dict, conversation_history: List[Dict], turn_num: int) -> str:
        """Use LLM to generate natural user query based on persona and context"""

        # Build prompt for user simulator with explicit role-playing instructions
        prompt = f"""B·∫†N ƒêANG ƒê√ìNG VAI M·ªòT NG∆Ø·ªúI D√ôNG TH·∫¨T ƒëang t√¨m nh√† tr√™n ·ª©ng d·ª•ng b·∫•t ƒë·ªông s·∫£n.

===== TH√îNG TIN V·ªÄ B·∫†N =====
- B·∫°n l√†: {persona['type']}
- Tu·ªïi: {persona['age_range']}
- Ng√¢n s√°ch: {persona['budget']}
- Mu·ªën t√¨m ·ªü: {persona['district_preference']}
- Y√™u c·∫ßu: {persona['requirements']}
- T√≠nh c√°ch: {persona['characteristics']}

===== T√åNH HU·ªêNG =====
B·∫°n ƒëang chat v·ªõi tr·ª£ l√Ω AI b·∫•t ƒë·ªông s·∫£n. ƒê√¢y l√† l∆∞·ª£t {turn_num}/{persona['conversation_turns']} trong cu·ªôc tr√≤ chuy·ªán.
"""

        # Add conversation history
        if conversation_history:
            prompt += "\n===== L·ªäCH S·ª¨ TR√í CHUY·ªÜN =====\n"
            for msg in conversation_history[-6:]:  # Last 3 exchanges
                role = "B·∫°n ƒë√£ n√≥i" if msg['role'] == 'user' else "AI ƒë√£ tr·∫£ l·ªùi"
                prompt += f"{role}: \"{msg['content'][:200]}\"\n"
            prompt += "\n"

        # Add instruction based on turn number with EXAMPLES
        if turn_num == 1:
            prompt += """===== NHI·ªÜM V·ª§: C√ÇU M·ªû ƒê·∫¶U =====
H√£y ch√†o h·ªèi v√† n√≥i v·ªÅ nhu c·∫ßu c·ªßa b·∫°n m·ªôt c√°ch T·ª∞ NHI√äN nh∆∞ ng∆∞·ªùi th·∫≠t.

V√ç D·ª§ T·ªêT:
- "Xin ch√†o, m√¨nh mu·ªën t√¨m cƒÉn h·ªô 2 ph√≤ng ng·ªß ·ªü qu·∫≠n 7 trong kho·∫£ng 3 t·ª∑"
- "Ch√†o b·∫°n, gia ƒë√¨nh m√¨nh ƒëang t√¨m nh√† g·∫ßn tr∆∞·ªùng qu·ªëc t·∫ø"
- "Hi, m√¨nh mu·ªën t√¨m hi·ªÉu th√¥ng tin v·ªÅ b·∫•t ƒë·ªông s·∫£n khu v·ª±c Th·ªß ƒê·ª©c"

V√ç D·ª§ T·ªÜ (KH√îNG L√ÄM TH·∫æ N√ÄY):
- "M·ª•c ti√™u c·ªßa b·∫°n l√† g√¨?" ‚Üê ƒê√¢y l√† AI h·ªèi user, KH√îNG ph·∫£i user h·ªèi AI!
- "B·∫°n c√≥ th·ªÉ gi√∫p g√¨?" ‚Üê Qu√° chung chung
- "Ch√†o b·∫°n!" ‚Üê Ch·ªâ ch√†o, kh√¥ng n√≥i nhu c·∫ßu

H√ÉY NH·ªö: B·∫†N L√Ä KH√ÅCH H√ÄNG, kh√¥ng ph·∫£i AI assistant!
"""
        elif turn_num == persona['conversation_turns']:
            prompt += """===== NHI·ªÜM V·ª§: C√ÇU K·∫æT TH√öC =====
K·∫øt th√∫c cu·ªôc tr√≤ chuy·ªán m·ªôt c√°ch t·ª± nhi√™n.

V√ç D·ª§:
- "C·∫£m ∆°n b·∫°n! ƒê·ªÉ m√¨nh suy nghƒ© th√™m"
- "OK, m√¨nh s·∫Ω li√™n h·ªá l·∫°i sau. Th·ªß t·ª•c mua nh√† c·∫ßn gi·∫•y t·ªù g√¨?"
- "Thanks, cho m√¨nh xin contact ƒë·ªÉ h·∫πn xem nh√†"
"""
        else:
            prompt += f"""===== NHI·ªÜM V·ª§: C√ÇU H·ªéI TI·∫æP THEO =====
D·ª±a v√†o l·ªãch s·ª≠ tr√≤ chuy·ªán, h√£y h·ªèi c√¢u TI·∫æP THEO m·ªôt c√°ch t·ª± nhi√™n.

QUY T·∫ÆC QUAN TR·ªåNG:
1. PH·∫¢I li√™n quan ƒë·∫øn ng·ªØ c·∫£nh ƒë√£ n√≥i
2. C√ì TH·ªÇ tham chi·∫øu th√¥ng tin c≈©: "cƒÉn ƒë√≥", "gi√° v·ª´a n√≥i", "khu v·ª±c em v·ª´a gi·ªõi thi·ªáu"
3. Ph√π h·ª£p v·ªõi t√≠nh c√°ch: {persona['characteristics']}
4. ƒê·ª´ng l·∫∑p l·∫°i c√¢u h·ªèi c≈©!

V√ç D·ª§ T·ªêT (c√≥ tham chi·∫øu context):
- "CƒÉn ƒë√≥ c√≥ view ƒë·∫πp kh√¥ng?" ‚Üê Tham chi·∫øu "cƒÉn ƒë√≥" t·ª´ c√¢u tr∆∞·ªõc
- "Gi√° 2.5 t·ª∑ cho 70m¬≤ c√≥ h·ª£p l√Ω kh√¥ng?" ‚Üê H·ªèi c·ª• th·ªÉ
- "So v·ªõi khu v·ª±c em v·ª´a n√≥i, qu·∫≠n 2 th√¨ sao?" ‚Üê Tham chi·∫øu khu v·ª±c ƒë√£ ƒë·ªÅ c·∫≠p

V√ç D·ª§ T·ªÜ:
- L·∫∑p l·∫°i c√¢u c≈© nguy√™n xi
- H·ªèi c√¢u kh√¥ng li√™n quan g√¨ ƒë·∫øn cu·ªôc n√≥i chuy·ªán
"""

        prompt += "\n===== Y√äU C·∫¶U OUTPUT =====\n"
        prompt += "Ch·ªâ vi·∫øt C√ÇU H·ªéI c·ªßa b·∫°n (ng∆∞·ªùi d√πng), KH√îNG th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch n√†o.\n"
        prompt += "C√¢u h·ªèi (ti·∫øng Vi·ªát t·ª± nhi√™n): "

        # Call Ollama API with user model
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.user_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.8,  # Higher for more natural variation
                        "top_p": 0.9
                    }
                }
            )
            response.raise_for_status()
            data = response.json()
            user_query = data["response"].strip()

            # Clean up if model adds extra explanation
            if "C√¢u h·ªèi:" in user_query:
                user_query = user_query.split("C√¢u h·ªèi:")[-1].strip()
            if "\n" in user_query:
                user_query = user_query.split("\n")[0].strip()

            return user_query

    async def send_to_system(self, query: str, session_id: str, history: List[Dict]) -> Dict:
        """Send user query to orchestrator and get response

        IMPORTANT: Send FULL conversation history from beginning to test memory context
        """
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.orchestrator_url}/orchestrate",
                json={
                    "user_id": session_id,
                    "query": query,
                    "conversation_id": session_id,
                    "metadata": {
                        "history": history  # FULL history to test memory context
                    }
                }
            )
            response.raise_for_status()
            return response.json()

    async def evaluate_response(self, persona: Dict, user_query: str, system_response: Dict, turn_num: int) -> Dict:
        """Evaluate system response quality"""
        evaluation = {
            "intent_correct": None,  # Will be filled based on expected intent
            "response_quality": None,  # 1-5 scale
            "vietnamese_quality": None,  # 1-5 scale
            "relevance": None,  # 1-5 scale
            "context_preserved": None,  # True/False
            "issues": []
        }

        response_text = system_response.get("response", "")

        # Check Vietnamese quality (basic checks)
        if not response_text:
            evaluation["issues"].append("Empty response")
            evaluation["vietnamese_quality"] = 1
        elif any(char in response_text for char in ["‰πå", "Ê¨¢", "Ë∞¢", "ÊÇ®", "Âú®", "ÊòØ"]):  # Chinese characters
            evaluation["issues"].append("Contains Chinese characters")
            evaluation["vietnamese_quality"] = 1
        elif len(response_text) < 20:
            evaluation["issues"].append("Response too short")
            evaluation["vietnamese_quality"] = 2
        else:
            # Simple heuristic: longer, Vietnamese-like response is better
            evaluation["vietnamese_quality"] = 4

        # Check response time
        exec_time = system_response.get("execution_time_ms", 0)
        if exec_time > 30000:
            evaluation["issues"].append(f"Slow response: {exec_time}ms")

        return evaluation

    async def run_scenario(self, persona: Dict, scenario_num: int) -> Dict:
        """Run a complete conversation scenario"""
        session_id = persona['id']
        conversation_history = []
        turns_data = []

        print(f"\n{'='*100}")
        print(f"üìã Scenario {scenario_num}/100: {persona['type']} ({persona['id']})")
        print(f"üí∞ Budget: {persona['budget']}")
        print(f"üìç District: {persona['district_preference']}")
        print(f"üéØ Requirements: {persona['requirements']}")
        print(f"üìù Characteristics: {persona['characteristics']}")
        print(f"{'='*100}\n")

        for turn_num in range(1, persona['conversation_turns'] + 1):
            print(f"{'‚îÄ'*100}")
            print(f"Turn {turn_num}/{persona['conversation_turns']}")

            start_time = time.time()

            try:
                # 1. Generate user query using LLM
                user_query = await self.generate_user_query(persona, conversation_history, turn_num)
                print(f"üë§ User: {user_query}")

                # 2. Send to system
                system_response = await self.send_to_system(user_query, session_id, conversation_history)

                ai_response = system_response.get("response", "")
                intent = system_response.get("intent", "unknown")
                confidence = system_response.get("confidence", 0)

                print(f"ü§ñ AI: {ai_response[:200]}{'...' if len(ai_response) > 200 else ''}")
                print(f"üìä Intent: {intent} (confidence: {confidence:.2f})")

                # 3. Evaluate response
                evaluation = await self.evaluate_response(persona, user_query, system_response, turn_num)

                elapsed_ms = (time.time() - start_time) * 1000
                print(f"‚è±Ô∏è  Time: {elapsed_ms:.0f}ms")
                print(f"‚≠ê Quality: Vietnamese {evaluation['vietnamese_quality']}/5")
                if evaluation['issues']:
                    print(f"‚ö†Ô∏è  Issues: {', '.join(evaluation['issues'])}")

                # Store turn data
                turn_data = {
                    "turn": turn_num,
                    "user_query": user_query,
                    "system_response": ai_response,
                    "intent": intent,
                    "confidence": confidence,
                    "response_time_ms": elapsed_ms,
                    "evaluation": evaluation
                }
                turns_data.append(turn_data)

                # Update history
                conversation_history.append({"role": "user", "content": user_query})
                conversation_history.append({"role": "assistant", "content": ai_response})

            except Exception as e:
                print(f"‚ùå Error: {str(e)}")
                turns_data.append({
                    "turn": turn_num,
                    "error": str(e)
                })

            # Brief pause
            await asyncio.sleep(0.5)

        # Calculate scenario metrics
        successful_turns = [t for t in turns_data if "error" not in t]
        avg_response_time = sum(t.get("response_time_ms", 0) for t in successful_turns) / len(successful_turns) if successful_turns else 0
        avg_quality = sum(t.get("evaluation", {}).get("vietnamese_quality", 0) for t in successful_turns) / len(successful_turns) if successful_turns else 0

        scenario_result = {
            "scenario_num": scenario_num,
            "persona": persona,
            "total_turns": persona['conversation_turns'],
            "successful_turns": len(successful_turns),
            "avg_response_time_ms": avg_response_time,
            "avg_vietnamese_quality": avg_quality,
            "turns": turns_data
        }

        print(f"\n{'‚îÄ'*100}")
        print(f"üìä Scenario Summary:")
        print(f"   Success Rate: {len(successful_turns)}/{persona['conversation_turns']}")
        print(f"   Avg Response Time: {avg_response_time:.0f}ms")
        print(f"   Avg Quality: {avg_quality:.1f}/5")
        print(f"{'‚îÄ'*100}\n")

        return scenario_result

    async def run_all_scenarios(self, num_scenarios: int = 100):
        """Run all 100 scenarios"""
        print("\n" + "="*100)
        print("üî• AI USER SIMULATOR - 100 Scenarios")
        print(f"User Model: {self.user_model}")
        print(f"System Model: deepseek-v3.1:671b-cloud (via orchestrator)")
        print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*100)

        all_results = []

        for i in range(min(num_scenarios, len(USER_PERSONAS))):
            persona = USER_PERSONAS[i]
            result = await self.run_scenario(persona, i + 1)
            all_results.append(result)

            # Save incremental results every 10 scenarios
            if (i + 1) % 10 == 0:
                await self.save_results(all_results, partial=True)

        # Save final results
        await self.save_results(all_results, partial=False)

        print("\n" + "="*100)
        print("‚úÖ ALL SCENARIOS COMPLETED!")
        print("="*100 + "\n")

    async def save_results(self, results: List[Dict], partial: bool = False):
        """Save evaluation results"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        prefix = "partial_" if partial else ""

        # Save detailed JSON
        json_path = f"/tmp/{prefix}ai_evaluation_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump({
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "user_model": self.user_model,
                    "system_model": "deepseek-v3.1:671b-cloud",
                    "total_scenarios": len(results),
                    "partial": partial
                },
                "results": results
            }, f, ensure_ascii=False, indent=2)

        if not partial:
            print(f"üìÅ Detailed results saved: {json_path}")

async def main():
    simulator = AIUserSimulator(user_model="qwen3:0.6b")
    await simulator.run_all_scenarios(num_scenarios=100)

if __name__ == "__main__":
    asyncio.run(main())
